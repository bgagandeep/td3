{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd31c486",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pybullet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05ff1176",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pybullet_envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "968d1f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from gym import wrappers\n",
    "from torch.autograd import Variable\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4304019",
   "metadata": {},
   "source": [
    "## Step 1: We initialize the Experience Replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6382cb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, max_size=1e6):\n",
    "        # initialize an array to store transitions\n",
    "        self.storage = []\n",
    "    \n",
    "        # initialize the max size array, this is will the maximum size of the relpay buffer\n",
    "        self.max_size = max_size\n",
    "    \n",
    "        # initialize pointer which will be used to push the oldest entry out\n",
    "        self.ptr = 0\n",
    "\n",
    "    def add(self, transition):\n",
    "    # check if the pointer buffer is full\n",
    "        if len(self.storage) == self.max_size:\n",
    "            # if the pointer buffer is full replace the oldest entry of transitions with the new one\n",
    "            self.storage[int(self.ptr)] = transition\n",
    "\n",
    "            # the below code will keep increasing the pointer as it will replace the oldest entries until\n",
    "            # the maximum ptr value is attained and it will reset itself to zero and hence repeating the \n",
    "            # process of removing the older entry and entering the new one\n",
    "            self.ptr = (self.ptr + 1) % self.max_size\n",
    "\n",
    "        else:\n",
    "            # if the buffer is not yet full, just append the new transition to the next available slot\n",
    "            self.storage.append(transition)\n",
    "\n",
    "    # this function will help get samples from the replay buffer to use when training the model\n",
    "    def sample(self, batch_size):\n",
    "        # ind will store random integers which will be from 0 to the used size of the replay buffer\n",
    "        # it will generate the same number of random integers that are equal to the batch size\n",
    "        ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
    "        \n",
    "        # initializing the 5 variables which will store the random values extracted from the replay buffer\n",
    "        batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n",
    "        \n",
    "        # now we loop through all the random integers found in the ind variable and we use them to pull data\n",
    "        # from the replay buffer\n",
    "        for i in ind: \n",
    "            # 1st step in the loop simply pulls a single iteration and stores them in the 5 variables below\n",
    "            state, next_state, action, reward, done = self.storage[i]\n",
    "            \n",
    "            # the following 5 lines of code are part of the loop and we basically append the data we just pulled\n",
    "            # from the single iteration of the replay buffer (above) and append it to the list.\n",
    "            # the lists below are then returned as a result of this function\n",
    "            batch_states.append(np.array(state, copy=False))\n",
    "            batch_next_states.append(np.array(next_state, copy=False))\n",
    "            batch_actions.append(np.array(action, copy=False))\n",
    "            batch_rewards.append(np.array(reward, copy=False))\n",
    "            batch_dones.append(np.array(done, copy=False))\n",
    "            \n",
    "        # notice that before returning the lists, they are converted into numpy arrays as that's how pytorch would be able to understand it\n",
    "        # also please note that rewards need to be reshaped from horizontal entries to vertical for the rewards array (it will still be a numpy array)\n",
    "        return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98fd824",
   "metadata": {},
   "source": [
    "## Step 3: We build two neural networks for the two Critic models and two neural networks for the two Critic targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2fd396b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now create a class for the Actor neural network. This class will be used for both the actor model and actor target neural network\n",
    "# also notice that this class is inheriting the torch.nn.module class (here's a link to it's documentation https://pytorch.org/docs/stable/generated/torch.nn.Module.html)\n",
    "class Actor(nn.Module):\n",
    "    \n",
    "    # the first function i.e. the initialization function will take the total number of states, total actions and max actions\n",
    "    # using the data provided, it will create a 3 layer neural network (1 input layer, 1 hidden layer and 1 output/action layer)\n",
    "    # the number of nodes in the input layer and the number nodes in the output/action layer will defined based on the parameters \n",
    "    # that will be provided when creating an object of this class\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        # this command is required for the object to inherit the nn.module class (don't think too much about it... just use it)\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        # now to start creating the neural network from here\n",
    "        self.layer_1 = nn.Linear(state_dim, 400)\n",
    "        self.layer_2 = nn.Linear(400,300)\n",
    "        self.layer_3 = nn.Linear(300, action_dim)\n",
    "        self.max_action = max_action\n",
    "        \n",
    "    # next function is the forward propagation function that simply connects the layers and forms the neural network\n",
    "    # the x is a parameter and the input state will be fed into this parameter to be able to get the actions\n",
    "    # F is a functional module of torch\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer_1(x))\n",
    "        x = F.relu(self.layer_2(x))\n",
    "        \n",
    "        # the reason we are multiplying the 3rd layer with tanh activation function with self.max_action is because\n",
    "        # the output of the tanh activation will be from -1 to +1, we need to convert it to the continuous value of the actions\n",
    "        # hence we multiply the number with the max_action\n",
    "        x = self.max_action * torch.tanh(self.layer_3(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acbb984",
   "metadata": {},
   "source": [
    "## Step 3: We build two neural networks for the two Critic models and two neural networks for the two Critic targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34300c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now create a class for the Critic neural network. This class will be used for both the 2 critic model and 2 critic target neural networks\n",
    "# also notice that this class is inheriting the torch.nn.module class (here's a link to it's documentation https://pytorch.org/docs/stable/generated/torch.nn.Module.html)\n",
    "class Critic(nn.Module):\n",
    "    \n",
    "    # the first function i.e. the initialization function will take the total number of states, total actions only. max_actions will not be required here\n",
    "    # using the data provided, it will create a 3 layer neural network (1 input layer, 1 hidden layer and 1 output layer)\n",
    "    # the number of nodes in the input layer will defined based on the state_dim parameter + the action_dim parameter and the output layer will only give one value\n",
    "    # the job of the critic network is to generate q-values and to get that i uses the state and the action data\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        # there will be two neural networks that will be required which will need to generate two q-values (one each)\n",
    "        # here's the first Critic Neural Network, it takes the number of states and number of actions as input nodes\n",
    "        # and the output layer will be the hidden layer with 400 nodes.\n",
    "        self.layer_1_1 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.layer_1_2 = nn.Linear(400, 300)\n",
    "        \n",
    "        # as explained above, the output layer has just one node as it will return only a single q-value\n",
    "        self.layer_1_3 = nn.Linear(300, 1)\n",
    "        \n",
    "        # here's the second Critic Neural Network, it takes the number of states and number of actions as input nodes\n",
    "        # and the output layer will be the hidden layer with 400 nodes.\n",
    "        self.layer_2_1 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.layer_2_2 = nn.Linear(400, 300)\n",
    "        \n",
    "        # as explained above, the output layer has just one node as it will return only a single q-value\n",
    "        self.layer_2_3 = nn.Linear(300, 1)\n",
    "        \n",
    "    # now to setup the forward propogation of the layers created above\n",
    "    # please note that this time apart from just the input state, there will be one more parameter and that will be the action taken\n",
    "    def forward(self, x, u):\n",
    "        \n",
    "        # the following code helps concatenate the input state (x) and the action taken (u) into one using torch\n",
    "        xu = torch.cat([x,u], 1)\n",
    "        \n",
    "        # now let's begin the forward propagation for the first Critic Neural Network\n",
    "        x1 = F.relu(self.layer_1_1(xu))\n",
    "        x1 = F.relu(self.layer_1_2(x1))\n",
    "        x1 = self.layer_1_3(x1) # there's no activation function here because we need the final value as q-value\n",
    "        \n",
    "        # now let's begin the forward propagation for the second Critic Neural Network\n",
    "        x2 = F.relu(self.layer_2_1(xu))\n",
    "        x2 = F.relu(self.layer_2_2(x2))\n",
    "        x2 = self.layer_2_3(x2) # there's no activation function here because we need the final value as q-value\n",
    "        \n",
    "        return x1, x2\n",
    "    \n",
    "    # the function below is very similar to the one above but is used to forward propagate only the first critic neural network\n",
    "    # this is used when we do gradient ascent using the single critic neural network\n",
    "    def Q1(self, x, u):\n",
    "        xu = torch.cat([x,u], 1)\n",
    "        x1 = F.relu(self.layer_1_1(xu))\n",
    "        x1 = F.relu(self.layer_1_2(x1))\n",
    "        x1 = self.layer_1_3(x1)\n",
    "        return x1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17872d09",
   "metadata": {},
   "source": [
    "## Steps 4 to 15: Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c43a421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40bdf90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the whole Training Process into a class\n",
    "\n",
    "# this is the key TD3 class where all the required objects are created and the primary functions done\n",
    "class TD3(object):\n",
    "    \n",
    "    # the init function will require the 3 main parameters i.e. the number of states, number of actions and \n",
    "    # what is the maximum action that can be taken\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        \n",
    "        # first the actor model and we pass the tensor to the device that will run the math (i.e. cuda/graphic card or CPU)\n",
    "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        \n",
    "        # next the actor target \n",
    "        self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        \n",
    "        # this command below will allow to load the pre-trained model in the future (i.e the parameter weights and biases)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        \n",
    "        # next we need to define the optimizer when we perform stochastic gradient descent\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
    "\n",
    "        # next we create the object for the critic model which will take the state and the action\n",
    "        self.critic = Critic(state_dim, action_dim).to(device)\n",
    "        \n",
    "        # next the critic target\n",
    "        self.critic_target = Critic(state_dim, action_dim).to(device)\n",
    "        \n",
    "        # then similar to the actor target model getting loaded, we do the same for critic target model\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        \n",
    "        # now we initialize the optimizer for SGD for the critic\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
    "        \n",
    "        # here we just pass the max_action parameter and load it to the max_action variable within the object\n",
    "        self.max_action = max_action\n",
    "        \n",
    "    # the below function get the action to be taken based on the state passed to it as a parameter\n",
    "    def select_action(self, state):\n",
    "        # first the state is converted to a tensor so that it can run through the neural network\n",
    "        state = torch.Tensor(state.reshape(1,-1)).to(device)\n",
    "        \n",
    "        # the return function below uses the actor object created above (not the actor_target) \n",
    "        # and passes the state we just got and runs it through the forward function created \n",
    "        # under the Actor class\n",
    "        return self.actor(state).cpu().data.numpy().flatten()\n",
    "    \n",
    "    # the next function actuall describes how the training of the model will be done it will use the replay buffer, \n",
    "    # the number of times you want to iterate the training process, the batch size, what should be gamma i.e. the discount,\n",
    "    # the value of tau which is used with copying the weights from the actor model to the actor_target neural network,\n",
    "    # policy noise to xxxx, noise clip value to ensure we don't exceed the max_action and policy_freq which checks \n",
    "    # how often the weights and biases of the actor model will get copied to the actor target neural network\n",
    "    def train(self, replay_buffer, iterations, batch_size = 100, discount = 0.99, tau = 0.005, policy_noise = 0.2, noise_clip = 0.5, policy_freq = 2):\n",
    "        \n",
    "        # now we iterate through random samples from the replay buffer and train the model\n",
    "        for it in range(iterations):\n",
    "            \n",
    "            # for every iteration sample a batch of transitions (s, s', a, r) from replay_buffer memory \n",
    "            # i.e. s = current state, s' = next state a = action taken to reach next state and \n",
    "            # r = reward received when entered next state\n",
    "            # please note that this will be a bunch of samples and not a single one (as described by the batch_size)\n",
    "            batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
    "            \n",
    "            # now we convert the individual batches to tensors and load them to new variables\n",
    "            state = torch.Tensor(batch_states).to(device)\n",
    "            next_state = torch.Tensor(batch_next_states).to(device)\n",
    "            action = torch.Tensor(batch_actions).to(device)\n",
    "            reward = torch.Tensor(batch_rewards).to(device)\n",
    "            done = torch.Tensor(batch_dones).to(device)\n",
    "            \n",
    "            # we now try and get the next action based on the next state i.e. s' and this will give us a new action i.e. a'\n",
    "            next_action = self.actor_target(next_state)\n",
    "            \n",
    "            # now we add Gaussian noise to the next action a' and we clamp it in a range of values supported by the environment\n",
    "            noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
    "            noise = noise.clamp(-noise_clip, noise_clip)\n",
    "            next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
    "            \n",
    "            # now we need to get the q-values using the critic neural networks, so we take next state (s') and next action (a')\n",
    "            # and get two q-values... refer to the Critic class's forward fucntion\n",
    "            target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
    "            \n",
    "            # now as per the model, we need the minimum of the two q-values... we need to use a torch function for this\n",
    "            target_Q = torch.min(target_Q1, target_Q2)\n",
    "            \n",
    "            # now as per the documentation, we get the final target of the two Critic models, \n",
    "            # which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
    "            # the value of min(Qt1, Qt2) has been calculated above and we got the variable target_Q, let's use that\n",
    "            # the 1 - done part is there to ensure that if we are at the done stage then the q_value should be 0 + the reward\n",
    "            target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
    "\n",
    "            # now that we get the target_Q (the target q-value) we now need to get the q-value that the model gives\n",
    "            # we get this from the critic model (not the critic target we used above) and passing \n",
    "            # the current state (s) and action (a)that takes us to the next state  \n",
    "            current_Q1, current_Q2 = self.critic(state, action)\n",
    "            \n",
    "            # now we need to compute the loss between the two q-values viz. target_Q and the two current q-values\n",
    "            critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "            \n",
    "            # now that we have the critic loss, we can now do SGD to back-propogate and optimize our weights and biases\n",
    "            # https://pytorch.org/docs/stable/optim.html\n",
    "            \n",
    "            # first we initialize the optimizer\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            \n",
    "            # backpropogate using the critic_loss \n",
    "            critic_loss.backward()\n",
    "            \n",
    "            # update the parameters of the optimizer\n",
    "            self.critic_optimizer.step()\n",
    "            \n",
    "            # now we need a way to update the actor_target weights and biases as well, we use the policy_freq value to ensure\n",
    "            # every time we hit that policy_freq number, we copy the weights and biases from the actor_model to actor_target\n",
    "            # please note it's not a complete copy and we use tau to have some difference between the two\n",
    "            if it % policy_freq == 0:\n",
    "                \n",
    "                # the following is a multipart function in the same line, first look at self.actor(state), this runs the forward\n",
    "                # function in the Actor class and gives the Action to be taken. That action then feeds into the Q1 function of \n",
    "                # the critic_model object of the critic class (note this is not critic_target, but critic model)\n",
    "                # the result is a q-value, that q-value is converted to it's negative value and gives its mean\n",
    "                actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
    "                \n",
    "                # now we update the weights and biases for the actor model using the optimizer\n",
    "                # https://pytorch.org/docs/stable/optim.html\n",
    "                \n",
    "                # first to initialize the optimizer\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                \n",
    "                # back-propogate the loss and get new values for weights and biases\n",
    "                actor_loss.backward()\n",
    "                \n",
    "                # update the parameters of the optimizer\n",
    "                self.actor_optimizer.step()\n",
    "                \n",
    "                # now let's update the weights and biases of the actor_target from actor model using polyak averaging\n",
    "                for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "                \n",
    "                # now let's update the weights and biases of the critic_target from critic model using polyak averaging\n",
    "                for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "                \n",
    "    # now we need a function that can be used to save a trained model for future use\n",
    "    def save(self, filename, directory):\n",
    "        torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
    "        torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
    "        \n",
    "\n",
    "    # now we need a function that can be used to load a trained model \n",
    "    def load(self, filename, directory):\n",
    "        self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
    "        self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bcb84a",
   "metadata": {},
   "source": [
    "## We make a function that evaluates the policy by calculating its average reward over 10 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79a4dbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(policy, eval_episodes=10):\n",
    "    \n",
    "    # first initialize the average reward to 0. (The . is to ensure it's a float type and not int)\n",
    "    avg_reward = 0.\n",
    "    \n",
    "    # now let's iterate the number of times defined by eval_episodes to get the average reward\n",
    "    for _ in range(eval_episodes):\n",
    "        # reset the environment\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = policy.select_action(np.array(obs)) # you will find the select_action function in the TD3 class\n",
    "            obs, reward, done, _ = env.step(action) # this step actually takes the action and gets the new state and reward\n",
    "            avg_reward += reward\n",
    "    \n",
    "    # avg reward till now is actually a cumulative of all rewards, we divide it by the episodes to get the average\n",
    "    avg_reward /= eval_episodes\n",
    "    \n",
    "    print('--------------------------------------------------------')\n",
    "    print('Average Reward over the Evaluation Step: %f' % (avg_reward))\n",
    "    print('--------------------------------------------------------')\n",
    "    \n",
    "    return avg_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0110b8c3",
   "metadata": {},
   "source": [
    "## We set the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a3db44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"Pendulum-v1\" # Name of a environment (set it to any Continous environment you want)\n",
    "seed = 0 # Random seed number\n",
    "start_timesteps = 1e4 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
    "eval_freq = 5e3 # How often the evaluation step is performed (after how many timesteps)\n",
    "max_timesteps = 5e5 # Total number of iterations/timesteps\n",
    "save_models = True # Boolean checker whether or not to save the pre-trained model\n",
    "expl_noise = 0.1 # Exploration noise - STD value of exploration Gaussian noise\n",
    "batch_size = 100 # Size of the batch\n",
    "discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
    "tau = 0.005 # Target network update rate\n",
    "policy_noise = 0.2 # STD of Gaussian noise added to the actions for the exploration purposes\n",
    "noise_clip = 0.5 # Maximum value of the Gaussian noise added to the actions (policy)\n",
    "policy_freq = 2 # Number of iterations to wait before the policy network (Actor model) is updated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e16f2d7",
   "metadata": {},
   "source": [
    "## We create a file name for the two saved models: the Actor and Critic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0a83994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Settings: TD3_Pendulum-v1_0\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
    "print (\"---------------------------------------\")\n",
    "print (\"Settings: %s\" % (file_name))\n",
    "print (\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298f2fee",
   "metadata": {},
   "source": [
    "## We create a folder inside which will be saved the trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae5eb87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./results\"):\n",
    "  os.makedirs(\"./results\")\n",
    "if save_models and not os.path.exists(\"./pytorch_models\"):\n",
    "  os.makedirs(\"./pytorch_models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6d6455",
   "metadata": {},
   "source": [
    "## We create the gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9087246",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d34fa5",
   "metadata": {},
   "source": [
    "## We set seeds and we get the necessary information on the states and actions in the chosen environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3289273b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d0490c",
   "metadata": {},
   "source": [
    "## We create the policy network (the Actor model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89cb6ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = TD3(state_dim, action_dim, max_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f02a1f5",
   "metadata": {},
   "source": [
    "## We create the Experience Replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a1e9ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dc0a73",
   "metadata": {},
   "source": [
    "## We define a list where all the evaluation results over 10 episodes are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "803dc689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -1400.479691\n",
      "--------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "evaluations = [evaluate_policy(policy)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be415e6c",
   "metadata": {},
   "source": [
    "## We create a new folder directory in which the final results (videos of the agent) will be populated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5543524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir(base, name):\n",
    "    path = os.path.join(base, name)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    return path\n",
    "work_dir = mkdir('exp', 'brs')\n",
    "monitor_dir = mkdir(work_dir, 'monitor')\n",
    "max_episode_steps = env._max_episode_steps\n",
    "save_env_vid = False\n",
    "if save_env_vid:\n",
    "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
    "  env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafddeb9",
   "metadata": {},
   "source": [
    "## We initialize the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85fe79fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_timesteps = 0\n",
    "timesteps_since_eval = 0\n",
    "episode_num = 0\n",
    "done = True\n",
    "t0 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ca303f",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33b374a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps not 0\n",
      "Total Timesteps: 200 Episode Num: 1 Reward: -1289.9048630392751\n",
      "timesteps not 0\n",
      "Total Timesteps: 400 Episode Num: 2 Reward: -863.1462610238716\n",
      "timesteps not 0\n",
      "Total Timesteps: 600 Episode Num: 3 Reward: -1459.964404998601\n",
      "timesteps not 0\n",
      "Total Timesteps: 800 Episode Num: 4 Reward: -1220.8036976946337\n",
      "timesteps not 0\n",
      "Total Timesteps: 1000 Episode Num: 5 Reward: -933.5313329243754\n",
      "timesteps not 0\n",
      "Total Timesteps: 1200 Episode Num: 6 Reward: -1051.1878104786335\n",
      "timesteps not 0\n",
      "Total Timesteps: 1400 Episode Num: 7 Reward: -1106.930946054087\n",
      "timesteps not 0\n",
      "Total Timesteps: 1600 Episode Num: 8 Reward: -896.372327948861\n",
      "timesteps not 0\n",
      "Total Timesteps: 1800 Episode Num: 9 Reward: -1405.8152725102436\n",
      "timesteps not 0\n",
      "Total Timesteps: 2000 Episode Num: 10 Reward: -954.734615665277\n",
      "timesteps not 0\n",
      "Total Timesteps: 2200 Episode Num: 11 Reward: -1066.882866694956\n",
      "timesteps not 0\n",
      "Total Timesteps: 2400 Episode Num: 12 Reward: -1829.7846441642628\n",
      "timesteps not 0\n",
      "Total Timesteps: 2600 Episode Num: 13 Reward: -1766.3361799738807\n",
      "timesteps not 0\n",
      "Total Timesteps: 2800 Episode Num: 14 Reward: -1609.4019418708954\n",
      "timesteps not 0\n",
      "Total Timesteps: 3000 Episode Num: 15 Reward: -902.1711664061363\n",
      "timesteps not 0\n",
      "Total Timesteps: 3200 Episode Num: 16 Reward: -1382.727446405198\n",
      "timesteps not 0\n",
      "Total Timesteps: 3400 Episode Num: 17 Reward: -1431.8782954648373\n",
      "timesteps not 0\n",
      "Total Timesteps: 3600 Episode Num: 18 Reward: -1278.8315946172543\n",
      "timesteps not 0\n",
      "Total Timesteps: 3800 Episode Num: 19 Reward: -1067.6510255655842\n",
      "timesteps not 0\n",
      "Total Timesteps: 4000 Episode Num: 20 Reward: -1608.3046561513027\n",
      "timesteps not 0\n",
      "Total Timesteps: 4200 Episode Num: 21 Reward: -1306.7365851244913\n",
      "timesteps not 0\n",
      "Total Timesteps: 4400 Episode Num: 22 Reward: -1503.6933878176749\n",
      "timesteps not 0\n",
      "Total Timesteps: 4600 Episode Num: 23 Reward: -882.4738942021096\n",
      "timesteps not 0\n",
      "Total Timesteps: 4800 Episode Num: 24 Reward: -1210.895769647048\n",
      "timesteps not 0\n",
      "Total Timesteps: 5000 Episode Num: 25 Reward: -1678.1684803028884\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -204.231996\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 5200 Episode Num: 26 Reward: -1094.4948453966917\n",
      "timesteps not 0\n",
      "Total Timesteps: 5400 Episode Num: 27 Reward: -1292.2478788041278\n",
      "timesteps not 0\n",
      "Total Timesteps: 5600 Episode Num: 28 Reward: -938.4862694644063\n",
      "timesteps not 0\n",
      "Total Timesteps: 5800 Episode Num: 29 Reward: -762.6334117802132\n",
      "timesteps not 0\n",
      "Total Timesteps: 6000 Episode Num: 30 Reward: -758.2240861520577\n",
      "timesteps not 0\n",
      "Total Timesteps: 6200 Episode Num: 31 Reward: -1570.8487659731086\n",
      "timesteps not 0\n",
      "Total Timesteps: 6400 Episode Num: 32 Reward: -882.7301719074197\n",
      "timesteps not 0\n",
      "Total Timesteps: 6600 Episode Num: 33 Reward: -1216.3778092045\n",
      "timesteps not 0\n",
      "Total Timesteps: 6800 Episode Num: 34 Reward: -1386.6174272802118\n",
      "timesteps not 0\n",
      "Total Timesteps: 7000 Episode Num: 35 Reward: -1174.4944452429681\n",
      "timesteps not 0\n",
      "Total Timesteps: 7200 Episode Num: 36 Reward: -1438.5186688918395\n",
      "timesteps not 0\n",
      "Total Timesteps: 7400 Episode Num: 37 Reward: -1007.4264326854427\n",
      "timesteps not 0\n",
      "Total Timesteps: 7600 Episode Num: 38 Reward: -1282.5266309566332\n",
      "timesteps not 0\n",
      "Total Timesteps: 7800 Episode Num: 39 Reward: -1317.7387080427657\n",
      "timesteps not 0\n",
      "Total Timesteps: 8000 Episode Num: 40 Reward: -1262.5240002660025\n",
      "timesteps not 0\n",
      "Total Timesteps: 8200 Episode Num: 41 Reward: -1090.1599804614307\n",
      "timesteps not 0\n",
      "Total Timesteps: 8400 Episode Num: 42 Reward: -1075.268039600186\n",
      "timesteps not 0\n",
      "Total Timesteps: 8600 Episode Num: 43 Reward: -1321.3098488280464\n",
      "timesteps not 0\n",
      "Total Timesteps: 8800 Episode Num: 44 Reward: -1100.9973143516117\n",
      "timesteps not 0\n",
      "Total Timesteps: 9000 Episode Num: 45 Reward: -1173.2210534006263\n",
      "timesteps not 0\n",
      "Total Timesteps: 9200 Episode Num: 46 Reward: -872.4918303839246\n",
      "timesteps not 0\n",
      "Total Timesteps: 9400 Episode Num: 47 Reward: -966.8422104185206\n",
      "timesteps not 0\n",
      "Total Timesteps: 9600 Episode Num: 48 Reward: -1503.831468205522\n",
      "timesteps not 0\n",
      "Total Timesteps: 9800 Episode Num: 49 Reward: -1598.5499249587492\n",
      "timesteps not 0\n",
      "Total Timesteps: 10000 Episode Num: 50 Reward: -1315.1188347866732\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -106.595097\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 10200 Episode Num: 51 Reward: -240.40167575264272\n",
      "timesteps not 0\n",
      "Total Timesteps: 10400 Episode Num: 52 Reward: -5.799521550824088\n",
      "timesteps not 0\n",
      "Total Timesteps: 10600 Episode Num: 53 Reward: -329.67263398565956\n",
      "timesteps not 0\n",
      "Total Timesteps: 10800 Episode Num: 54 Reward: -131.4404166477448\n",
      "timesteps not 0\n",
      "Total Timesteps: 11000 Episode Num: 55 Reward: -121.10887525785554\n",
      "timesteps not 0\n",
      "Total Timesteps: 11200 Episode Num: 56 Reward: -124.49196037235649\n",
      "timesteps not 0\n",
      "Total Timesteps: 11400 Episode Num: 57 Reward: -324.6728042177747\n",
      "timesteps not 0\n",
      "Total Timesteps: 11600 Episode Num: 58 Reward: -116.83783858034344\n",
      "timesteps not 0\n",
      "Total Timesteps: 11800 Episode Num: 59 Reward: -127.270634741749\n",
      "timesteps not 0\n",
      "Total Timesteps: 12000 Episode Num: 60 Reward: -119.78889779553974\n",
      "timesteps not 0\n",
      "Total Timesteps: 12200 Episode Num: 61 Reward: -124.47569554828677\n",
      "timesteps not 0\n",
      "Total Timesteps: 12400 Episode Num: 62 Reward: -122.37760569728937\n",
      "timesteps not 0\n",
      "Total Timesteps: 12600 Episode Num: 63 Reward: -117.3323775521836\n",
      "timesteps not 0\n",
      "Total Timesteps: 12800 Episode Num: 64 Reward: -127.31550992376118\n",
      "timesteps not 0\n",
      "Total Timesteps: 13000 Episode Num: 65 Reward: -2.389056140806235\n",
      "timesteps not 0\n",
      "Total Timesteps: 13200 Episode Num: 66 Reward: -287.5258813710558\n",
      "timesteps not 0\n",
      "Total Timesteps: 13400 Episode Num: 67 Reward: -121.51540548933782\n",
      "timesteps not 0\n",
      "Total Timesteps: 13600 Episode Num: 68 Reward: -118.7877115254259\n",
      "timesteps not 0\n",
      "Total Timesteps: 13800 Episode Num: 69 Reward: -120.83404058751393\n",
      "timesteps not 0\n",
      "Total Timesteps: 14000 Episode Num: 70 Reward: -225.32719823974136\n",
      "timesteps not 0\n",
      "Total Timesteps: 14200 Episode Num: 71 Reward: -118.80966635752985\n",
      "timesteps not 0\n",
      "Total Timesteps: 14400 Episode Num: 72 Reward: -116.50251275691369\n",
      "timesteps not 0\n",
      "Total Timesteps: 14600 Episode Num: 73 Reward: -319.86969514233067\n",
      "timesteps not 0\n",
      "Total Timesteps: 14800 Episode Num: 74 Reward: -242.92791396983742\n",
      "timesteps not 0\n",
      "Total Timesteps: 15000 Episode Num: 75 Reward: -242.3037387483968\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -155.092132\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 15200 Episode Num: 76 Reward: -222.87277068747315\n",
      "timesteps not 0\n",
      "Total Timesteps: 15400 Episode Num: 77 Reward: -243.82509866838092\n",
      "timesteps not 0\n",
      "Total Timesteps: 15600 Episode Num: 78 Reward: -124.77024042978947\n",
      "timesteps not 0\n",
      "Total Timesteps: 15800 Episode Num: 79 Reward: -118.32269600886357\n",
      "timesteps not 0\n",
      "Total Timesteps: 16000 Episode Num: 80 Reward: -127.03507914338968\n",
      "timesteps not 0\n",
      "Total Timesteps: 16200 Episode Num: 81 Reward: -248.05331831390802\n",
      "timesteps not 0\n",
      "Total Timesteps: 16400 Episode Num: 82 Reward: -118.6831866709117\n",
      "timesteps not 0\n",
      "Total Timesteps: 16600 Episode Num: 83 Reward: -118.47679584388939\n",
      "timesteps not 0\n",
      "Total Timesteps: 16800 Episode Num: 84 Reward: -256.8136006554985\n",
      "timesteps not 0\n",
      "Total Timesteps: 17000 Episode Num: 85 Reward: -122.02383518937077\n",
      "timesteps not 0\n",
      "Total Timesteps: 17200 Episode Num: 86 Reward: -245.39029615508161\n",
      "timesteps not 0\n",
      "Total Timesteps: 17400 Episode Num: 87 Reward: -129.05414386456442\n",
      "timesteps not 0\n",
      "Total Timesteps: 17600 Episode Num: 88 Reward: -127.68683639640464\n",
      "timesteps not 0\n",
      "Total Timesteps: 17800 Episode Num: 89 Reward: -122.8920577568231\n",
      "timesteps not 0\n",
      "Total Timesteps: 18000 Episode Num: 90 Reward: -220.2924703052079\n",
      "timesteps not 0\n",
      "Total Timesteps: 18200 Episode Num: 91 Reward: -2.020084834742634\n",
      "timesteps not 0\n",
      "Total Timesteps: 18400 Episode Num: 92 Reward: -230.48043329997802\n",
      "timesteps not 0\n",
      "Total Timesteps: 18600 Episode Num: 93 Reward: -128.01173540467798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps not 0\n",
      "Total Timesteps: 18800 Episode Num: 94 Reward: -117.41923239265073\n",
      "timesteps not 0\n",
      "Total Timesteps: 19000 Episode Num: 95 Reward: -240.5224746545498\n",
      "timesteps not 0\n",
      "Total Timesteps: 19200 Episode Num: 96 Reward: -288.3300396804313\n",
      "timesteps not 0\n",
      "Total Timesteps: 19400 Episode Num: 97 Reward: -117.25058966192036\n",
      "timesteps not 0\n",
      "Total Timesteps: 19600 Episode Num: 98 Reward: -228.43820101843684\n",
      "timesteps not 0\n",
      "Total Timesteps: 19800 Episode Num: 99 Reward: -120.5016277629517\n",
      "timesteps not 0\n",
      "Total Timesteps: 20000 Episode Num: 100 Reward: -225.37586979560376\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -135.414201\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 20200 Episode Num: 101 Reward: -238.9532640323789\n",
      "timesteps not 0\n",
      "Total Timesteps: 20400 Episode Num: 102 Reward: -228.84188849863813\n",
      "timesteps not 0\n",
      "Total Timesteps: 20600 Episode Num: 103 Reward: -2.9155123281977264\n",
      "timesteps not 0\n",
      "Total Timesteps: 20800 Episode Num: 104 Reward: -115.29211499496634\n",
      "timesteps not 0\n",
      "Total Timesteps: 21000 Episode Num: 105 Reward: -121.39312777945423\n",
      "timesteps not 0\n",
      "Total Timesteps: 21200 Episode Num: 106 Reward: -251.32307019114353\n",
      "timesteps not 0\n",
      "Total Timesteps: 21400 Episode Num: 107 Reward: -115.9367389955682\n",
      "timesteps not 0\n",
      "Total Timesteps: 21600 Episode Num: 108 Reward: -124.18761021080178\n",
      "timesteps not 0\n",
      "Total Timesteps: 21800 Episode Num: 109 Reward: -117.6990137606553\n",
      "timesteps not 0\n",
      "Total Timesteps: 22000 Episode Num: 110 Reward: -125.57666954528952\n",
      "timesteps not 0\n",
      "Total Timesteps: 22200 Episode Num: 111 Reward: -224.2484686147592\n",
      "timesteps not 0\n",
      "Total Timesteps: 22400 Episode Num: 112 Reward: -121.10936707787694\n",
      "timesteps not 0\n",
      "Total Timesteps: 22600 Episode Num: 113 Reward: -332.9006445856468\n",
      "timesteps not 0\n",
      "Total Timesteps: 22800 Episode Num: 114 Reward: -120.85705980550082\n",
      "timesteps not 0\n",
      "Total Timesteps: 23000 Episode Num: 115 Reward: -122.12503197121292\n",
      "timesteps not 0\n",
      "Total Timesteps: 23200 Episode Num: 116 Reward: -128.31677977020712\n",
      "timesteps not 0\n",
      "Total Timesteps: 23400 Episode Num: 117 Reward: -120.45913889120884\n",
      "timesteps not 0\n",
      "Total Timesteps: 23600 Episode Num: 118 Reward: -338.0182611366423\n",
      "timesteps not 0\n",
      "Total Timesteps: 23800 Episode Num: 119 Reward: -121.0186556557222\n",
      "timesteps not 0\n",
      "Total Timesteps: 24000 Episode Num: 120 Reward: -120.50494690763647\n",
      "timesteps not 0\n",
      "Total Timesteps: 24200 Episode Num: 121 Reward: -227.53379548675596\n",
      "timesteps not 0\n",
      "Total Timesteps: 24400 Episode Num: 122 Reward: -127.77871636196959\n",
      "timesteps not 0\n",
      "Total Timesteps: 24600 Episode Num: 123 Reward: -123.75836450670595\n",
      "timesteps not 0\n",
      "Total Timesteps: 24800 Episode Num: 124 Reward: -313.83550931068754\n",
      "timesteps not 0\n",
      "Total Timesteps: 25000 Episode Num: 125 Reward: -120.0748364574632\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -178.359134\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 25200 Episode Num: 126 Reward: -125.17600802285173\n",
      "timesteps not 0\n",
      "Total Timesteps: 25400 Episode Num: 127 Reward: -123.65472377970654\n",
      "timesteps not 0\n",
      "Total Timesteps: 25600 Episode Num: 128 Reward: -3.8794930950358255\n",
      "timesteps not 0\n",
      "Total Timesteps: 25800 Episode Num: 129 Reward: -243.97024603795296\n",
      "timesteps not 0\n",
      "Total Timesteps: 26000 Episode Num: 130 Reward: -3.232240568406847\n",
      "timesteps not 0\n",
      "Total Timesteps: 26200 Episode Num: 131 Reward: -116.63604859532902\n",
      "timesteps not 0\n",
      "Total Timesteps: 26400 Episode Num: 132 Reward: -228.83857479848118\n",
      "timesteps not 0\n",
      "Total Timesteps: 26600 Episode Num: 133 Reward: -118.84889808785145\n",
      "timesteps not 0\n",
      "Total Timesteps: 26800 Episode Num: 134 Reward: -325.532788821641\n",
      "timesteps not 0\n",
      "Total Timesteps: 27000 Episode Num: 135 Reward: -129.23626059918942\n",
      "timesteps not 0\n",
      "Total Timesteps: 27200 Episode Num: 136 Reward: -246.98671259822578\n",
      "timesteps not 0\n",
      "Total Timesteps: 27400 Episode Num: 137 Reward: -122.24591049416398\n",
      "timesteps not 0\n",
      "Total Timesteps: 27600 Episode Num: 138 Reward: -228.65288450810039\n",
      "timesteps not 0\n",
      "Total Timesteps: 27800 Episode Num: 139 Reward: -228.66108406814425\n",
      "timesteps not 0\n",
      "Total Timesteps: 28000 Episode Num: 140 Reward: -128.85043220328484\n",
      "timesteps not 0\n",
      "Total Timesteps: 28200 Episode Num: 141 Reward: -234.69482443200482\n",
      "timesteps not 0\n",
      "Total Timesteps: 28400 Episode Num: 142 Reward: -118.59479636220267\n",
      "timesteps not 0\n",
      "Total Timesteps: 28600 Episode Num: 143 Reward: -128.78327902526212\n",
      "timesteps not 0\n",
      "Total Timesteps: 28800 Episode Num: 144 Reward: -123.11513549897175\n",
      "timesteps not 0\n",
      "Total Timesteps: 29000 Episode Num: 145 Reward: -120.45199966785293\n",
      "timesteps not 0\n",
      "Total Timesteps: 29200 Episode Num: 146 Reward: -126.73613649756336\n",
      "timesteps not 0\n",
      "Total Timesteps: 29400 Episode Num: 147 Reward: -124.40234201306872\n",
      "timesteps not 0\n",
      "Total Timesteps: 29600 Episode Num: 148 Reward: -118.78541353103246\n",
      "timesteps not 0\n",
      "Total Timesteps: 29800 Episode Num: 149 Reward: -117.89213042537338\n",
      "timesteps not 0\n",
      "Total Timesteps: 30000 Episode Num: 150 Reward: -2.7580777468961415\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -128.671904\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 30200 Episode Num: 151 Reward: -122.95337725090317\n",
      "timesteps not 0\n",
      "Total Timesteps: 30400 Episode Num: 152 Reward: -121.02099597653094\n",
      "timesteps not 0\n",
      "Total Timesteps: 30600 Episode Num: 153 Reward: -1.8090482618602697\n",
      "timesteps not 0\n",
      "Total Timesteps: 30800 Episode Num: 154 Reward: -231.25326949451224\n",
      "timesteps not 0\n",
      "Total Timesteps: 31000 Episode Num: 155 Reward: -225.87099016712676\n",
      "timesteps not 0\n",
      "Total Timesteps: 31200 Episode Num: 156 Reward: -131.13663138262845\n",
      "timesteps not 0\n",
      "Total Timesteps: 31400 Episode Num: 157 Reward: -369.39673345661595\n",
      "timesteps not 0\n",
      "Total Timesteps: 31600 Episode Num: 158 Reward: -120.90388694793756\n",
      "timesteps not 0\n",
      "Total Timesteps: 31800 Episode Num: 159 Reward: -243.40640015560328\n",
      "timesteps not 0\n",
      "Total Timesteps: 32000 Episode Num: 160 Reward: -5.6274301682003065\n",
      "timesteps not 0\n",
      "Total Timesteps: 32200 Episode Num: 161 Reward: -115.74290781590828\n",
      "timesteps not 0\n",
      "Total Timesteps: 32400 Episode Num: 162 Reward: -249.44068636878592\n",
      "timesteps not 0\n",
      "Total Timesteps: 32600 Episode Num: 163 Reward: -124.91721451616814\n",
      "timesteps not 0\n",
      "Total Timesteps: 32800 Episode Num: 164 Reward: -4.503876123059983\n",
      "timesteps not 0\n",
      "Total Timesteps: 33000 Episode Num: 165 Reward: -250.3910216170177\n",
      "timesteps not 0\n",
      "Total Timesteps: 33200 Episode Num: 166 Reward: -329.63186306847126\n",
      "timesteps not 0\n",
      "Total Timesteps: 33400 Episode Num: 167 Reward: -121.42889587532699\n",
      "timesteps not 0\n",
      "Total Timesteps: 33600 Episode Num: 168 Reward: -127.96517036402335\n",
      "timesteps not 0\n",
      "Total Timesteps: 33800 Episode Num: 169 Reward: -118.41431156520004\n",
      "timesteps not 0\n",
      "Total Timesteps: 34000 Episode Num: 170 Reward: -114.47431040927756\n",
      "timesteps not 0\n",
      "Total Timesteps: 34200 Episode Num: 171 Reward: -121.08832041207188\n",
      "timesteps not 0\n",
      "Total Timesteps: 34400 Episode Num: 172 Reward: -282.06244916989283\n",
      "timesteps not 0\n",
      "Total Timesteps: 34600 Episode Num: 173 Reward: -128.77375350978514\n",
      "timesteps not 0\n",
      "Total Timesteps: 34800 Episode Num: 174 Reward: -120.75912886492036\n",
      "timesteps not 0\n",
      "Total Timesteps: 35000 Episode Num: 175 Reward: -332.58567899811993\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -168.880963\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 35200 Episode Num: 176 Reward: -225.38998835193985\n",
      "timesteps not 0\n",
      "Total Timesteps: 35400 Episode Num: 177 Reward: -115.60692963617896\n",
      "timesteps not 0\n",
      "Total Timesteps: 35600 Episode Num: 178 Reward: -126.99853459408236\n",
      "timesteps not 0\n",
      "Total Timesteps: 35800 Episode Num: 179 Reward: -124.02718210260213\n",
      "timesteps not 0\n",
      "Total Timesteps: 36000 Episode Num: 180 Reward: -243.1110568338978\n",
      "timesteps not 0\n",
      "Total Timesteps: 36200 Episode Num: 181 Reward: -125.1660505572269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps not 0\n",
      "Total Timesteps: 36400 Episode Num: 182 Reward: -229.7156381458689\n",
      "timesteps not 0\n",
      "Total Timesteps: 36600 Episode Num: 183 Reward: -128.8626778151869\n",
      "timesteps not 0\n",
      "Total Timesteps: 36800 Episode Num: 184 Reward: -2.82750660417411\n",
      "timesteps not 0\n",
      "Total Timesteps: 37000 Episode Num: 185 Reward: -120.34828515958964\n",
      "timesteps not 0\n",
      "Total Timesteps: 37200 Episode Num: 186 Reward: -120.20335885141873\n",
      "timesteps not 0\n",
      "Total Timesteps: 37400 Episode Num: 187 Reward: -223.71667621233004\n",
      "timesteps not 0\n",
      "Total Timesteps: 37600 Episode Num: 188 Reward: -120.3340146443045\n",
      "timesteps not 0\n",
      "Total Timesteps: 37800 Episode Num: 189 Reward: -129.287869638507\n",
      "timesteps not 0\n",
      "Total Timesteps: 38000 Episode Num: 190 Reward: -125.54610983826875\n",
      "timesteps not 0\n",
      "Total Timesteps: 38200 Episode Num: 191 Reward: -354.73172030160976\n",
      "timesteps not 0\n",
      "Total Timesteps: 38400 Episode Num: 192 Reward: -2.6903150420105018\n",
      "timesteps not 0\n",
      "Total Timesteps: 38600 Episode Num: 193 Reward: -123.74763311625783\n",
      "timesteps not 0\n",
      "Total Timesteps: 38800 Episode Num: 194 Reward: -119.13356133921417\n",
      "timesteps not 0\n",
      "Total Timesteps: 39000 Episode Num: 195 Reward: -124.92426991233181\n",
      "timesteps not 0\n",
      "Total Timesteps: 39200 Episode Num: 196 Reward: -125.02844656760689\n",
      "timesteps not 0\n",
      "Total Timesteps: 39400 Episode Num: 197 Reward: -117.91951105681123\n",
      "timesteps not 0\n",
      "Total Timesteps: 39600 Episode Num: 198 Reward: -121.7766405483528\n",
      "timesteps not 0\n",
      "Total Timesteps: 39800 Episode Num: 199 Reward: -348.70344680479076\n",
      "timesteps not 0\n",
      "Total Timesteps: 40000 Episode Num: 200 Reward: -120.55906642369891\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -123.655954\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 40200 Episode Num: 201 Reward: -119.63777914980143\n",
      "timesteps not 0\n",
      "Total Timesteps: 40400 Episode Num: 202 Reward: -215.8723048429766\n",
      "timesteps not 0\n",
      "Total Timesteps: 40600 Episode Num: 203 Reward: -114.89024448182813\n",
      "timesteps not 0\n",
      "Total Timesteps: 40800 Episode Num: 204 Reward: -1.9337444169952505\n",
      "timesteps not 0\n",
      "Total Timesteps: 41000 Episode Num: 205 Reward: -127.5297520232049\n",
      "timesteps not 0\n",
      "Total Timesteps: 41200 Episode Num: 206 Reward: -115.27399412259358\n",
      "timesteps not 0\n",
      "Total Timesteps: 41400 Episode Num: 207 Reward: -2.1221454785060523\n",
      "timesteps not 0\n",
      "Total Timesteps: 41600 Episode Num: 208 Reward: -228.89103856368195\n",
      "timesteps not 0\n",
      "Total Timesteps: 41800 Episode Num: 209 Reward: -119.2564546836228\n",
      "timesteps not 0\n",
      "Total Timesteps: 42000 Episode Num: 210 Reward: -115.30753424372234\n",
      "timesteps not 0\n",
      "Total Timesteps: 42200 Episode Num: 211 Reward: -124.4661597034762\n",
      "timesteps not 0\n",
      "Total Timesteps: 42400 Episode Num: 212 Reward: -222.46023692194268\n",
      "timesteps not 0\n",
      "Total Timesteps: 42600 Episode Num: 213 Reward: -116.27232254254685\n",
      "timesteps not 0\n",
      "Total Timesteps: 42800 Episode Num: 214 Reward: -121.15062069980637\n",
      "timesteps not 0\n",
      "Total Timesteps: 43000 Episode Num: 215 Reward: -236.74009666996108\n",
      "timesteps not 0\n",
      "Total Timesteps: 43200 Episode Num: 216 Reward: -120.98808295704158\n",
      "timesteps not 0\n",
      "Total Timesteps: 43400 Episode Num: 217 Reward: -126.6495379371096\n",
      "timesteps not 0\n",
      "Total Timesteps: 43600 Episode Num: 218 Reward: -3.878462311862448\n",
      "timesteps not 0\n",
      "Total Timesteps: 43800 Episode Num: 219 Reward: -121.21453938625874\n",
      "timesteps not 0\n",
      "Total Timesteps: 44000 Episode Num: 220 Reward: -129.532576330588\n",
      "timesteps not 0\n",
      "Total Timesteps: 44200 Episode Num: 221 Reward: -126.00563696656103\n",
      "timesteps not 0\n",
      "Total Timesteps: 44400 Episode Num: 222 Reward: -120.16958889703103\n",
      "timesteps not 0\n",
      "Total Timesteps: 44600 Episode Num: 223 Reward: -229.5069725009918\n",
      "timesteps not 0\n",
      "Total Timesteps: 44800 Episode Num: 224 Reward: -127.69231549207973\n",
      "timesteps not 0\n",
      "Total Timesteps: 45000 Episode Num: 225 Reward: -242.91455293878244\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -129.521617\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 45200 Episode Num: 226 Reward: -125.55354134850651\n",
      "timesteps not 0\n",
      "Total Timesteps: 45400 Episode Num: 227 Reward: -307.8030154270517\n",
      "timesteps not 0\n",
      "Total Timesteps: 45600 Episode Num: 228 Reward: -1.509208185968731\n",
      "timesteps not 0\n",
      "Total Timesteps: 45800 Episode Num: 229 Reward: -2.173070162701324\n",
      "timesteps not 0\n",
      "Total Timesteps: 46000 Episode Num: 230 Reward: -130.93889132628934\n",
      "timesteps not 0\n",
      "Total Timesteps: 46200 Episode Num: 231 Reward: -118.49916896116501\n",
      "timesteps not 0\n",
      "Total Timesteps: 46400 Episode Num: 232 Reward: -125.74619628911468\n",
      "timesteps not 0\n",
      "Total Timesteps: 46600 Episode Num: 233 Reward: -247.26631488822855\n",
      "timesteps not 0\n",
      "Total Timesteps: 46800 Episode Num: 234 Reward: -116.23986896016925\n",
      "timesteps not 0\n",
      "Total Timesteps: 47000 Episode Num: 235 Reward: -124.35568591918634\n",
      "timesteps not 0\n",
      "Total Timesteps: 47200 Episode Num: 236 Reward: -128.1753056804596\n",
      "timesteps not 0\n",
      "Total Timesteps: 47400 Episode Num: 237 Reward: -115.32927747992271\n",
      "timesteps not 0\n",
      "Total Timesteps: 47600 Episode Num: 238 Reward: -118.79079365386454\n",
      "timesteps not 0\n",
      "Total Timesteps: 47800 Episode Num: 239 Reward: -117.94792191623591\n",
      "timesteps not 0\n",
      "Total Timesteps: 48000 Episode Num: 240 Reward: -238.4382466971533\n",
      "timesteps not 0\n",
      "Total Timesteps: 48200 Episode Num: 241 Reward: -216.42748311507904\n",
      "timesteps not 0\n",
      "Total Timesteps: 48400 Episode Num: 242 Reward: -128.61141721074372\n",
      "timesteps not 0\n",
      "Total Timesteps: 48600 Episode Num: 243 Reward: -232.93093876624172\n",
      "timesteps not 0\n",
      "Total Timesteps: 48800 Episode Num: 244 Reward: -228.39433670248232\n",
      "timesteps not 0\n",
      "Total Timesteps: 49000 Episode Num: 245 Reward: -117.32892794996496\n",
      "timesteps not 0\n",
      "Total Timesteps: 49200 Episode Num: 246 Reward: -131.23454179604886\n",
      "timesteps not 0\n",
      "Total Timesteps: 49400 Episode Num: 247 Reward: -228.40495384122252\n",
      "timesteps not 0\n",
      "Total Timesteps: 49600 Episode Num: 248 Reward: -227.8248407092446\n",
      "timesteps not 0\n",
      "Total Timesteps: 49800 Episode Num: 249 Reward: -119.32239584115088\n",
      "timesteps not 0\n",
      "Total Timesteps: 50000 Episode Num: 250 Reward: -125.78055953774907\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -149.055764\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 50200 Episode Num: 251 Reward: -225.08145227734607\n",
      "timesteps not 0\n",
      "Total Timesteps: 50400 Episode Num: 252 Reward: -124.2533831223084\n",
      "timesteps not 0\n",
      "Total Timesteps: 50600 Episode Num: 253 Reward: -3.236400709347318\n",
      "timesteps not 0\n",
      "Total Timesteps: 50800 Episode Num: 254 Reward: -227.02116935366706\n",
      "timesteps not 0\n",
      "Total Timesteps: 51000 Episode Num: 255 Reward: -0.8370007333185988\n",
      "timesteps not 0\n",
      "Total Timesteps: 51200 Episode Num: 256 Reward: -126.68231376297109\n",
      "timesteps not 0\n",
      "Total Timesteps: 51400 Episode Num: 257 Reward: -115.87394624408525\n",
      "timesteps not 0\n",
      "Total Timesteps: 51600 Episode Num: 258 Reward: -1.0657301667276309\n",
      "timesteps not 0\n",
      "Total Timesteps: 51800 Episode Num: 259 Reward: -125.40479889298548\n",
      "timesteps not 0\n",
      "Total Timesteps: 52000 Episode Num: 260 Reward: -235.22261813559183\n",
      "timesteps not 0\n",
      "Total Timesteps: 52200 Episode Num: 261 Reward: -224.70835571697202\n",
      "timesteps not 0\n",
      "Total Timesteps: 52400 Episode Num: 262 Reward: -115.69111040812543\n",
      "timesteps not 0\n",
      "Total Timesteps: 52600 Episode Num: 263 Reward: -223.39427906047086\n",
      "timesteps not 0\n",
      "Total Timesteps: 52800 Episode Num: 264 Reward: -122.45251689821062\n",
      "timesteps not 0\n",
      "Total Timesteps: 53000 Episode Num: 265 Reward: -244.97700784385214\n",
      "timesteps not 0\n",
      "Total Timesteps: 53200 Episode Num: 266 Reward: -226.32079448735834\n",
      "timesteps not 0\n",
      "Total Timesteps: 53400 Episode Num: 267 Reward: -127.1208191466701\n",
      "timesteps not 0\n",
      "Total Timesteps: 53600 Episode Num: 268 Reward: -304.85886911711117\n",
      "timesteps not 0\n",
      "Total Timesteps: 53800 Episode Num: 269 Reward: -128.00970554428926\n",
      "timesteps not 0\n",
      "Total Timesteps: 54000 Episode Num: 270 Reward: -223.9959209410944\n",
      "timesteps not 0\n",
      "Total Timesteps: 54200 Episode Num: 271 Reward: -219.5255135187576\n",
      "timesteps not 0\n",
      "Total Timesteps: 54400 Episode Num: 272 Reward: -237.3171133386181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps not 0\n",
      "Total Timesteps: 54600 Episode Num: 273 Reward: -128.0210673100832\n",
      "timesteps not 0\n",
      "Total Timesteps: 54800 Episode Num: 274 Reward: -120.68422541602753\n",
      "timesteps not 0\n",
      "Total Timesteps: 55000 Episode Num: 275 Reward: -121.79406480350187\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -148.505864\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 55200 Episode Num: 276 Reward: -119.05875708269804\n",
      "timesteps not 0\n",
      "Total Timesteps: 55400 Episode Num: 277 Reward: -234.14620458684087\n",
      "timesteps not 0\n",
      "Total Timesteps: 55600 Episode Num: 278 Reward: -226.42976239367866\n",
      "timesteps not 0\n",
      "Total Timesteps: 55800 Episode Num: 279 Reward: -114.2557913159124\n",
      "timesteps not 0\n",
      "Total Timesteps: 56000 Episode Num: 280 Reward: -124.93299557534186\n",
      "timesteps not 0\n",
      "Total Timesteps: 56200 Episode Num: 281 Reward: -124.82863046754458\n",
      "timesteps not 0\n",
      "Total Timesteps: 56400 Episode Num: 282 Reward: -127.46737929712464\n",
      "timesteps not 0\n",
      "Total Timesteps: 56600 Episode Num: 283 Reward: -122.02822856362542\n",
      "timesteps not 0\n",
      "Total Timesteps: 56800 Episode Num: 284 Reward: -0.07263645376085026\n",
      "timesteps not 0\n",
      "Total Timesteps: 57000 Episode Num: 285 Reward: -228.3519071514938\n",
      "timesteps not 0\n",
      "Total Timesteps: 57200 Episode Num: 286 Reward: -118.02214257498032\n",
      "timesteps not 0\n",
      "Total Timesteps: 57400 Episode Num: 287 Reward: -221.87545994318702\n",
      "timesteps not 0\n",
      "Total Timesteps: 57600 Episode Num: 288 Reward: -0.4268745774281356\n",
      "timesteps not 0\n",
      "Total Timesteps: 57800 Episode Num: 289 Reward: -116.3798702145787\n",
      "timesteps not 0\n",
      "Total Timesteps: 58000 Episode Num: 290 Reward: -242.53967725279617\n",
      "timesteps not 0\n",
      "Total Timesteps: 58200 Episode Num: 291 Reward: -120.3193588683295\n",
      "timesteps not 0\n",
      "Total Timesteps: 58400 Episode Num: 292 Reward: -116.46581870307661\n",
      "timesteps not 0\n",
      "Total Timesteps: 58600 Episode Num: 293 Reward: -120.74944040486811\n",
      "timesteps not 0\n",
      "Total Timesteps: 58800 Episode Num: 294 Reward: -229.86628164161039\n",
      "timesteps not 0\n",
      "Total Timesteps: 59000 Episode Num: 295 Reward: -124.1768789789907\n",
      "timesteps not 0\n",
      "Total Timesteps: 59200 Episode Num: 296 Reward: -116.18389153691052\n",
      "timesteps not 0\n",
      "Total Timesteps: 59400 Episode Num: 297 Reward: -125.20192528565227\n",
      "timesteps not 0\n",
      "Total Timesteps: 59600 Episode Num: 298 Reward: -377.54819672560285\n",
      "timesteps not 0\n",
      "Total Timesteps: 59800 Episode Num: 299 Reward: -116.29888291520587\n",
      "timesteps not 0\n",
      "Total Timesteps: 60000 Episode Num: 300 Reward: -227.60688240974724\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -97.523503\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 60200 Episode Num: 301 Reward: -124.3339507300293\n",
      "timesteps not 0\n",
      "Total Timesteps: 60400 Episode Num: 302 Reward: -220.9143452754272\n",
      "timesteps not 0\n",
      "Total Timesteps: 60600 Episode Num: 303 Reward: -0.867836332810777\n",
      "timesteps not 0\n",
      "Total Timesteps: 60800 Episode Num: 304 Reward: -232.43226866219982\n",
      "timesteps not 0\n",
      "Total Timesteps: 61000 Episode Num: 305 Reward: -116.11096215668628\n",
      "timesteps not 0\n",
      "Total Timesteps: 61200 Episode Num: 306 Reward: -122.57027830790445\n",
      "timesteps not 0\n",
      "Total Timesteps: 61400 Episode Num: 307 Reward: -120.44293911744253\n",
      "timesteps not 0\n",
      "Total Timesteps: 61600 Episode Num: 308 Reward: -119.97873226365652\n",
      "timesteps not 0\n",
      "Total Timesteps: 61800 Episode Num: 309 Reward: -114.76118389901758\n",
      "timesteps not 0\n",
      "Total Timesteps: 62000 Episode Num: 310 Reward: -231.64132135603813\n",
      "timesteps not 0\n",
      "Total Timesteps: 62200 Episode Num: 311 Reward: -120.13148987357539\n",
      "timesteps not 0\n",
      "Total Timesteps: 62400 Episode Num: 312 Reward: -126.00534488535142\n",
      "timesteps not 0\n",
      "Total Timesteps: 62600 Episode Num: 313 Reward: -221.7613873268115\n",
      "timesteps not 0\n",
      "Total Timesteps: 62800 Episode Num: 314 Reward: -120.67254921851342\n",
      "timesteps not 0\n",
      "Total Timesteps: 63000 Episode Num: 315 Reward: -229.4405900096525\n",
      "timesteps not 0\n",
      "Total Timesteps: 63200 Episode Num: 316 Reward: -116.29977877017416\n",
      "timesteps not 0\n",
      "Total Timesteps: 63400 Episode Num: 317 Reward: -234.11208452822098\n",
      "timesteps not 0\n",
      "Total Timesteps: 63600 Episode Num: 318 Reward: -118.30471641909442\n",
      "timesteps not 0\n",
      "Total Timesteps: 63800 Episode Num: 319 Reward: -215.90594205186954\n",
      "timesteps not 0\n",
      "Total Timesteps: 64000 Episode Num: 320 Reward: -119.152201376485\n",
      "timesteps not 0\n",
      "Total Timesteps: 64200 Episode Num: 321 Reward: -246.7350881446921\n",
      "timesteps not 0\n",
      "Total Timesteps: 64400 Episode Num: 322 Reward: -127.36069054231149\n",
      "timesteps not 0\n",
      "Total Timesteps: 64600 Episode Num: 323 Reward: -123.83435599903247\n",
      "timesteps not 0\n",
      "Total Timesteps: 64800 Episode Num: 324 Reward: -1.548782888315096\n",
      "timesteps not 0\n",
      "Total Timesteps: 65000 Episode Num: 325 Reward: -119.51208048406632\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -130.967509\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 65200 Episode Num: 326 Reward: -128.44867055395767\n",
      "timesteps not 0\n",
      "Total Timesteps: 65400 Episode Num: 327 Reward: -130.36105692457227\n",
      "timesteps not 0\n",
      "Total Timesteps: 65600 Episode Num: 328 Reward: -0.357019939686212\n",
      "timesteps not 0\n",
      "Total Timesteps: 65800 Episode Num: 329 Reward: -232.1993382049967\n",
      "timesteps not 0\n",
      "Total Timesteps: 66000 Episode Num: 330 Reward: -123.93428475431891\n",
      "timesteps not 0\n",
      "Total Timesteps: 66200 Episode Num: 331 Reward: -234.84509820118512\n",
      "timesteps not 0\n",
      "Total Timesteps: 66400 Episode Num: 332 Reward: -114.02343267003391\n",
      "timesteps not 0\n",
      "Total Timesteps: 66600 Episode Num: 333 Reward: -127.21176921197578\n",
      "timesteps not 0\n",
      "Total Timesteps: 66800 Episode Num: 334 Reward: -113.2954285801374\n",
      "timesteps not 0\n",
      "Total Timesteps: 67000 Episode Num: 335 Reward: -113.54147334578663\n",
      "timesteps not 0\n",
      "Total Timesteps: 67200 Episode Num: 336 Reward: -2.360916821092846\n",
      "timesteps not 0\n",
      "Total Timesteps: 67400 Episode Num: 337 Reward: -391.3337849024289\n",
      "timesteps not 0\n",
      "Total Timesteps: 67600 Episode Num: 338 Reward: -124.41951424612117\n",
      "timesteps not 0\n",
      "Total Timesteps: 67800 Episode Num: 339 Reward: -120.15601143325696\n",
      "timesteps not 0\n",
      "Total Timesteps: 68000 Episode Num: 340 Reward: -233.43232533285493\n",
      "timesteps not 0\n",
      "Total Timesteps: 68200 Episode Num: 341 Reward: -114.12599740321602\n",
      "timesteps not 0\n",
      "Total Timesteps: 68400 Episode Num: 342 Reward: -128.27239189968972\n",
      "timesteps not 0\n",
      "Total Timesteps: 68600 Episode Num: 343 Reward: -125.91640323894255\n",
      "timesteps not 0\n",
      "Total Timesteps: 68800 Episode Num: 344 Reward: -229.0548507255094\n",
      "timesteps not 0\n",
      "Total Timesteps: 69000 Episode Num: 345 Reward: -242.50506552282178\n",
      "timesteps not 0\n",
      "Total Timesteps: 69200 Episode Num: 346 Reward: -118.99437162464012\n",
      "timesteps not 0\n",
      "Total Timesteps: 69400 Episode Num: 347 Reward: -125.73634571832137\n",
      "timesteps not 0\n",
      "Total Timesteps: 69600 Episode Num: 348 Reward: -117.2587612280767\n",
      "timesteps not 0\n",
      "Total Timesteps: 69800 Episode Num: 349 Reward: -229.14128423968694\n",
      "timesteps not 0\n",
      "Total Timesteps: 70000 Episode Num: 350 Reward: -117.26507768061077\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -132.776732\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 70200 Episode Num: 351 Reward: -113.986641454247\n",
      "timesteps not 0\n",
      "Total Timesteps: 70400 Episode Num: 352 Reward: -228.42108535775733\n",
      "timesteps not 0\n",
      "Total Timesteps: 70600 Episode Num: 353 Reward: -221.56028059048583\n",
      "timesteps not 0\n",
      "Total Timesteps: 70800 Episode Num: 354 Reward: -231.82927609338253\n",
      "timesteps not 0\n",
      "Total Timesteps: 71000 Episode Num: 355 Reward: -128.4327612816824\n",
      "timesteps not 0\n",
      "Total Timesteps: 71200 Episode Num: 356 Reward: -120.69602571975761\n",
      "timesteps not 0\n",
      "Total Timesteps: 71400 Episode Num: 357 Reward: -127.3969577221222\n",
      "timesteps not 0\n",
      "Total Timesteps: 71600 Episode Num: 358 Reward: -127.20816053578176\n",
      "timesteps not 0\n",
      "Total Timesteps: 71800 Episode Num: 359 Reward: -239.71773045306685\n",
      "timesteps not 0\n",
      "Total Timesteps: 72000 Episode Num: 360 Reward: -127.16318599860722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps not 0\n",
      "Total Timesteps: 72200 Episode Num: 361 Reward: -124.34096883871105\n",
      "timesteps not 0\n",
      "Total Timesteps: 72400 Episode Num: 362 Reward: -129.5428378679373\n",
      "timesteps not 0\n",
      "Total Timesteps: 72600 Episode Num: 363 Reward: -235.69718921534158\n",
      "timesteps not 0\n",
      "Total Timesteps: 72800 Episode Num: 364 Reward: -225.91459579552895\n",
      "timesteps not 0\n",
      "Total Timesteps: 73000 Episode Num: 365 Reward: -125.71527932532689\n",
      "timesteps not 0\n",
      "Total Timesteps: 73200 Episode Num: 366 Reward: -125.46355548114289\n",
      "timesteps not 0\n",
      "Total Timesteps: 73400 Episode Num: 367 Reward: -1.7882575069345976\n",
      "timesteps not 0\n",
      "Total Timesteps: 73600 Episode Num: 368 Reward: -125.79993314788909\n",
      "timesteps not 0\n",
      "Total Timesteps: 73800 Episode Num: 369 Reward: -226.98536633374502\n",
      "timesteps not 0\n",
      "Total Timesteps: 74000 Episode Num: 370 Reward: -122.58918502388775\n",
      "timesteps not 0\n",
      "Total Timesteps: 74200 Episode Num: 371 Reward: -122.97248761960958\n",
      "timesteps not 0\n",
      "Total Timesteps: 74400 Episode Num: 372 Reward: -118.792135362949\n",
      "timesteps not 0\n",
      "Total Timesteps: 74600 Episode Num: 373 Reward: -0.26167290905659457\n",
      "timesteps not 0\n",
      "Total Timesteps: 74800 Episode Num: 374 Reward: -228.5184443371637\n",
      "timesteps not 0\n",
      "Total Timesteps: 75000 Episode Num: 375 Reward: -2.731943074582501\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -160.691318\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 75200 Episode Num: 376 Reward: -303.7554957321159\n",
      "timesteps not 0\n",
      "Total Timesteps: 75400 Episode Num: 377 Reward: -126.88015964501614\n",
      "timesteps not 0\n",
      "Total Timesteps: 75600 Episode Num: 378 Reward: -127.88494579163395\n",
      "timesteps not 0\n",
      "Total Timesteps: 75800 Episode Num: 379 Reward: -127.70587582920888\n",
      "timesteps not 0\n",
      "Total Timesteps: 76000 Episode Num: 380 Reward: -126.59675720437136\n",
      "timesteps not 0\n",
      "Total Timesteps: 76200 Episode Num: 381 Reward: -118.74949947512697\n",
      "timesteps not 0\n",
      "Total Timesteps: 76400 Episode Num: 382 Reward: -223.78413955716573\n",
      "timesteps not 0\n",
      "Total Timesteps: 76600 Episode Num: 383 Reward: -231.4009193550866\n",
      "timesteps not 0\n",
      "Total Timesteps: 76800 Episode Num: 384 Reward: -233.5697797227882\n",
      "timesteps not 0\n",
      "Total Timesteps: 77000 Episode Num: 385 Reward: -114.29428738347612\n",
      "timesteps not 0\n",
      "Total Timesteps: 77200 Episode Num: 386 Reward: -120.4315481409865\n",
      "timesteps not 0\n",
      "Total Timesteps: 77400 Episode Num: 387 Reward: -120.72195484457784\n",
      "timesteps not 0\n",
      "Total Timesteps: 77600 Episode Num: 388 Reward: -119.11421364074066\n",
      "timesteps not 0\n",
      "Total Timesteps: 77800 Episode Num: 389 Reward: -118.21982121157069\n",
      "timesteps not 0\n",
      "Total Timesteps: 78000 Episode Num: 390 Reward: -0.9930215827796116\n",
      "timesteps not 0\n",
      "Total Timesteps: 78200 Episode Num: 391 Reward: -118.22222721842637\n",
      "timesteps not 0\n",
      "Total Timesteps: 78400 Episode Num: 392 Reward: -360.8739707311696\n",
      "timesteps not 0\n",
      "Total Timesteps: 78600 Episode Num: 393 Reward: -3.6023627560036626\n",
      "timesteps not 0\n",
      "Total Timesteps: 78800 Episode Num: 394 Reward: -116.66210067737478\n",
      "timesteps not 0\n",
      "Total Timesteps: 79000 Episode Num: 395 Reward: -225.02166936239857\n",
      "timesteps not 0\n",
      "Total Timesteps: 79200 Episode Num: 396 Reward: -120.31234086652532\n",
      "timesteps not 0\n",
      "Total Timesteps: 79400 Episode Num: 397 Reward: -128.2931087000771\n",
      "timesteps not 0\n",
      "Total Timesteps: 79600 Episode Num: 398 Reward: -238.9697221518421\n",
      "timesteps not 0\n",
      "Total Timesteps: 79800 Episode Num: 399 Reward: -351.8012342181794\n",
      "timesteps not 0\n",
      "Total Timesteps: 80000 Episode Num: 400 Reward: -0.3328602563772065\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -144.512655\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 80200 Episode Num: 401 Reward: -234.60513684978346\n",
      "timesteps not 0\n",
      "Total Timesteps: 80400 Episode Num: 402 Reward: -330.6425385308329\n",
      "timesteps not 0\n",
      "Total Timesteps: 80600 Episode Num: 403 Reward: -0.473979112015187\n",
      "timesteps not 0\n",
      "Total Timesteps: 80800 Episode Num: 404 Reward: -119.03093480032743\n",
      "timesteps not 0\n",
      "Total Timesteps: 81000 Episode Num: 405 Reward: -2.3480093317081243\n",
      "timesteps not 0\n",
      "Total Timesteps: 81200 Episode Num: 406 Reward: -114.7448097785004\n",
      "timesteps not 0\n",
      "Total Timesteps: 81400 Episode Num: 407 Reward: -122.9766445759692\n",
      "timesteps not 0\n",
      "Total Timesteps: 81600 Episode Num: 408 Reward: -229.07568634340254\n",
      "timesteps not 0\n",
      "Total Timesteps: 81800 Episode Num: 409 Reward: -125.68500784207706\n",
      "timesteps not 0\n",
      "Total Timesteps: 82000 Episode Num: 410 Reward: -117.40558953681328\n",
      "timesteps not 0\n",
      "Total Timesteps: 82200 Episode Num: 411 Reward: -221.64604785813384\n",
      "timesteps not 0\n",
      "Total Timesteps: 82400 Episode Num: 412 Reward: -228.39432160674792\n",
      "timesteps not 0\n",
      "Total Timesteps: 82600 Episode Num: 413 Reward: -310.34632379541534\n",
      "timesteps not 0\n",
      "Total Timesteps: 82800 Episode Num: 414 Reward: -229.6881531698599\n",
      "timesteps not 0\n",
      "Total Timesteps: 83000 Episode Num: 415 Reward: -117.97309334034962\n",
      "timesteps not 0\n",
      "Total Timesteps: 83200 Episode Num: 416 Reward: -0.4634331834555878\n",
      "timesteps not 0\n",
      "Total Timesteps: 83400 Episode Num: 417 Reward: -122.11864580061717\n",
      "timesteps not 0\n",
      "Total Timesteps: 83600 Episode Num: 418 Reward: -0.6553780709645456\n",
      "timesteps not 0\n",
      "Total Timesteps: 83800 Episode Num: 419 Reward: -224.815870572517\n",
      "timesteps not 0\n",
      "Total Timesteps: 84000 Episode Num: 420 Reward: -1.4708320959234875\n",
      "timesteps not 0\n",
      "Total Timesteps: 84200 Episode Num: 421 Reward: -218.78786424888713\n",
      "timesteps not 0\n",
      "Total Timesteps: 84400 Episode Num: 422 Reward: -119.13890446098735\n",
      "timesteps not 0\n",
      "Total Timesteps: 84600 Episode Num: 423 Reward: -1.4282152348145591\n",
      "timesteps not 0\n",
      "Total Timesteps: 84800 Episode Num: 424 Reward: -255.5125705356532\n",
      "timesteps not 0\n",
      "Total Timesteps: 85000 Episode Num: 425 Reward: -118.87571652664667\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -149.438558\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 85200 Episode Num: 426 Reward: -1.3867889240655058\n",
      "timesteps not 0\n",
      "Total Timesteps: 85400 Episode Num: 427 Reward: -230.8466533147113\n",
      "timesteps not 0\n",
      "Total Timesteps: 85600 Episode Num: 428 Reward: -3.4245934302507384\n",
      "timesteps not 0\n",
      "Total Timesteps: 85800 Episode Num: 429 Reward: -123.83726543985112\n",
      "timesteps not 0\n",
      "Total Timesteps: 86000 Episode Num: 430 Reward: -226.3934535468748\n",
      "timesteps not 0\n",
      "Total Timesteps: 86200 Episode Num: 431 Reward: -229.26436858169004\n",
      "timesteps not 0\n",
      "Total Timesteps: 86400 Episode Num: 432 Reward: -2.2809596529027516\n",
      "timesteps not 0\n",
      "Total Timesteps: 86600 Episode Num: 433 Reward: -128.36568777963626\n",
      "timesteps not 0\n",
      "Total Timesteps: 86800 Episode Num: 434 Reward: -221.42906490837413\n",
      "timesteps not 0\n",
      "Total Timesteps: 87000 Episode Num: 435 Reward: -2.0925569107932684\n",
      "timesteps not 0\n",
      "Total Timesteps: 87200 Episode Num: 436 Reward: -126.44125462706248\n",
      "timesteps not 0\n",
      "Total Timesteps: 87400 Episode Num: 437 Reward: -117.11286868218359\n",
      "timesteps not 0\n",
      "Total Timesteps: 87600 Episode Num: 438 Reward: -119.83107685785576\n",
      "timesteps not 0\n",
      "Total Timesteps: 87800 Episode Num: 439 Reward: -221.1635255980954\n",
      "timesteps not 0\n",
      "Total Timesteps: 88000 Episode Num: 440 Reward: -228.38372333455402\n",
      "timesteps not 0\n",
      "Total Timesteps: 88200 Episode Num: 441 Reward: -131.07883409753222\n",
      "timesteps not 0\n",
      "Total Timesteps: 88400 Episode Num: 442 Reward: -129.56900067053817\n",
      "timesteps not 0\n",
      "Total Timesteps: 88600 Episode Num: 443 Reward: -125.01991765551679\n",
      "timesteps not 0\n",
      "Total Timesteps: 88800 Episode Num: 444 Reward: -125.35398670897669\n",
      "timesteps not 0\n",
      "Total Timesteps: 89000 Episode Num: 445 Reward: -219.94835115524594\n",
      "timesteps not 0\n",
      "Total Timesteps: 89200 Episode Num: 446 Reward: -116.07522612538664\n",
      "timesteps not 0\n",
      "Total Timesteps: 89400 Episode Num: 447 Reward: -0.21409028354139464\n",
      "timesteps not 0\n",
      "Total Timesteps: 89600 Episode Num: 448 Reward: -219.12435074988863\n",
      "timesteps not 0\n",
      "Total Timesteps: 89800 Episode Num: 449 Reward: -125.66849858256936\n",
      "timesteps not 0\n",
      "Total Timesteps: 90000 Episode Num: 450 Reward: -116.04150380402\n",
      "timesteps since eval more than eval freq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -82.900740\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 90200 Episode Num: 451 Reward: -116.2757585624621\n",
      "timesteps not 0\n",
      "Total Timesteps: 90400 Episode Num: 452 Reward: -117.12967273608695\n",
      "timesteps not 0\n",
      "Total Timesteps: 90600 Episode Num: 453 Reward: -223.89511792668316\n",
      "timesteps not 0\n",
      "Total Timesteps: 90800 Episode Num: 454 Reward: -124.6047951528656\n",
      "timesteps not 0\n",
      "Total Timesteps: 91000 Episode Num: 455 Reward: -118.9506266993242\n",
      "timesteps not 0\n",
      "Total Timesteps: 91200 Episode Num: 456 Reward: -118.74880582684956\n",
      "timesteps not 0\n",
      "Total Timesteps: 91400 Episode Num: 457 Reward: -123.01892131006754\n",
      "timesteps not 0\n",
      "Total Timesteps: 91600 Episode Num: 458 Reward: -124.3473523419812\n",
      "timesteps not 0\n",
      "Total Timesteps: 91800 Episode Num: 459 Reward: -114.05317907563325\n",
      "timesteps not 0\n",
      "Total Timesteps: 92000 Episode Num: 460 Reward: -0.875972735551465\n",
      "timesteps not 0\n",
      "Total Timesteps: 92200 Episode Num: 461 Reward: -224.01466595245785\n",
      "timesteps not 0\n",
      "Total Timesteps: 92400 Episode Num: 462 Reward: -230.3470964173054\n",
      "timesteps not 0\n",
      "Total Timesteps: 92600 Episode Num: 463 Reward: -294.9407374505623\n",
      "timesteps not 0\n",
      "Total Timesteps: 92800 Episode Num: 464 Reward: -119.81423754715696\n",
      "timesteps not 0\n",
      "Total Timesteps: 93000 Episode Num: 465 Reward: -124.276667172491\n",
      "timesteps not 0\n",
      "Total Timesteps: 93200 Episode Num: 466 Reward: -0.12156279006668105\n",
      "timesteps not 0\n",
      "Total Timesteps: 93400 Episode Num: 467 Reward: -232.33241140566827\n",
      "timesteps not 0\n",
      "Total Timesteps: 93600 Episode Num: 468 Reward: -221.30746247083505\n",
      "timesteps not 0\n",
      "Total Timesteps: 93800 Episode Num: 469 Reward: -119.97818712788037\n",
      "timesteps not 0\n",
      "Total Timesteps: 94000 Episode Num: 470 Reward: -0.197117356927587\n",
      "timesteps not 0\n",
      "Total Timesteps: 94200 Episode Num: 471 Reward: -223.789155378583\n",
      "timesteps not 0\n",
      "Total Timesteps: 94400 Episode Num: 472 Reward: -130.8303301085227\n",
      "timesteps not 0\n",
      "Total Timesteps: 94600 Episode Num: 473 Reward: -229.86561315082773\n",
      "timesteps not 0\n",
      "Total Timesteps: 94800 Episode Num: 474 Reward: -118.4361782483888\n",
      "timesteps not 0\n",
      "Total Timesteps: 95000 Episode Num: 475 Reward: -237.48036945659337\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -126.778358\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 95200 Episode Num: 476 Reward: -116.44069991569059\n",
      "timesteps not 0\n",
      "Total Timesteps: 95400 Episode Num: 477 Reward: -301.1032076776444\n",
      "timesteps not 0\n",
      "Total Timesteps: 95600 Episode Num: 478 Reward: -5.796626306650542\n",
      "timesteps not 0\n",
      "Total Timesteps: 95800 Episode Num: 479 Reward: -127.22854458820849\n",
      "timesteps not 0\n",
      "Total Timesteps: 96000 Episode Num: 480 Reward: -322.20343073378376\n",
      "timesteps not 0\n",
      "Total Timesteps: 96200 Episode Num: 481 Reward: -118.93978822368983\n",
      "timesteps not 0\n",
      "Total Timesteps: 96400 Episode Num: 482 Reward: -126.5713639135438\n",
      "timesteps not 0\n",
      "Total Timesteps: 96600 Episode Num: 483 Reward: -116.91108947080197\n",
      "timesteps not 0\n",
      "Total Timesteps: 96800 Episode Num: 484 Reward: -121.31781670701089\n",
      "timesteps not 0\n",
      "Total Timesteps: 97000 Episode Num: 485 Reward: -121.766799389785\n",
      "timesteps not 0\n",
      "Total Timesteps: 97200 Episode Num: 486 Reward: -0.22382058231622248\n",
      "timesteps not 0\n",
      "Total Timesteps: 97400 Episode Num: 487 Reward: -1.8233352014081092\n",
      "timesteps not 0\n",
      "Total Timesteps: 97600 Episode Num: 488 Reward: -2.2963553509184935\n",
      "timesteps not 0\n",
      "Total Timesteps: 97800 Episode Num: 489 Reward: -314.2394900955488\n",
      "timesteps not 0\n",
      "Total Timesteps: 98000 Episode Num: 490 Reward: -224.56035815153433\n",
      "timesteps not 0\n",
      "Total Timesteps: 98200 Episode Num: 491 Reward: -339.1594887488803\n",
      "timesteps not 0\n",
      "Total Timesteps: 98400 Episode Num: 492 Reward: -116.25372932176343\n",
      "timesteps not 0\n",
      "Total Timesteps: 98600 Episode Num: 493 Reward: -2.7609738911882893\n",
      "timesteps not 0\n",
      "Total Timesteps: 98800 Episode Num: 494 Reward: -224.03311395714385\n",
      "timesteps not 0\n",
      "Total Timesteps: 99000 Episode Num: 495 Reward: -224.28443999364407\n",
      "timesteps not 0\n",
      "Total Timesteps: 99200 Episode Num: 496 Reward: -234.07628236680986\n",
      "timesteps not 0\n",
      "Total Timesteps: 99400 Episode Num: 497 Reward: -1.4860684803669988\n",
      "timesteps not 0\n",
      "Total Timesteps: 99600 Episode Num: 498 Reward: -1.4003866292465388\n",
      "timesteps not 0\n",
      "Total Timesteps: 99800 Episode Num: 499 Reward: -225.958120489522\n",
      "timesteps not 0\n",
      "Total Timesteps: 100000 Episode Num: 500 Reward: -218.0903957878636\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -161.236907\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 100200 Episode Num: 501 Reward: -125.71282044239263\n",
      "timesteps not 0\n",
      "Total Timesteps: 100400 Episode Num: 502 Reward: -218.85500741573657\n",
      "timesteps not 0\n",
      "Total Timesteps: 100600 Episode Num: 503 Reward: -230.279838474859\n",
      "timesteps not 0\n",
      "Total Timesteps: 100800 Episode Num: 504 Reward: -1.6142461034968292\n",
      "timesteps not 0\n",
      "Total Timesteps: 101000 Episode Num: 505 Reward: -115.63099280415776\n",
      "timesteps not 0\n",
      "Total Timesteps: 101200 Episode Num: 506 Reward: -228.210787943642\n",
      "timesteps not 0\n",
      "Total Timesteps: 101400 Episode Num: 507 Reward: -127.9972104956405\n",
      "timesteps not 0\n",
      "Total Timesteps: 101600 Episode Num: 508 Reward: -130.5619355629085\n",
      "timesteps not 0\n",
      "Total Timesteps: 101800 Episode Num: 509 Reward: -1.9761512938472483\n",
      "timesteps not 0\n",
      "Total Timesteps: 102000 Episode Num: 510 Reward: -115.954354428322\n",
      "timesteps not 0\n",
      "Total Timesteps: 102200 Episode Num: 511 Reward: -114.7547619819262\n",
      "timesteps not 0\n",
      "Total Timesteps: 102400 Episode Num: 512 Reward: -119.52725124813246\n",
      "timesteps not 0\n",
      "Total Timesteps: 102600 Episode Num: 513 Reward: -118.13048053431706\n",
      "timesteps not 0\n",
      "Total Timesteps: 102800 Episode Num: 514 Reward: -124.68433454904023\n",
      "timesteps not 0\n",
      "Total Timesteps: 103000 Episode Num: 515 Reward: -231.81236675823905\n",
      "timesteps not 0\n",
      "Total Timesteps: 103200 Episode Num: 516 Reward: -225.23848435323876\n",
      "timesteps not 0\n",
      "Total Timesteps: 103400 Episode Num: 517 Reward: -115.40984113314956\n",
      "timesteps not 0\n",
      "Total Timesteps: 103600 Episode Num: 518 Reward: -121.85782983460827\n",
      "timesteps not 0\n",
      "Total Timesteps: 103800 Episode Num: 519 Reward: -225.29462439444322\n",
      "timesteps not 0\n",
      "Total Timesteps: 104000 Episode Num: 520 Reward: -115.33754161576478\n",
      "timesteps not 0\n",
      "Total Timesteps: 104200 Episode Num: 521 Reward: -244.3765989630049\n",
      "timesteps not 0\n",
      "Total Timesteps: 104400 Episode Num: 522 Reward: -125.18163582579861\n",
      "timesteps not 0\n",
      "Total Timesteps: 104600 Episode Num: 523 Reward: -116.15370270335681\n",
      "timesteps not 0\n",
      "Total Timesteps: 104800 Episode Num: 524 Reward: -125.17189759867681\n",
      "timesteps not 0\n",
      "Total Timesteps: 105000 Episode Num: 525 Reward: -2.9592948292476793\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -133.645518\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 105200 Episode Num: 526 Reward: -116.99913514455437\n",
      "timesteps not 0\n",
      "Total Timesteps: 105400 Episode Num: 527 Reward: -126.32703550318793\n",
      "timesteps not 0\n",
      "Total Timesteps: 105600 Episode Num: 528 Reward: -117.10756601023525\n",
      "timesteps not 0\n",
      "Total Timesteps: 105800 Episode Num: 529 Reward: -127.22184455737214\n",
      "timesteps not 0\n",
      "Total Timesteps: 106000 Episode Num: 530 Reward: -126.65295301836201\n",
      "timesteps not 0\n",
      "Total Timesteps: 106200 Episode Num: 531 Reward: -2.2438892773184516\n",
      "timesteps not 0\n",
      "Total Timesteps: 106400 Episode Num: 532 Reward: -127.87945596917683\n",
      "timesteps not 0\n",
      "Total Timesteps: 106600 Episode Num: 533 Reward: -121.12562424742337\n",
      "timesteps not 0\n",
      "Total Timesteps: 106800 Episode Num: 534 Reward: -1.1202806018955285\n",
      "timesteps not 0\n",
      "Total Timesteps: 107000 Episode Num: 535 Reward: -2.090764142419688\n",
      "timesteps not 0\n",
      "Total Timesteps: 107200 Episode Num: 536 Reward: -118.146743942602\n",
      "timesteps not 0\n",
      "Total Timesteps: 107400 Episode Num: 537 Reward: -123.98075681300105\n",
      "timesteps not 0\n",
      "Total Timesteps: 107600 Episode Num: 538 Reward: -4.5487107950709005\n",
      "timesteps not 0\n",
      "Total Timesteps: 107800 Episode Num: 539 Reward: -229.89876639152135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps not 0\n",
      "Total Timesteps: 108000 Episode Num: 540 Reward: -231.78209613118\n",
      "timesteps not 0\n",
      "Total Timesteps: 108200 Episode Num: 541 Reward: -124.96620698386768\n",
      "timesteps not 0\n",
      "Total Timesteps: 108400 Episode Num: 542 Reward: -120.90408748502412\n",
      "timesteps not 0\n",
      "Total Timesteps: 108600 Episode Num: 543 Reward: -2.2279485345692764\n",
      "timesteps not 0\n",
      "Total Timesteps: 108800 Episode Num: 544 Reward: -2.0943067543730542\n",
      "timesteps not 0\n",
      "Total Timesteps: 109000 Episode Num: 545 Reward: -2.6458114470477936\n",
      "timesteps not 0\n",
      "Total Timesteps: 109200 Episode Num: 546 Reward: -128.28502533487833\n",
      "timesteps not 0\n",
      "Total Timesteps: 109400 Episode Num: 547 Reward: -232.49206144252324\n",
      "timesteps not 0\n",
      "Total Timesteps: 109600 Episode Num: 548 Reward: -121.9551834571182\n",
      "timesteps not 0\n",
      "Total Timesteps: 109800 Episode Num: 549 Reward: -121.42734212536386\n",
      "timesteps not 0\n",
      "Total Timesteps: 110000 Episode Num: 550 Reward: -361.5204422078826\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -129.629629\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 110200 Episode Num: 551 Reward: -126.2264237443754\n",
      "timesteps not 0\n",
      "Total Timesteps: 110400 Episode Num: 552 Reward: -252.05806807575632\n",
      "timesteps not 0\n",
      "Total Timesteps: 110600 Episode Num: 553 Reward: -128.10999219564465\n",
      "timesteps not 0\n",
      "Total Timesteps: 110800 Episode Num: 554 Reward: -122.70821458828199\n",
      "timesteps not 0\n",
      "Total Timesteps: 111000 Episode Num: 555 Reward: -225.12513275813504\n",
      "timesteps not 0\n",
      "Total Timesteps: 111200 Episode Num: 556 Reward: -126.42962993296115\n",
      "timesteps not 0\n",
      "Total Timesteps: 111400 Episode Num: 557 Reward: -250.04057178294846\n",
      "timesteps not 0\n",
      "Total Timesteps: 111600 Episode Num: 558 Reward: -121.67962446388121\n",
      "timesteps not 0\n",
      "Total Timesteps: 111800 Episode Num: 559 Reward: -325.5299376130871\n",
      "timesteps not 0\n",
      "Total Timesteps: 112000 Episode Num: 560 Reward: -221.06905649660632\n",
      "timesteps not 0\n",
      "Total Timesteps: 112200 Episode Num: 561 Reward: -1.9922078991264895\n",
      "timesteps not 0\n",
      "Total Timesteps: 112400 Episode Num: 562 Reward: -125.90662371777557\n",
      "timesteps not 0\n",
      "Total Timesteps: 112600 Episode Num: 563 Reward: -223.0238532325574\n",
      "timesteps not 0\n",
      "Total Timesteps: 112800 Episode Num: 564 Reward: -223.66176073911686\n",
      "timesteps not 0\n",
      "Total Timesteps: 113000 Episode Num: 565 Reward: -231.47163091321326\n",
      "timesteps not 0\n",
      "Total Timesteps: 113200 Episode Num: 566 Reward: -122.86527912754877\n",
      "timesteps not 0\n",
      "Total Timesteps: 113400 Episode Num: 567 Reward: -117.51353709511817\n",
      "timesteps not 0\n",
      "Total Timesteps: 113600 Episode Num: 568 Reward: -116.88439993134239\n",
      "timesteps not 0\n",
      "Total Timesteps: 113800 Episode Num: 569 Reward: -243.722739151587\n",
      "timesteps not 0\n",
      "Total Timesteps: 114000 Episode Num: 570 Reward: -118.62386274285122\n",
      "timesteps not 0\n",
      "Total Timesteps: 114200 Episode Num: 571 Reward: -240.82746350498036\n",
      "timesteps not 0\n",
      "Total Timesteps: 114400 Episode Num: 572 Reward: -117.5990942446\n",
      "timesteps not 0\n",
      "Total Timesteps: 114600 Episode Num: 573 Reward: -228.15611540291616\n",
      "timesteps not 0\n",
      "Total Timesteps: 114800 Episode Num: 574 Reward: -0.036276789406594255\n",
      "timesteps not 0\n",
      "Total Timesteps: 115000 Episode Num: 575 Reward: -242.35326734652463\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -166.506021\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 115200 Episode Num: 576 Reward: -116.07009544794118\n",
      "timesteps not 0\n",
      "Total Timesteps: 115400 Episode Num: 577 Reward: -239.74172031629973\n",
      "timesteps not 0\n",
      "Total Timesteps: 115600 Episode Num: 578 Reward: -129.70297556074073\n",
      "timesteps not 0\n",
      "Total Timesteps: 115800 Episode Num: 579 Reward: -119.72578597601276\n",
      "timesteps not 0\n",
      "Total Timesteps: 116000 Episode Num: 580 Reward: -127.49838377895733\n",
      "timesteps not 0\n",
      "Total Timesteps: 116200 Episode Num: 581 Reward: -283.403201465034\n",
      "timesteps not 0\n",
      "Total Timesteps: 116400 Episode Num: 582 Reward: -0.8620807470760501\n",
      "timesteps not 0\n",
      "Total Timesteps: 116600 Episode Num: 583 Reward: -122.63189302224588\n",
      "timesteps not 0\n",
      "Total Timesteps: 116800 Episode Num: 584 Reward: -119.36126927179535\n",
      "timesteps not 0\n",
      "Total Timesteps: 117000 Episode Num: 585 Reward: -129.14486961531298\n",
      "timesteps not 0\n",
      "Total Timesteps: 117200 Episode Num: 586 Reward: -131.1815744601727\n",
      "timesteps not 0\n",
      "Total Timesteps: 117400 Episode Num: 587 Reward: -115.16675084699177\n",
      "timesteps not 0\n",
      "Total Timesteps: 117600 Episode Num: 588 Reward: -126.21989379068295\n",
      "timesteps not 0\n",
      "Total Timesteps: 117800 Episode Num: 589 Reward: -2.0227663463168275\n",
      "timesteps not 0\n",
      "Total Timesteps: 118000 Episode Num: 590 Reward: -231.11195564612638\n",
      "timesteps not 0\n",
      "Total Timesteps: 118200 Episode Num: 591 Reward: -227.8378551906642\n",
      "timesteps not 0\n",
      "Total Timesteps: 118400 Episode Num: 592 Reward: -124.70988175334416\n",
      "timesteps not 0\n",
      "Total Timesteps: 118600 Episode Num: 593 Reward: -244.62588871376346\n",
      "timesteps not 0\n",
      "Total Timesteps: 118800 Episode Num: 594 Reward: -127.17007972451563\n",
      "timesteps not 0\n",
      "Total Timesteps: 119000 Episode Num: 595 Reward: -114.78556474941563\n",
      "timesteps not 0\n",
      "Total Timesteps: 119200 Episode Num: 596 Reward: -128.33726895096495\n",
      "timesteps not 0\n",
      "Total Timesteps: 119400 Episode Num: 597 Reward: -2.119429236477369\n",
      "timesteps not 0\n",
      "Total Timesteps: 119600 Episode Num: 598 Reward: -121.63851634093082\n",
      "timesteps not 0\n",
      "Total Timesteps: 119800 Episode Num: 599 Reward: -118.29180179192585\n",
      "timesteps not 0\n",
      "Total Timesteps: 120000 Episode Num: 600 Reward: -309.10960713054897\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -158.609607\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 120200 Episode Num: 601 Reward: -123.4747785828239\n",
      "timesteps not 0\n",
      "Total Timesteps: 120400 Episode Num: 602 Reward: -5.597691128415602\n",
      "timesteps not 0\n",
      "Total Timesteps: 120600 Episode Num: 603 Reward: -248.51690390183563\n",
      "timesteps not 0\n",
      "Total Timesteps: 120800 Episode Num: 604 Reward: -128.5314938109633\n",
      "timesteps not 0\n",
      "Total Timesteps: 121000 Episode Num: 605 Reward: -240.98703942747585\n",
      "timesteps not 0\n",
      "Total Timesteps: 121200 Episode Num: 606 Reward: -233.15878886481545\n",
      "timesteps not 0\n",
      "Total Timesteps: 121400 Episode Num: 607 Reward: -126.29429700061154\n",
      "timesteps not 0\n",
      "Total Timesteps: 121600 Episode Num: 608 Reward: -228.94431851932382\n",
      "timesteps not 0\n",
      "Total Timesteps: 121800 Episode Num: 609 Reward: -221.3365088577365\n",
      "timesteps not 0\n",
      "Total Timesteps: 122000 Episode Num: 610 Reward: -126.93733181834675\n",
      "timesteps not 0\n",
      "Total Timesteps: 122200 Episode Num: 611 Reward: -119.83349895403298\n",
      "timesteps not 0\n",
      "Total Timesteps: 122400 Episode Num: 612 Reward: -1.9959246599667684\n",
      "timesteps not 0\n",
      "Total Timesteps: 122600 Episode Num: 613 Reward: -118.93587664871153\n",
      "timesteps not 0\n",
      "Total Timesteps: 122800 Episode Num: 614 Reward: -127.35312701051528\n",
      "timesteps not 0\n",
      "Total Timesteps: 123000 Episode Num: 615 Reward: -118.79056685066027\n",
      "timesteps not 0\n",
      "Total Timesteps: 123200 Episode Num: 616 Reward: -125.14443805967022\n",
      "timesteps not 0\n",
      "Total Timesteps: 123400 Episode Num: 617 Reward: -121.27548859482663\n",
      "timesteps not 0\n",
      "Total Timesteps: 123600 Episode Num: 618 Reward: -125.46749447936212\n",
      "timesteps not 0\n",
      "Total Timesteps: 123800 Episode Num: 619 Reward: -363.3729143748453\n",
      "timesteps not 0\n",
      "Total Timesteps: 124000 Episode Num: 620 Reward: -116.34753524985459\n",
      "timesteps not 0\n",
      "Total Timesteps: 124200 Episode Num: 621 Reward: -223.2094033975822\n",
      "timesteps not 0\n",
      "Total Timesteps: 124400 Episode Num: 622 Reward: -116.82254485602418\n",
      "timesteps not 0\n",
      "Total Timesteps: 124600 Episode Num: 623 Reward: -118.71050217455581\n",
      "timesteps not 0\n",
      "Total Timesteps: 124800 Episode Num: 624 Reward: -1.1555842665997318\n",
      "timesteps not 0\n",
      "Total Timesteps: 125000 Episode Num: 625 Reward: -119.37659609405026\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -135.525818\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 125200 Episode Num: 626 Reward: -116.409943593311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps not 0\n",
      "Total Timesteps: 125400 Episode Num: 627 Reward: -297.13461591527397\n",
      "timesteps not 0\n",
      "Total Timesteps: 125600 Episode Num: 628 Reward: -122.45777657127742\n",
      "timesteps not 0\n",
      "Total Timesteps: 125800 Episode Num: 629 Reward: -218.06622353351693\n",
      "timesteps not 0\n",
      "Total Timesteps: 126000 Episode Num: 630 Reward: -117.18664701365998\n",
      "timesteps not 0\n",
      "Total Timesteps: 126200 Episode Num: 631 Reward: -126.57743214452499\n",
      "timesteps not 0\n",
      "Total Timesteps: 126400 Episode Num: 632 Reward: -126.26968008571565\n",
      "timesteps not 0\n",
      "Total Timesteps: 126600 Episode Num: 633 Reward: -117.07002452967068\n",
      "timesteps not 0\n",
      "Total Timesteps: 126800 Episode Num: 634 Reward: -125.37050797416684\n",
      "timesteps not 0\n",
      "Total Timesteps: 127000 Episode Num: 635 Reward: -0.5028864099756802\n",
      "timesteps not 0\n",
      "Total Timesteps: 127200 Episode Num: 636 Reward: -118.8385150501174\n",
      "timesteps not 0\n",
      "Total Timesteps: 127400 Episode Num: 637 Reward: -116.52848765664967\n",
      "timesteps not 0\n",
      "Total Timesteps: 127600 Episode Num: 638 Reward: -221.9386719418917\n",
      "timesteps not 0\n",
      "Total Timesteps: 127800 Episode Num: 639 Reward: -118.60616028696886\n",
      "timesteps not 0\n",
      "Total Timesteps: 128000 Episode Num: 640 Reward: -228.04082371780197\n",
      "timesteps not 0\n",
      "Total Timesteps: 128200 Episode Num: 641 Reward: -127.08198272982717\n",
      "timesteps not 0\n",
      "Total Timesteps: 128400 Episode Num: 642 Reward: -1.2197077573825077\n",
      "timesteps not 0\n",
      "Total Timesteps: 128600 Episode Num: 643 Reward: -114.78423355969765\n",
      "timesteps not 0\n",
      "Total Timesteps: 128800 Episode Num: 644 Reward: -223.12649214561498\n",
      "timesteps not 0\n",
      "Total Timesteps: 129000 Episode Num: 645 Reward: -235.3814432762672\n",
      "timesteps not 0\n",
      "Total Timesteps: 129200 Episode Num: 646 Reward: -2.073800409954245\n",
      "timesteps not 0\n",
      "Total Timesteps: 129400 Episode Num: 647 Reward: -127.08148871952015\n",
      "timesteps not 0\n",
      "Total Timesteps: 129600 Episode Num: 648 Reward: -125.11634009186731\n",
      "timesteps not 0\n",
      "Total Timesteps: 129800 Episode Num: 649 Reward: -126.32597388094382\n",
      "timesteps not 0\n",
      "Total Timesteps: 130000 Episode Num: 650 Reward: -123.118593508358\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -163.194337\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 130200 Episode Num: 651 Reward: -119.61967380294092\n",
      "timesteps not 0\n",
      "Total Timesteps: 130400 Episode Num: 652 Reward: -243.87255903489506\n",
      "timesteps not 0\n",
      "Total Timesteps: 130600 Episode Num: 653 Reward: -119.03404787508124\n",
      "timesteps not 0\n",
      "Total Timesteps: 130800 Episode Num: 654 Reward: -233.56437438140293\n",
      "timesteps not 0\n",
      "Total Timesteps: 131000 Episode Num: 655 Reward: -115.62904442165771\n",
      "timesteps not 0\n",
      "Total Timesteps: 131200 Episode Num: 656 Reward: -289.96783068549485\n",
      "timesteps not 0\n",
      "Total Timesteps: 131400 Episode Num: 657 Reward: -243.37644306561165\n",
      "timesteps not 0\n",
      "Total Timesteps: 131600 Episode Num: 658 Reward: -125.30163142649418\n",
      "timesteps not 0\n",
      "Total Timesteps: 131800 Episode Num: 659 Reward: -119.34210328018669\n",
      "timesteps not 0\n",
      "Total Timesteps: 132000 Episode Num: 660 Reward: -117.54339422739721\n",
      "timesteps not 0\n",
      "Total Timesteps: 132200 Episode Num: 661 Reward: -2.344896255075982\n",
      "timesteps not 0\n",
      "Total Timesteps: 132400 Episode Num: 662 Reward: -228.45212869088328\n",
      "timesteps not 0\n",
      "Total Timesteps: 132600 Episode Num: 663 Reward: -2.869785026918209\n",
      "timesteps not 0\n",
      "Total Timesteps: 132800 Episode Num: 664 Reward: -226.5649143862397\n",
      "timesteps not 0\n",
      "Total Timesteps: 133000 Episode Num: 665 Reward: -125.22626621524222\n",
      "timesteps not 0\n",
      "Total Timesteps: 133200 Episode Num: 666 Reward: -119.10219877303044\n",
      "timesteps not 0\n",
      "Total Timesteps: 133400 Episode Num: 667 Reward: -116.52867915805825\n",
      "timesteps not 0\n",
      "Total Timesteps: 133600 Episode Num: 668 Reward: -130.24953559150833\n",
      "timesteps not 0\n",
      "Total Timesteps: 133800 Episode Num: 669 Reward: -4.1287479074675275\n",
      "timesteps not 0\n",
      "Total Timesteps: 134000 Episode Num: 670 Reward: -120.56233072979315\n",
      "timesteps not 0\n",
      "Total Timesteps: 134200 Episode Num: 671 Reward: -114.93053351009074\n",
      "timesteps not 0\n",
      "Total Timesteps: 134400 Episode Num: 672 Reward: -131.50411026778292\n",
      "timesteps not 0\n",
      "Total Timesteps: 134600 Episode Num: 673 Reward: -125.58736820959139\n",
      "timesteps not 0\n",
      "Total Timesteps: 134800 Episode Num: 674 Reward: -123.84242742903506\n",
      "timesteps not 0\n",
      "Total Timesteps: 135000 Episode Num: 675 Reward: -245.26497629738154\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -120.580716\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 135200 Episode Num: 676 Reward: -3.163711905894868\n",
      "timesteps not 0\n",
      "Total Timesteps: 135400 Episode Num: 677 Reward: -114.37673015651227\n",
      "timesteps not 0\n",
      "Total Timesteps: 135600 Episode Num: 678 Reward: -232.0509462058877\n",
      "timesteps not 0\n",
      "Total Timesteps: 135800 Episode Num: 679 Reward: -127.68923782912752\n",
      "timesteps not 0\n",
      "Total Timesteps: 136000 Episode Num: 680 Reward: -123.79904643771326\n",
      "timesteps not 0\n",
      "Total Timesteps: 136200 Episode Num: 681 Reward: -2.148138893549379\n",
      "timesteps not 0\n",
      "Total Timesteps: 136400 Episode Num: 682 Reward: -117.17415714602895\n",
      "timesteps not 0\n",
      "Total Timesteps: 136600 Episode Num: 683 Reward: -120.64318353795618\n",
      "timesteps not 0\n",
      "Total Timesteps: 136800 Episode Num: 684 Reward: -126.800416272629\n",
      "timesteps not 0\n",
      "Total Timesteps: 137000 Episode Num: 685 Reward: -220.31783711812156\n",
      "timesteps not 0\n",
      "Total Timesteps: 137200 Episode Num: 686 Reward: -218.95459481979657\n",
      "timesteps not 0\n",
      "Total Timesteps: 137400 Episode Num: 687 Reward: -1.8057633078079485\n",
      "timesteps not 0\n",
      "Total Timesteps: 137600 Episode Num: 688 Reward: -129.34616640707378\n",
      "timesteps not 0\n",
      "Total Timesteps: 137800 Episode Num: 689 Reward: -2.267966020501997\n",
      "timesteps not 0\n",
      "Total Timesteps: 138000 Episode Num: 690 Reward: -118.43138229027471\n",
      "timesteps not 0\n",
      "Total Timesteps: 138200 Episode Num: 691 Reward: -115.83395345894473\n",
      "timesteps not 0\n",
      "Total Timesteps: 138400 Episode Num: 692 Reward: -2.2096805659738195\n",
      "timesteps not 0\n",
      "Total Timesteps: 138600 Episode Num: 693 Reward: -217.42878310169024\n",
      "timesteps not 0\n",
      "Total Timesteps: 138800 Episode Num: 694 Reward: -216.86092041051168\n",
      "timesteps not 0\n",
      "Total Timesteps: 139000 Episode Num: 695 Reward: -266.444130201028\n",
      "timesteps not 0\n",
      "Total Timesteps: 139200 Episode Num: 696 Reward: -225.8072364669225\n",
      "timesteps not 0\n",
      "Total Timesteps: 139400 Episode Num: 697 Reward: -117.49135874462979\n",
      "timesteps not 0\n",
      "Total Timesteps: 139600 Episode Num: 698 Reward: -124.18549339045525\n",
      "timesteps not 0\n",
      "Total Timesteps: 139800 Episode Num: 699 Reward: -237.81018134466197\n",
      "timesteps not 0\n",
      "Total Timesteps: 140000 Episode Num: 700 Reward: -3.2355102255977584\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -145.467191\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 140200 Episode Num: 701 Reward: -121.82995865819231\n",
      "timesteps not 0\n",
      "Total Timesteps: 140400 Episode Num: 702 Reward: -243.84706160649614\n",
      "timesteps not 0\n",
      "Total Timesteps: 140600 Episode Num: 703 Reward: -227.72083872820753\n",
      "timesteps not 0\n",
      "Total Timesteps: 140800 Episode Num: 704 Reward: -121.12621945755683\n",
      "timesteps not 0\n",
      "Total Timesteps: 141000 Episode Num: 705 Reward: -226.43801527484703\n",
      "timesteps not 0\n",
      "Total Timesteps: 141200 Episode Num: 706 Reward: -225.27413137219955\n",
      "timesteps not 0\n",
      "Total Timesteps: 141400 Episode Num: 707 Reward: -128.75392742587397\n",
      "timesteps not 0\n",
      "Total Timesteps: 141600 Episode Num: 708 Reward: -120.51741129691213\n",
      "timesteps not 0\n",
      "Total Timesteps: 141800 Episode Num: 709 Reward: -1.6722633759331385\n",
      "timesteps not 0\n",
      "Total Timesteps: 142000 Episode Num: 710 Reward: -120.89235587636664\n",
      "timesteps not 0\n",
      "Total Timesteps: 142200 Episode Num: 711 Reward: -122.15427984342884\n",
      "timesteps not 0\n",
      "Total Timesteps: 142400 Episode Num: 712 Reward: -3.386858872250841\n",
      "timesteps not 0\n",
      "Total Timesteps: 142600 Episode Num: 713 Reward: -2.0725385406761436\n",
      "timesteps not 0\n",
      "Total Timesteps: 142800 Episode Num: 714 Reward: -129.34287936257874\n",
      "timesteps not 0\n",
      "Total Timesteps: 143000 Episode Num: 715 Reward: -3.4808763209362312\n",
      "timesteps not 0\n",
      "Total Timesteps: 143200 Episode Num: 716 Reward: -226.47489233344638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps not 0\n",
      "Total Timesteps: 143400 Episode Num: 717 Reward: -116.82360853562332\n",
      "timesteps not 0\n",
      "Total Timesteps: 143600 Episode Num: 718 Reward: -120.50627145458307\n",
      "timesteps not 0\n",
      "Total Timesteps: 143800 Episode Num: 719 Reward: -125.51456265895854\n",
      "timesteps not 0\n",
      "Total Timesteps: 144000 Episode Num: 720 Reward: -122.02853182603275\n",
      "timesteps not 0\n",
      "Total Timesteps: 144200 Episode Num: 721 Reward: -116.09354901426764\n",
      "timesteps not 0\n",
      "Total Timesteps: 144400 Episode Num: 722 Reward: -228.2848305485938\n",
      "timesteps not 0\n",
      "Total Timesteps: 144600 Episode Num: 723 Reward: -114.59253917194026\n",
      "timesteps not 0\n",
      "Total Timesteps: 144800 Episode Num: 724 Reward: -117.76304700269148\n",
      "timesteps not 0\n",
      "Total Timesteps: 145000 Episode Num: 725 Reward: -120.16317785807085\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -111.062232\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 145200 Episode Num: 726 Reward: -119.74143230988518\n",
      "timesteps not 0\n",
      "Total Timesteps: 145400 Episode Num: 727 Reward: -118.66720316632903\n",
      "timesteps not 0\n",
      "Total Timesteps: 145600 Episode Num: 728 Reward: -127.36679824065928\n",
      "timesteps not 0\n",
      "Total Timesteps: 145800 Episode Num: 729 Reward: -229.0402295759319\n",
      "timesteps not 0\n",
      "Total Timesteps: 146000 Episode Num: 730 Reward: -1.357260993760154\n",
      "timesteps not 0\n",
      "Total Timesteps: 146200 Episode Num: 731 Reward: -128.2725221757327\n",
      "timesteps not 0\n",
      "Total Timesteps: 146400 Episode Num: 732 Reward: -125.8988887709113\n",
      "timesteps not 0\n",
      "Total Timesteps: 146600 Episode Num: 733 Reward: -121.60520229671965\n",
      "timesteps not 0\n",
      "Total Timesteps: 146800 Episode Num: 734 Reward: -314.90049307213445\n",
      "timesteps not 0\n",
      "Total Timesteps: 147000 Episode Num: 735 Reward: -323.05288286906057\n",
      "timesteps not 0\n",
      "Total Timesteps: 147200 Episode Num: 736 Reward: -2.4243793517165195\n",
      "timesteps not 0\n",
      "Total Timesteps: 147400 Episode Num: 737 Reward: -119.16988443412397\n",
      "timesteps not 0\n",
      "Total Timesteps: 147600 Episode Num: 738 Reward: -121.89793270177339\n",
      "timesteps not 0\n",
      "Total Timesteps: 147800 Episode Num: 739 Reward: -125.54519654242907\n",
      "timesteps not 0\n",
      "Total Timesteps: 148000 Episode Num: 740 Reward: -120.07796661696855\n",
      "timesteps not 0\n",
      "Total Timesteps: 148200 Episode Num: 741 Reward: -118.61375498658617\n",
      "timesteps not 0\n",
      "Total Timesteps: 148400 Episode Num: 742 Reward: -123.33811622458686\n",
      "timesteps not 0\n",
      "Total Timesteps: 148600 Episode Num: 743 Reward: -127.97123283245578\n",
      "timesteps not 0\n",
      "Total Timesteps: 148800 Episode Num: 744 Reward: -222.21977042268466\n",
      "timesteps not 0\n",
      "Total Timesteps: 149000 Episode Num: 745 Reward: -120.81172411997537\n",
      "timesteps not 0\n",
      "Total Timesteps: 149200 Episode Num: 746 Reward: -243.6343391440805\n",
      "timesteps not 0\n",
      "Total Timesteps: 149400 Episode Num: 747 Reward: -121.54040069456259\n",
      "timesteps not 0\n",
      "Total Timesteps: 149600 Episode Num: 748 Reward: -129.98435616081846\n",
      "timesteps not 0\n",
      "Total Timesteps: 149800 Episode Num: 749 Reward: -129.5822831586979\n",
      "timesteps not 0\n",
      "Total Timesteps: 150000 Episode Num: 750 Reward: -120.2785463290206\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -162.069171\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 150200 Episode Num: 751 Reward: -117.91552471844395\n",
      "timesteps not 0\n",
      "Total Timesteps: 150400 Episode Num: 752 Reward: -122.9658629908107\n",
      "timesteps not 0\n",
      "Total Timesteps: 150600 Episode Num: 753 Reward: -122.60246396647163\n",
      "timesteps not 0\n",
      "Total Timesteps: 150800 Episode Num: 754 Reward: -226.379751253489\n",
      "timesteps not 0\n",
      "Total Timesteps: 151000 Episode Num: 755 Reward: -245.56404511983683\n",
      "timesteps not 0\n",
      "Total Timesteps: 151200 Episode Num: 756 Reward: -4.477945624910871\n",
      "timesteps not 0\n",
      "Total Timesteps: 151400 Episode Num: 757 Reward: -231.148341956771\n",
      "timesteps not 0\n",
      "Total Timesteps: 151600 Episode Num: 758 Reward: -120.37102285468737\n",
      "timesteps not 0\n",
      "Total Timesteps: 151800 Episode Num: 759 Reward: -4.552575660471861\n",
      "timesteps not 0\n",
      "Total Timesteps: 152000 Episode Num: 760 Reward: -3.2139634193589894\n",
      "timesteps not 0\n",
      "Total Timesteps: 152200 Episode Num: 761 Reward: -127.42366266347608\n",
      "timesteps not 0\n",
      "Total Timesteps: 152400 Episode Num: 762 Reward: -129.63008706595207\n",
      "timesteps not 0\n",
      "Total Timesteps: 152600 Episode Num: 763 Reward: -131.3633477400298\n",
      "timesteps not 0\n",
      "Total Timesteps: 152800 Episode Num: 764 Reward: -126.23545297701872\n",
      "timesteps not 0\n",
      "Total Timesteps: 153000 Episode Num: 765 Reward: -119.51148753310558\n",
      "timesteps not 0\n",
      "Total Timesteps: 153200 Episode Num: 766 Reward: -122.73195706142252\n",
      "timesteps not 0\n",
      "Total Timesteps: 153400 Episode Num: 767 Reward: -230.93407535286673\n",
      "timesteps not 0\n",
      "Total Timesteps: 153600 Episode Num: 768 Reward: -119.11590457388748\n",
      "timesteps not 0\n",
      "Total Timesteps: 153800 Episode Num: 769 Reward: -243.9718542374062\n",
      "timesteps not 0\n",
      "Total Timesteps: 154000 Episode Num: 770 Reward: -229.99167512156507\n",
      "timesteps not 0\n",
      "Total Timesteps: 154200 Episode Num: 771 Reward: -129.2174911771104\n",
      "timesteps not 0\n",
      "Total Timesteps: 154400 Episode Num: 772 Reward: -130.11831048555572\n",
      "timesteps not 0\n",
      "Total Timesteps: 154600 Episode Num: 773 Reward: -123.33924155323146\n",
      "timesteps not 0\n",
      "Total Timesteps: 154800 Episode Num: 774 Reward: -3.6611671123456735\n",
      "timesteps not 0\n",
      "Total Timesteps: 155000 Episode Num: 775 Reward: -116.10912249808064\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -128.880583\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 155200 Episode Num: 776 Reward: -120.67452307474\n",
      "timesteps not 0\n",
      "Total Timesteps: 155400 Episode Num: 777 Reward: -118.7140831987891\n",
      "timesteps not 0\n",
      "Total Timesteps: 155600 Episode Num: 778 Reward: -123.83254300652902\n",
      "timesteps not 0\n",
      "Total Timesteps: 155800 Episode Num: 779 Reward: -231.52149718025294\n",
      "timesteps not 0\n",
      "Total Timesteps: 156000 Episode Num: 780 Reward: -226.7343028254047\n",
      "timesteps not 0\n",
      "Total Timesteps: 156200 Episode Num: 781 Reward: -238.0938490515578\n",
      "timesteps not 0\n",
      "Total Timesteps: 156400 Episode Num: 782 Reward: -131.7232444178589\n",
      "timesteps not 0\n",
      "Total Timesteps: 156600 Episode Num: 783 Reward: -125.92487897964874\n",
      "timesteps not 0\n",
      "Total Timesteps: 156800 Episode Num: 784 Reward: -121.69631088370274\n",
      "timesteps not 0\n",
      "Total Timesteps: 157000 Episode Num: 785 Reward: -129.63986067784722\n",
      "timesteps not 0\n",
      "Total Timesteps: 157200 Episode Num: 786 Reward: -218.93050748210922\n",
      "timesteps not 0\n",
      "Total Timesteps: 157400 Episode Num: 787 Reward: -128.79325877368933\n",
      "timesteps not 0\n",
      "Total Timesteps: 157600 Episode Num: 788 Reward: -132.33580923295239\n",
      "timesteps not 0\n",
      "Total Timesteps: 157800 Episode Num: 789 Reward: -129.2016725412942\n",
      "timesteps not 0\n",
      "Total Timesteps: 158000 Episode Num: 790 Reward: -120.40772771882544\n",
      "timesteps not 0\n",
      "Total Timesteps: 158200 Episode Num: 791 Reward: -123.24344036380582\n",
      "timesteps not 0\n",
      "Total Timesteps: 158400 Episode Num: 792 Reward: -124.01729843844875\n",
      "timesteps not 0\n",
      "Total Timesteps: 158600 Episode Num: 793 Reward: -123.18434222444968\n",
      "timesteps not 0\n",
      "Total Timesteps: 158800 Episode Num: 794 Reward: -120.42770710980624\n",
      "timesteps not 0\n",
      "Total Timesteps: 159000 Episode Num: 795 Reward: -121.30128885153721\n",
      "timesteps not 0\n",
      "Total Timesteps: 159200 Episode Num: 796 Reward: -122.31198724105019\n",
      "timesteps not 0\n",
      "Total Timesteps: 159400 Episode Num: 797 Reward: -243.58536278978755\n",
      "timesteps not 0\n",
      "Total Timesteps: 159600 Episode Num: 798 Reward: -126.90216375786704\n",
      "timesteps not 0\n",
      "Total Timesteps: 159800 Episode Num: 799 Reward: -242.7631142825603\n",
      "timesteps not 0\n",
      "Total Timesteps: 160000 Episode Num: 800 Reward: -230.12314451736285\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -168.742696\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 160200 Episode Num: 801 Reward: -130.00857481524633\n",
      "timesteps not 0\n",
      "Total Timesteps: 160400 Episode Num: 802 Reward: -120.8539898939769\n",
      "timesteps not 0\n",
      "Total Timesteps: 160600 Episode Num: 803 Reward: -120.6298256222914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps not 0\n",
      "Total Timesteps: 160800 Episode Num: 804 Reward: -129.4791110160085\n",
      "timesteps not 0\n",
      "Total Timesteps: 161000 Episode Num: 805 Reward: -239.8346794531895\n",
      "timesteps not 0\n",
      "Total Timesteps: 161200 Episode Num: 806 Reward: -124.95831050750796\n",
      "timesteps not 0\n",
      "Total Timesteps: 161400 Episode Num: 807 Reward: -12.445275007919044\n",
      "timesteps not 0\n",
      "Total Timesteps: 161600 Episode Num: 808 Reward: -124.3041747407093\n",
      "timesteps not 0\n",
      "Total Timesteps: 161800 Episode Num: 809 Reward: -363.9375126143566\n",
      "timesteps not 0\n",
      "Total Timesteps: 162000 Episode Num: 810 Reward: -128.60082033757485\n",
      "timesteps not 0\n",
      "Total Timesteps: 162200 Episode Num: 811 Reward: -246.20064867964618\n",
      "timesteps not 0\n",
      "Total Timesteps: 162400 Episode Num: 812 Reward: -129.53128110129978\n",
      "timesteps not 0\n",
      "Total Timesteps: 162600 Episode Num: 813 Reward: -5.355867985224817\n",
      "timesteps not 0\n",
      "Total Timesteps: 162800 Episode Num: 814 Reward: -120.00716630957822\n",
      "timesteps not 0\n",
      "Total Timesteps: 163000 Episode Num: 815 Reward: -126.7396373793435\n",
      "timesteps not 0\n",
      "Total Timesteps: 163200 Episode Num: 816 Reward: -238.96026317046662\n",
      "timesteps not 0\n",
      "Total Timesteps: 163400 Episode Num: 817 Reward: -127.96026700084153\n",
      "timesteps not 0\n",
      "Total Timesteps: 163600 Episode Num: 818 Reward: -119.23394919089623\n",
      "timesteps not 0\n",
      "Total Timesteps: 163800 Episode Num: 819 Reward: -6.34384690590007\n",
      "timesteps not 0\n",
      "Total Timesteps: 164000 Episode Num: 820 Reward: -132.28076657232933\n",
      "timesteps not 0\n",
      "Total Timesteps: 164200 Episode Num: 821 Reward: -121.24030147060353\n",
      "timesteps not 0\n",
      "Total Timesteps: 164400 Episode Num: 822 Reward: -119.27603288144765\n",
      "timesteps not 0\n",
      "Total Timesteps: 164600 Episode Num: 823 Reward: -128.78334375598473\n",
      "timesteps not 0\n",
      "Total Timesteps: 164800 Episode Num: 824 Reward: -230.28734961939247\n",
      "timesteps not 0\n",
      "Total Timesteps: 165000 Episode Num: 825 Reward: -229.81334822389087\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -155.579740\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 165200 Episode Num: 826 Reward: -124.8556900551533\n",
      "timesteps not 0\n",
      "Total Timesteps: 165400 Episode Num: 827 Reward: -130.6893879119647\n",
      "timesteps not 0\n",
      "Total Timesteps: 165600 Episode Num: 828 Reward: -135.1354355460259\n",
      "timesteps not 0\n",
      "Total Timesteps: 165800 Episode Num: 829 Reward: -121.89437062267795\n",
      "timesteps not 0\n",
      "Total Timesteps: 166000 Episode Num: 830 Reward: -4.904367459153222\n",
      "timesteps not 0\n",
      "Total Timesteps: 166200 Episode Num: 831 Reward: -4.728188455662353\n",
      "timesteps not 0\n",
      "Total Timesteps: 166400 Episode Num: 832 Reward: -4.401251343326976\n",
      "timesteps not 0\n",
      "Total Timesteps: 166600 Episode Num: 833 Reward: -130.2711233073356\n",
      "timesteps not 0\n",
      "Total Timesteps: 166800 Episode Num: 834 Reward: -226.47351660319825\n",
      "timesteps not 0\n",
      "Total Timesteps: 167000 Episode Num: 835 Reward: -117.25057564217698\n",
      "timesteps not 0\n",
      "Total Timesteps: 167200 Episode Num: 836 Reward: -125.1610916101108\n",
      "timesteps not 0\n",
      "Total Timesteps: 167400 Episode Num: 837 Reward: -120.93589597594861\n",
      "timesteps not 0\n",
      "Total Timesteps: 167600 Episode Num: 838 Reward: -129.7828889062077\n",
      "timesteps not 0\n",
      "Total Timesteps: 167800 Episode Num: 839 Reward: -250.9254761054169\n",
      "timesteps not 0\n",
      "Total Timesteps: 168000 Episode Num: 840 Reward: -227.02933749662614\n",
      "timesteps not 0\n",
      "Total Timesteps: 168200 Episode Num: 841 Reward: -228.70560726637075\n",
      "timesteps not 0\n",
      "Total Timesteps: 168400 Episode Num: 842 Reward: -124.57057800504856\n",
      "timesteps not 0\n",
      "Total Timesteps: 168600 Episode Num: 843 Reward: -316.3024811777726\n",
      "timesteps not 0\n",
      "Total Timesteps: 168800 Episode Num: 844 Reward: -127.6919231068171\n",
      "timesteps not 0\n",
      "Total Timesteps: 169000 Episode Num: 845 Reward: -127.02261555830773\n",
      "timesteps not 0\n",
      "Total Timesteps: 169200 Episode Num: 846 Reward: -120.50771451799066\n",
      "timesteps not 0\n",
      "Total Timesteps: 169400 Episode Num: 847 Reward: -118.64196460409703\n",
      "timesteps not 0\n",
      "Total Timesteps: 169600 Episode Num: 848 Reward: -2.6147526763764475\n",
      "timesteps not 0\n",
      "Total Timesteps: 169800 Episode Num: 849 Reward: -124.69440604621462\n",
      "timesteps not 0\n",
      "Total Timesteps: 170000 Episode Num: 850 Reward: -123.3464713283646\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -110.530785\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 170200 Episode Num: 851 Reward: -129.2871638701162\n",
      "timesteps not 0\n",
      "Total Timesteps: 170400 Episode Num: 852 Reward: -125.92262293285862\n",
      "timesteps not 0\n",
      "Total Timesteps: 170600 Episode Num: 853 Reward: -130.12614559585955\n",
      "timesteps not 0\n",
      "Total Timesteps: 170800 Episode Num: 854 Reward: -236.1238269402075\n",
      "timesteps not 0\n",
      "Total Timesteps: 171000 Episode Num: 855 Reward: -121.83735932876407\n",
      "timesteps not 0\n",
      "Total Timesteps: 171200 Episode Num: 856 Reward: -131.615008488677\n",
      "timesteps not 0\n",
      "Total Timesteps: 171400 Episode Num: 857 Reward: -219.00435247181866\n",
      "timesteps not 0\n",
      "Total Timesteps: 171600 Episode Num: 858 Reward: -1.4000943556797865\n",
      "timesteps not 0\n",
      "Total Timesteps: 171800 Episode Num: 859 Reward: -115.20028541263144\n",
      "timesteps not 0\n",
      "Total Timesteps: 172000 Episode Num: 860 Reward: -125.83318671374883\n",
      "timesteps not 0\n",
      "Total Timesteps: 172200 Episode Num: 861 Reward: -222.35819914521986\n",
      "timesteps not 0\n",
      "Total Timesteps: 172400 Episode Num: 862 Reward: -227.28529175868402\n",
      "timesteps not 0\n",
      "Total Timesteps: 172600 Episode Num: 863 Reward: -124.1749521844726\n",
      "timesteps not 0\n",
      "Total Timesteps: 172800 Episode Num: 864 Reward: -119.52438258256807\n",
      "timesteps not 0\n",
      "Total Timesteps: 173000 Episode Num: 865 Reward: -129.37198190416294\n",
      "timesteps not 0\n",
      "Total Timesteps: 173200 Episode Num: 866 Reward: -125.25785255374582\n",
      "timesteps not 0\n",
      "Total Timesteps: 173400 Episode Num: 867 Reward: -120.16713424167979\n",
      "timesteps not 0\n",
      "Total Timesteps: 173600 Episode Num: 868 Reward: -135.0897305552652\n",
      "timesteps not 0\n",
      "Total Timesteps: 173800 Episode Num: 869 Reward: -126.02407778911424\n",
      "timesteps not 0\n",
      "Total Timesteps: 174000 Episode Num: 870 Reward: -7.365130340237292\n",
      "timesteps not 0\n",
      "Total Timesteps: 174200 Episode Num: 871 Reward: -225.2690202378792\n",
      "timesteps not 0\n",
      "Total Timesteps: 174400 Episode Num: 872 Reward: -1.9891896500540116\n",
      "timesteps not 0\n",
      "Total Timesteps: 174600 Episode Num: 873 Reward: -119.40982344729152\n",
      "timesteps not 0\n",
      "Total Timesteps: 174800 Episode Num: 874 Reward: -116.31644184453255\n",
      "timesteps not 0\n",
      "Total Timesteps: 175000 Episode Num: 875 Reward: -126.14148678460347\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -155.394368\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 175200 Episode Num: 876 Reward: -221.63983927148485\n",
      "timesteps not 0\n",
      "Total Timesteps: 175400 Episode Num: 877 Reward: -123.23088445654241\n",
      "timesteps not 0\n",
      "Total Timesteps: 175600 Episode Num: 878 Reward: -128.88277142797037\n",
      "timesteps not 0\n",
      "Total Timesteps: 175800 Episode Num: 879 Reward: -220.35003131002838\n",
      "timesteps not 0\n",
      "Total Timesteps: 176000 Episode Num: 880 Reward: -238.6567323350362\n",
      "timesteps not 0\n",
      "Total Timesteps: 176200 Episode Num: 881 Reward: -219.99511316865923\n",
      "timesteps not 0\n",
      "Total Timesteps: 176400 Episode Num: 882 Reward: -127.57846318844132\n",
      "timesteps not 0\n",
      "Total Timesteps: 176600 Episode Num: 883 Reward: -129.89325043514248\n",
      "timesteps not 0\n",
      "Total Timesteps: 176800 Episode Num: 884 Reward: -130.5020573809924\n",
      "timesteps not 0\n",
      "Total Timesteps: 177000 Episode Num: 885 Reward: -123.65713846974518\n",
      "timesteps not 0\n",
      "Total Timesteps: 177200 Episode Num: 886 Reward: -133.1825285249205\n",
      "timesteps not 0\n",
      "Total Timesteps: 177400 Episode Num: 887 Reward: -224.09777588293866\n",
      "timesteps not 0\n",
      "Total Timesteps: 177600 Episode Num: 888 Reward: -236.18151761967155\n",
      "timesteps not 0\n",
      "Total Timesteps: 177800 Episode Num: 889 Reward: -116.83517234163054\n",
      "timesteps not 0\n",
      "Total Timesteps: 178000 Episode Num: 890 Reward: -127.62634523024698\n",
      "timesteps not 0\n",
      "Total Timesteps: 178200 Episode Num: 891 Reward: -117.80566566373719\n",
      "timesteps not 0\n",
      "Total Timesteps: 178400 Episode Num: 892 Reward: -1.399536547828658\n",
      "timesteps not 0\n",
      "Total Timesteps: 178600 Episode Num: 893 Reward: -227.4198185669418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps not 0\n",
      "Total Timesteps: 178800 Episode Num: 894 Reward: -233.4155298554491\n",
      "timesteps not 0\n",
      "Total Timesteps: 179000 Episode Num: 895 Reward: -116.29894443858566\n",
      "timesteps not 0\n",
      "Total Timesteps: 179200 Episode Num: 896 Reward: -223.34617401385776\n",
      "timesteps not 0\n",
      "Total Timesteps: 179400 Episode Num: 897 Reward: -230.6021541360065\n",
      "timesteps not 0\n",
      "Total Timesteps: 179600 Episode Num: 898 Reward: -126.0245838863683\n",
      "timesteps not 0\n",
      "Total Timesteps: 179800 Episode Num: 899 Reward: -1.2288234685450883\n",
      "timesteps not 0\n",
      "Total Timesteps: 180000 Episode Num: 900 Reward: -124.99994639908014\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -124.372689\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 180200 Episode Num: 901 Reward: -121.99148593394166\n",
      "timesteps not 0\n",
      "Total Timesteps: 180400 Episode Num: 902 Reward: -123.85371000252194\n",
      "timesteps not 0\n",
      "Total Timesteps: 180600 Episode Num: 903 Reward: -126.98425450379726\n",
      "timesteps not 0\n",
      "Total Timesteps: 180800 Episode Num: 904 Reward: -124.7538658523431\n",
      "timesteps not 0\n",
      "Total Timesteps: 181000 Episode Num: 905 Reward: -121.40376879207378\n",
      "timesteps not 0\n",
      "Total Timesteps: 181200 Episode Num: 906 Reward: -128.15243271114525\n",
      "timesteps not 0\n",
      "Total Timesteps: 181400 Episode Num: 907 Reward: -125.49150409728547\n",
      "timesteps not 0\n",
      "Total Timesteps: 181600 Episode Num: 908 Reward: -115.51606272979599\n",
      "timesteps not 0\n",
      "Total Timesteps: 181800 Episode Num: 909 Reward: -4.361677093765729\n",
      "timesteps not 0\n",
      "Total Timesteps: 182000 Episode Num: 910 Reward: -116.32318215143293\n",
      "timesteps not 0\n",
      "Total Timesteps: 182200 Episode Num: 911 Reward: -123.85690377513225\n",
      "timesteps not 0\n",
      "Total Timesteps: 182400 Episode Num: 912 Reward: -125.15225011897807\n",
      "timesteps not 0\n",
      "Total Timesteps: 182600 Episode Num: 913 Reward: -127.22969457474937\n",
      "timesteps not 0\n",
      "Total Timesteps: 182800 Episode Num: 914 Reward: -129.94447208223048\n",
      "timesteps not 0\n",
      "Total Timesteps: 183000 Episode Num: 915 Reward: -114.70453273373622\n",
      "timesteps not 0\n",
      "Total Timesteps: 183200 Episode Num: 916 Reward: -232.7011846091462\n",
      "timesteps not 0\n",
      "Total Timesteps: 183400 Episode Num: 917 Reward: -237.7010401431784\n",
      "timesteps not 0\n",
      "Total Timesteps: 183600 Episode Num: 918 Reward: -323.93802993011127\n",
      "timesteps not 0\n",
      "Total Timesteps: 183800 Episode Num: 919 Reward: -240.00192075581123\n",
      "timesteps not 0\n",
      "Total Timesteps: 184000 Episode Num: 920 Reward: -127.29532280619368\n",
      "timesteps not 0\n",
      "Total Timesteps: 184200 Episode Num: 921 Reward: -120.75633438427887\n",
      "timesteps not 0\n",
      "Total Timesteps: 184400 Episode Num: 922 Reward: -119.00455922970033\n",
      "timesteps not 0\n",
      "Total Timesteps: 184600 Episode Num: 923 Reward: -116.54219858499977\n",
      "timesteps not 0\n",
      "Total Timesteps: 184800 Episode Num: 924 Reward: -118.32756071745368\n",
      "timesteps not 0\n",
      "Total Timesteps: 185000 Episode Num: 925 Reward: -124.95083410360758\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -166.077569\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 185200 Episode Num: 926 Reward: -0.8303319824918969\n",
      "timesteps not 0\n",
      "Total Timesteps: 185400 Episode Num: 927 Reward: -124.54453564180866\n",
      "timesteps not 0\n",
      "Total Timesteps: 185600 Episode Num: 928 Reward: -219.84298692604688\n",
      "timesteps not 0\n",
      "Total Timesteps: 185800 Episode Num: 929 Reward: -124.44202465784335\n",
      "timesteps not 0\n",
      "Total Timesteps: 186000 Episode Num: 930 Reward: -221.2218503345351\n",
      "timesteps not 0\n",
      "Total Timesteps: 186200 Episode Num: 931 Reward: -123.72938513397752\n",
      "timesteps not 0\n",
      "Total Timesteps: 186400 Episode Num: 932 Reward: -232.02897757051764\n",
      "timesteps not 0\n",
      "Total Timesteps: 186600 Episode Num: 933 Reward: -222.1921554825031\n",
      "timesteps not 0\n",
      "Total Timesteps: 186800 Episode Num: 934 Reward: -119.9360681993432\n",
      "timesteps not 0\n",
      "Total Timesteps: 187000 Episode Num: 935 Reward: -229.66935610107083\n",
      "timesteps not 0\n",
      "Total Timesteps: 187200 Episode Num: 936 Reward: -1.4053599122558993\n",
      "timesteps not 0\n",
      "Total Timesteps: 187400 Episode Num: 937 Reward: -125.91714019401805\n",
      "timesteps not 0\n",
      "Total Timesteps: 187600 Episode Num: 938 Reward: -121.65225153307537\n",
      "timesteps not 0\n",
      "Total Timesteps: 187800 Episode Num: 939 Reward: -249.26441071578876\n",
      "timesteps not 0\n",
      "Total Timesteps: 188000 Episode Num: 940 Reward: -116.75216600554201\n",
      "timesteps not 0\n",
      "Total Timesteps: 188200 Episode Num: 941 Reward: -125.09751611908908\n",
      "timesteps not 0\n",
      "Total Timesteps: 188400 Episode Num: 942 Reward: -117.10837339920302\n",
      "timesteps not 0\n",
      "Total Timesteps: 188600 Episode Num: 943 Reward: -131.06213017515967\n",
      "timesteps not 0\n",
      "Total Timesteps: 188800 Episode Num: 944 Reward: -117.72910455738551\n",
      "timesteps not 0\n",
      "Total Timesteps: 189000 Episode Num: 945 Reward: -9.874060851669872\n",
      "timesteps not 0\n",
      "Total Timesteps: 189200 Episode Num: 946 Reward: -237.54001698669725\n",
      "timesteps not 0\n",
      "Total Timesteps: 189400 Episode Num: 947 Reward: -125.39135078584945\n",
      "timesteps not 0\n",
      "Total Timesteps: 189600 Episode Num: 948 Reward: -122.1282867443659\n",
      "timesteps not 0\n",
      "Total Timesteps: 189800 Episode Num: 949 Reward: -117.96294087873468\n",
      "timesteps not 0\n",
      "Total Timesteps: 190000 Episode Num: 950 Reward: -328.213301848604\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -127.606798\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 190200 Episode Num: 951 Reward: -118.35102738958955\n",
      "timesteps not 0\n",
      "Total Timesteps: 190400 Episode Num: 952 Reward: -117.00756508612459\n",
      "timesteps not 0\n",
      "Total Timesteps: 190600 Episode Num: 953 Reward: -118.4596270062477\n",
      "timesteps not 0\n",
      "Total Timesteps: 190800 Episode Num: 954 Reward: -122.2601983268987\n",
      "timesteps not 0\n",
      "Total Timesteps: 191000 Episode Num: 955 Reward: -240.94768199355548\n",
      "timesteps not 0\n",
      "Total Timesteps: 191200 Episode Num: 956 Reward: -125.37332742346187\n",
      "timesteps not 0\n",
      "Total Timesteps: 191400 Episode Num: 957 Reward: -115.570232373233\n",
      "timesteps not 0\n",
      "Total Timesteps: 191600 Episode Num: 958 Reward: -234.6111884060076\n",
      "timesteps not 0\n",
      "Total Timesteps: 191800 Episode Num: 959 Reward: -231.33309652709733\n",
      "timesteps not 0\n",
      "Total Timesteps: 192000 Episode Num: 960 Reward: -230.22784803558832\n",
      "timesteps not 0\n",
      "Total Timesteps: 192200 Episode Num: 961 Reward: -118.81688270294963\n",
      "timesteps not 0\n",
      "Total Timesteps: 192400 Episode Num: 962 Reward: -1.0669140786411808\n",
      "timesteps not 0\n",
      "Total Timesteps: 192600 Episode Num: 963 Reward: -123.60268414899208\n",
      "timesteps not 0\n",
      "Total Timesteps: 192800 Episode Num: 964 Reward: -242.96079216705442\n",
      "timesteps not 0\n",
      "Total Timesteps: 193000 Episode Num: 965 Reward: -115.57085824881032\n",
      "timesteps not 0\n",
      "Total Timesteps: 193200 Episode Num: 966 Reward: -121.72433243764536\n",
      "timesteps not 0\n",
      "Total Timesteps: 193400 Episode Num: 967 Reward: -217.36168739464946\n",
      "timesteps not 0\n",
      "Total Timesteps: 193600 Episode Num: 968 Reward: -126.1966550064852\n",
      "timesteps not 0\n",
      "Total Timesteps: 193800 Episode Num: 969 Reward: -126.94488592078258\n",
      "timesteps not 0\n",
      "Total Timesteps: 194000 Episode Num: 970 Reward: -120.26457506349382\n",
      "timesteps not 0\n",
      "Total Timesteps: 194200 Episode Num: 971 Reward: -233.44883200266239\n",
      "timesteps not 0\n",
      "Total Timesteps: 194400 Episode Num: 972 Reward: -129.74635444219362\n",
      "timesteps not 0\n",
      "Total Timesteps: 194600 Episode Num: 973 Reward: -227.44002244237805\n",
      "timesteps not 0\n",
      "Total Timesteps: 194800 Episode Num: 974 Reward: -125.23205587485218\n",
      "timesteps not 0\n",
      "Total Timesteps: 195000 Episode Num: 975 Reward: -228.44805218397488\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -141.699380\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 195200 Episode Num: 976 Reward: -126.68495533088607\n",
      "timesteps not 0\n",
      "Total Timesteps: 195400 Episode Num: 977 Reward: -125.32010323569395\n",
      "timesteps not 0\n",
      "Total Timesteps: 195600 Episode Num: 978 Reward: -116.37691972983633\n",
      "timesteps not 0\n",
      "Total Timesteps: 195800 Episode Num: 979 Reward: -119.48331305764256\n",
      "timesteps not 0\n",
      "Total Timesteps: 196000 Episode Num: 980 Reward: -126.12930793061685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps not 0\n",
      "Total Timesteps: 196200 Episode Num: 981 Reward: -224.3146176232199\n",
      "timesteps not 0\n",
      "Total Timesteps: 196400 Episode Num: 982 Reward: -113.8136137486279\n",
      "timesteps not 0\n",
      "Total Timesteps: 196600 Episode Num: 983 Reward: -252.0987701515706\n",
      "timesteps not 0\n",
      "Total Timesteps: 196800 Episode Num: 984 Reward: -236.91524657069442\n",
      "timesteps not 0\n",
      "Total Timesteps: 197000 Episode Num: 985 Reward: -117.09489049346874\n",
      "timesteps not 0\n",
      "Total Timesteps: 197200 Episode Num: 986 Reward: -117.78224247638822\n",
      "timesteps not 0\n",
      "Total Timesteps: 197400 Episode Num: 987 Reward: -119.56469354912625\n",
      "timesteps not 0\n",
      "Total Timesteps: 197600 Episode Num: 988 Reward: -229.9944373379731\n",
      "timesteps not 0\n",
      "Total Timesteps: 197800 Episode Num: 989 Reward: -115.14355575302403\n",
      "timesteps not 0\n",
      "Total Timesteps: 198000 Episode Num: 990 Reward: -124.59546436587557\n",
      "timesteps not 0\n",
      "Total Timesteps: 198200 Episode Num: 991 Reward: -1.7095706580765682\n",
      "timesteps not 0\n",
      "Total Timesteps: 198400 Episode Num: 992 Reward: -1.6308936054983232\n",
      "timesteps not 0\n",
      "Total Timesteps: 198600 Episode Num: 993 Reward: -218.1144257613473\n",
      "timesteps not 0\n",
      "Total Timesteps: 198800 Episode Num: 994 Reward: -123.85941208609951\n",
      "timesteps not 0\n",
      "Total Timesteps: 199000 Episode Num: 995 Reward: -120.93114771542646\n",
      "timesteps not 0\n",
      "Total Timesteps: 199200 Episode Num: 996 Reward: -124.87730514947735\n",
      "timesteps not 0\n",
      "Total Timesteps: 199400 Episode Num: 997 Reward: -227.7391472506359\n",
      "timesteps not 0\n",
      "Total Timesteps: 199600 Episode Num: 998 Reward: -122.26824770411869\n",
      "timesteps not 0\n",
      "Total Timesteps: 199800 Episode Num: 999 Reward: -118.76834609295003\n",
      "timesteps not 0\n",
      "Total Timesteps: 200000 Episode Num: 1000 Reward: -6.8357846330479575\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -147.314579\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 200200 Episode Num: 1001 Reward: -114.5767594087333\n",
      "timesteps not 0\n",
      "Total Timesteps: 200400 Episode Num: 1002 Reward: -1.1279468698381645\n",
      "timesteps not 0\n",
      "Total Timesteps: 200600 Episode Num: 1003 Reward: -1.0979740003493004\n",
      "timesteps not 0\n",
      "Total Timesteps: 200800 Episode Num: 1004 Reward: -117.70101320506669\n",
      "timesteps not 0\n",
      "Total Timesteps: 201000 Episode Num: 1005 Reward: -224.03724069139602\n",
      "timesteps not 0\n",
      "Total Timesteps: 201200 Episode Num: 1006 Reward: -5.213361388699041\n",
      "timesteps not 0\n",
      "Total Timesteps: 201400 Episode Num: 1007 Reward: -120.72455769945967\n",
      "timesteps not 0\n",
      "Total Timesteps: 201600 Episode Num: 1008 Reward: -118.52492764671713\n",
      "timesteps not 0\n",
      "Total Timesteps: 201800 Episode Num: 1009 Reward: -120.53925215797618\n",
      "timesteps not 0\n",
      "Total Timesteps: 202000 Episode Num: 1010 Reward: -120.45625769281865\n",
      "timesteps not 0\n",
      "Total Timesteps: 202200 Episode Num: 1011 Reward: -120.34629641941969\n",
      "timesteps not 0\n",
      "Total Timesteps: 202400 Episode Num: 1012 Reward: -227.89876729618246\n",
      "timesteps not 0\n",
      "Total Timesteps: 202600 Episode Num: 1013 Reward: -121.19215543834932\n",
      "timesteps not 0\n",
      "Total Timesteps: 202800 Episode Num: 1014 Reward: -258.416017389406\n",
      "timesteps not 0\n",
      "Total Timesteps: 203000 Episode Num: 1015 Reward: -121.38543911642213\n",
      "timesteps not 0\n",
      "Total Timesteps: 203200 Episode Num: 1016 Reward: -123.96024031808956\n",
      "timesteps not 0\n",
      "Total Timesteps: 203400 Episode Num: 1017 Reward: -228.8157605113378\n",
      "timesteps not 0\n",
      "Total Timesteps: 203600 Episode Num: 1018 Reward: -228.382179590555\n",
      "timesteps not 0\n",
      "Total Timesteps: 203800 Episode Num: 1019 Reward: -130.2858566875876\n",
      "timesteps not 0\n",
      "Total Timesteps: 204000 Episode Num: 1020 Reward: -119.67911871439998\n",
      "timesteps not 0\n",
      "Total Timesteps: 204200 Episode Num: 1021 Reward: -2.3862989902082212\n",
      "timesteps not 0\n",
      "Total Timesteps: 204400 Episode Num: 1022 Reward: -115.89591073666864\n",
      "timesteps not 0\n",
      "Total Timesteps: 204600 Episode Num: 1023 Reward: -127.36263446331093\n",
      "timesteps not 0\n",
      "Total Timesteps: 204800 Episode Num: 1024 Reward: -119.07805152082891\n",
      "timesteps not 0\n",
      "Total Timesteps: 205000 Episode Num: 1025 Reward: -217.37943510407823\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -162.395605\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 205200 Episode Num: 1026 Reward: -120.19476699162433\n",
      "timesteps not 0\n",
      "Total Timesteps: 205400 Episode Num: 1027 Reward: -120.61348502436003\n",
      "timesteps not 0\n",
      "Total Timesteps: 205600 Episode Num: 1028 Reward: -120.64844949391488\n",
      "timesteps not 0\n",
      "Total Timesteps: 205800 Episode Num: 1029 Reward: -219.43535431181039\n",
      "timesteps not 0\n",
      "Total Timesteps: 206000 Episode Num: 1030 Reward: -235.672620864136\n",
      "timesteps not 0\n",
      "Total Timesteps: 206200 Episode Num: 1031 Reward: -127.42405010321778\n",
      "timesteps not 0\n",
      "Total Timesteps: 206400 Episode Num: 1032 Reward: -229.91216733958802\n",
      "timesteps not 0\n",
      "Total Timesteps: 206600 Episode Num: 1033 Reward: -232.3220365373342\n",
      "timesteps not 0\n",
      "Total Timesteps: 206800 Episode Num: 1034 Reward: -126.41155744353692\n",
      "timesteps not 0\n",
      "Total Timesteps: 207000 Episode Num: 1035 Reward: -118.28728735670737\n",
      "timesteps not 0\n",
      "Total Timesteps: 207200 Episode Num: 1036 Reward: -116.7221366325535\n",
      "timesteps not 0\n",
      "Total Timesteps: 207400 Episode Num: 1037 Reward: -122.40163469407105\n",
      "timesteps not 0\n",
      "Total Timesteps: 207600 Episode Num: 1038 Reward: -125.70944501419868\n",
      "timesteps not 0\n",
      "Total Timesteps: 207800 Episode Num: 1039 Reward: -123.1361894924701\n",
      "timesteps not 0\n",
      "Total Timesteps: 208000 Episode Num: 1040 Reward: -241.53207903297323\n",
      "timesteps not 0\n",
      "Total Timesteps: 208200 Episode Num: 1041 Reward: -225.6046023118963\n",
      "timesteps not 0\n",
      "Total Timesteps: 208400 Episode Num: 1042 Reward: -119.34491074855416\n",
      "timesteps not 0\n",
      "Total Timesteps: 208600 Episode Num: 1043 Reward: -127.95707267490815\n",
      "timesteps not 0\n",
      "Total Timesteps: 208800 Episode Num: 1044 Reward: -1.6521507157062865\n",
      "timesteps not 0\n",
      "Total Timesteps: 209000 Episode Num: 1045 Reward: -229.18852626015138\n",
      "timesteps not 0\n",
      "Total Timesteps: 209200 Episode Num: 1046 Reward: -3.9440744161838115\n",
      "timesteps not 0\n",
      "Total Timesteps: 209400 Episode Num: 1047 Reward: -226.4895537649014\n",
      "timesteps not 0\n",
      "Total Timesteps: 209600 Episode Num: 1048 Reward: -128.39156729480302\n",
      "timesteps not 0\n",
      "Total Timesteps: 209800 Episode Num: 1049 Reward: -121.83014553610862\n",
      "timesteps not 0\n",
      "Total Timesteps: 210000 Episode Num: 1050 Reward: -120.33208133912768\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -109.096226\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 210200 Episode Num: 1051 Reward: -4.960416199728096\n",
      "timesteps not 0\n",
      "Total Timesteps: 210400 Episode Num: 1052 Reward: -121.39865619343439\n",
      "timesteps not 0\n",
      "Total Timesteps: 210600 Episode Num: 1053 Reward: -3.230210310283098\n",
      "timesteps not 0\n",
      "Total Timesteps: 210800 Episode Num: 1054 Reward: -129.33434370989775\n",
      "timesteps not 0\n",
      "Total Timesteps: 211000 Episode Num: 1055 Reward: -126.6045829870543\n",
      "timesteps not 0\n",
      "Total Timesteps: 211200 Episode Num: 1056 Reward: -118.91501871077399\n",
      "timesteps not 0\n",
      "Total Timesteps: 211400 Episode Num: 1057 Reward: -244.88008548000852\n",
      "timesteps not 0\n",
      "Total Timesteps: 211600 Episode Num: 1058 Reward: -295.08753891259096\n",
      "timesteps not 0\n",
      "Total Timesteps: 211800 Episode Num: 1059 Reward: -227.6603542487834\n",
      "timesteps not 0\n",
      "Total Timesteps: 212000 Episode Num: 1060 Reward: -132.63837886099478\n",
      "timesteps not 0\n",
      "Total Timesteps: 212200 Episode Num: 1061 Reward: -124.44863639365241\n",
      "timesteps not 0\n",
      "Total Timesteps: 212400 Episode Num: 1062 Reward: -225.9875460173296\n",
      "timesteps not 0\n",
      "Total Timesteps: 212600 Episode Num: 1063 Reward: -238.47812873643718\n",
      "timesteps not 0\n",
      "Total Timesteps: 212800 Episode Num: 1064 Reward: -132.2236543267842\n",
      "timesteps not 0\n",
      "Total Timesteps: 213000 Episode Num: 1065 Reward: -242.2265926335527\n",
      "timesteps not 0\n",
      "Total Timesteps: 213200 Episode Num: 1066 Reward: -6.795624264083509\n",
      "timesteps not 0\n",
      "Total Timesteps: 213400 Episode Num: 1067 Reward: -128.40903324447547\n",
      "timesteps not 0\n",
      "Total Timesteps: 213600 Episode Num: 1068 Reward: -258.0319250650075\n",
      "timesteps not 0\n",
      "Total Timesteps: 213800 Episode Num: 1069 Reward: -125.87187897084969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps not 0\n",
      "Total Timesteps: 214000 Episode Num: 1070 Reward: -322.6195823359443\n",
      "timesteps not 0\n",
      "Total Timesteps: 214200 Episode Num: 1071 Reward: -9.074466103215194\n",
      "timesteps not 0\n",
      "Total Timesteps: 214400 Episode Num: 1072 Reward: -8.850920762707783\n",
      "timesteps not 0\n",
      "Total Timesteps: 214600 Episode Num: 1073 Reward: -222.9054556360379\n",
      "timesteps not 0\n",
      "Total Timesteps: 214800 Episode Num: 1074 Reward: -129.20033013383662\n",
      "timesteps not 0\n",
      "Total Timesteps: 215000 Episode Num: 1075 Reward: -239.54180009449087\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -149.091468\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 215200 Episode Num: 1076 Reward: -125.5417885307917\n",
      "timesteps not 0\n",
      "Total Timesteps: 215400 Episode Num: 1077 Reward: -239.37167681961805\n",
      "timesteps not 0\n",
      "Total Timesteps: 215600 Episode Num: 1078 Reward: -232.5352000321118\n",
      "timesteps not 0\n",
      "Total Timesteps: 215800 Episode Num: 1079 Reward: -9.594334095073265\n",
      "timesteps not 0\n",
      "Total Timesteps: 216000 Episode Num: 1080 Reward: -8.260553095962116\n",
      "timesteps not 0\n",
      "Total Timesteps: 216200 Episode Num: 1081 Reward: -7.387226747890034\n",
      "timesteps not 0\n",
      "Total Timesteps: 216400 Episode Num: 1082 Reward: -126.26829686496687\n",
      "timesteps not 0\n",
      "Total Timesteps: 216600 Episode Num: 1083 Reward: -222.82515867332043\n",
      "timesteps not 0\n",
      "Total Timesteps: 216800 Episode Num: 1084 Reward: -227.9377737554432\n",
      "timesteps not 0\n",
      "Total Timesteps: 217000 Episode Num: 1085 Reward: -125.84354417244364\n",
      "timesteps not 0\n",
      "Total Timesteps: 217200 Episode Num: 1086 Reward: -117.32164432247428\n",
      "timesteps not 0\n",
      "Total Timesteps: 217400 Episode Num: 1087 Reward: -121.78776438160476\n",
      "timesteps not 0\n",
      "Total Timesteps: 217600 Episode Num: 1088 Reward: -130.40842930954295\n",
      "timesteps not 0\n",
      "Total Timesteps: 217800 Episode Num: 1089 Reward: -225.09978869574545\n",
      "timesteps not 0\n",
      "Total Timesteps: 218000 Episode Num: 1090 Reward: -125.28057061484112\n",
      "timesteps not 0\n",
      "Total Timesteps: 218200 Episode Num: 1091 Reward: -350.5259604878598\n",
      "timesteps not 0\n",
      "Total Timesteps: 218400 Episode Num: 1092 Reward: -291.89545317204636\n",
      "timesteps not 0\n",
      "Total Timesteps: 218600 Episode Num: 1093 Reward: -131.50467023343265\n",
      "timesteps not 0\n",
      "Total Timesteps: 218800 Episode Num: 1094 Reward: -129.61479277171964\n",
      "timesteps not 0\n",
      "Total Timesteps: 219000 Episode Num: 1095 Reward: -121.86160974818826\n",
      "timesteps not 0\n",
      "Total Timesteps: 219200 Episode Num: 1096 Reward: -132.9245907001152\n",
      "timesteps not 0\n",
      "Total Timesteps: 219400 Episode Num: 1097 Reward: -230.21852615102884\n",
      "timesteps not 0\n",
      "Total Timesteps: 219600 Episode Num: 1098 Reward: -131.30036393142896\n",
      "timesteps not 0\n",
      "Total Timesteps: 219800 Episode Num: 1099 Reward: -237.46180664428888\n",
      "timesteps not 0\n",
      "Total Timesteps: 220000 Episode Num: 1100 Reward: -130.69091097279002\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -104.095499\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 220200 Episode Num: 1101 Reward: -133.5769332150407\n",
      "timesteps not 0\n",
      "Total Timesteps: 220400 Episode Num: 1102 Reward: -120.19891135352762\n",
      "timesteps not 0\n",
      "Total Timesteps: 220600 Episode Num: 1103 Reward: -118.65899150641617\n",
      "timesteps not 0\n",
      "Total Timesteps: 220800 Episode Num: 1104 Reward: -129.34033078606146\n",
      "timesteps not 0\n",
      "Total Timesteps: 221000 Episode Num: 1105 Reward: -5.842924255178755\n",
      "timesteps not 0\n",
      "Total Timesteps: 221200 Episode Num: 1106 Reward: -127.67726168149484\n",
      "timesteps not 0\n",
      "Total Timesteps: 221400 Episode Num: 1107 Reward: -245.3203104814764\n",
      "timesteps not 0\n",
      "Total Timesteps: 221600 Episode Num: 1108 Reward: -312.4172274941422\n",
      "timesteps not 0\n",
      "Total Timesteps: 221800 Episode Num: 1109 Reward: -123.43770256439969\n",
      "timesteps not 0\n",
      "Total Timesteps: 222000 Episode Num: 1110 Reward: -126.16421537467644\n",
      "timesteps not 0\n",
      "Total Timesteps: 222200 Episode Num: 1111 Reward: -118.30709309006993\n",
      "timesteps not 0\n",
      "Total Timesteps: 222400 Episode Num: 1112 Reward: -380.8578972417542\n",
      "timesteps not 0\n",
      "Total Timesteps: 222600 Episode Num: 1113 Reward: -129.3841378451335\n",
      "timesteps not 0\n",
      "Total Timesteps: 222800 Episode Num: 1114 Reward: -353.58557782033023\n",
      "timesteps not 0\n",
      "Total Timesteps: 223000 Episode Num: 1115 Reward: -6.108948116400413\n",
      "timesteps not 0\n",
      "Total Timesteps: 223200 Episode Num: 1116 Reward: -222.85215961614753\n",
      "timesteps not 0\n",
      "Total Timesteps: 223400 Episode Num: 1117 Reward: -1.9470744417556378\n",
      "timesteps not 0\n",
      "Total Timesteps: 223600 Episode Num: 1118 Reward: -229.51264438822346\n",
      "timesteps not 0\n",
      "Total Timesteps: 223800 Episode Num: 1119 Reward: -122.28023073174792\n",
      "timesteps not 0\n",
      "Total Timesteps: 224000 Episode Num: 1120 Reward: -123.54687827254772\n",
      "timesteps not 0\n",
      "Total Timesteps: 224200 Episode Num: 1121 Reward: -126.97461720267493\n",
      "timesteps not 0\n",
      "Total Timesteps: 224400 Episode Num: 1122 Reward: -122.88851133134969\n",
      "timesteps not 0\n",
      "Total Timesteps: 224600 Episode Num: 1123 Reward: -227.49056183357462\n",
      "timesteps not 0\n",
      "Total Timesteps: 224800 Episode Num: 1124 Reward: -218.4507575355537\n",
      "timesteps not 0\n",
      "Total Timesteps: 225000 Episode Num: 1125 Reward: -227.69887358496607\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -140.468921\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 225200 Episode Num: 1126 Reward: -229.22041099859646\n",
      "timesteps not 0\n",
      "Total Timesteps: 225400 Episode Num: 1127 Reward: -126.26262147907768\n",
      "timesteps not 0\n",
      "Total Timesteps: 225600 Episode Num: 1128 Reward: -127.0343164930853\n",
      "timesteps not 0\n",
      "Total Timesteps: 225800 Episode Num: 1129 Reward: -116.67209360489191\n",
      "timesteps not 0\n",
      "Total Timesteps: 226000 Episode Num: 1130 Reward: -221.72953979333565\n",
      "timesteps not 0\n",
      "Total Timesteps: 226200 Episode Num: 1131 Reward: -1.5408319847386\n",
      "timesteps not 0\n",
      "Total Timesteps: 226400 Episode Num: 1132 Reward: -118.86579542081225\n",
      "timesteps not 0\n",
      "Total Timesteps: 226600 Episode Num: 1133 Reward: -5.197673010486984\n",
      "timesteps not 0\n",
      "Total Timesteps: 226800 Episode Num: 1134 Reward: -234.35527318549092\n",
      "timesteps not 0\n",
      "Total Timesteps: 227000 Episode Num: 1135 Reward: -233.79859164469195\n",
      "timesteps not 0\n",
      "Total Timesteps: 227200 Episode Num: 1136 Reward: -126.68085760875034\n",
      "timesteps not 0\n",
      "Total Timesteps: 227400 Episode Num: 1137 Reward: -127.18956737788294\n",
      "timesteps not 0\n",
      "Total Timesteps: 227600 Episode Num: 1138 Reward: -131.12311595014623\n",
      "timesteps not 0\n",
      "Total Timesteps: 227800 Episode Num: 1139 Reward: -130.3452585550788\n",
      "timesteps not 0\n",
      "Total Timesteps: 228000 Episode Num: 1140 Reward: -128.517102209468\n",
      "timesteps not 0\n",
      "Total Timesteps: 228200 Episode Num: 1141 Reward: -7.045392050047137\n",
      "timesteps not 0\n",
      "Total Timesteps: 228400 Episode Num: 1142 Reward: -122.48650698098898\n",
      "timesteps not 0\n",
      "Total Timesteps: 228600 Episode Num: 1143 Reward: -237.5319484563299\n",
      "timesteps not 0\n",
      "Total Timesteps: 228800 Episode Num: 1144 Reward: -117.39148598961117\n",
      "timesteps not 0\n",
      "Total Timesteps: 229000 Episode Num: 1145 Reward: -121.91951437675121\n",
      "timesteps not 0\n",
      "Total Timesteps: 229200 Episode Num: 1146 Reward: -2.2392480042981244\n",
      "timesteps not 0\n",
      "Total Timesteps: 229400 Episode Num: 1147 Reward: -226.86187483950894\n",
      "timesteps not 0\n",
      "Total Timesteps: 229600 Episode Num: 1148 Reward: -127.9931365795289\n",
      "timesteps not 0\n",
      "Total Timesteps: 229800 Episode Num: 1149 Reward: -123.38630154918623\n",
      "timesteps not 0\n",
      "Total Timesteps: 230000 Episode Num: 1150 Reward: -231.55501672321108\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -140.312306\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 230200 Episode Num: 1151 Reward: -119.41660328385126\n",
      "timesteps not 0\n",
      "Total Timesteps: 230400 Episode Num: 1152 Reward: -126.06809550102622\n",
      "timesteps not 0\n",
      "Total Timesteps: 230600 Episode Num: 1153 Reward: -130.26493123603007\n",
      "timesteps not 0\n",
      "Total Timesteps: 230800 Episode Num: 1154 Reward: -130.26976990313037\n",
      "timesteps not 0\n",
      "Total Timesteps: 231000 Episode Num: 1155 Reward: -125.43909854621006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps not 0\n",
      "Total Timesteps: 231200 Episode Num: 1156 Reward: -241.4938892748413\n",
      "timesteps not 0\n",
      "Total Timesteps: 231400 Episode Num: 1157 Reward: -127.99696119879515\n",
      "timesteps not 0\n",
      "Total Timesteps: 231600 Episode Num: 1158 Reward: -223.28561147626436\n",
      "timesteps not 0\n",
      "Total Timesteps: 231800 Episode Num: 1159 Reward: -128.97252206193326\n",
      "timesteps not 0\n",
      "Total Timesteps: 232000 Episode Num: 1160 Reward: -128.6797018472803\n",
      "timesteps not 0\n",
      "Total Timesteps: 232200 Episode Num: 1161 Reward: -9.194321900641441\n",
      "timesteps not 0\n",
      "Total Timesteps: 232400 Episode Num: 1162 Reward: -122.50314640339523\n",
      "timesteps not 0\n",
      "Total Timesteps: 232600 Episode Num: 1163 Reward: -292.87206048225676\n",
      "timesteps not 0\n",
      "Total Timesteps: 232800 Episode Num: 1164 Reward: -120.47378015057394\n",
      "timesteps not 0\n",
      "Total Timesteps: 233000 Episode Num: 1165 Reward: -125.42355960470671\n",
      "timesteps not 0\n",
      "Total Timesteps: 233200 Episode Num: 1166 Reward: -247.82780396339052\n",
      "timesteps not 0\n",
      "Total Timesteps: 233400 Episode Num: 1167 Reward: -3.394515021710314\n",
      "timesteps not 0\n",
      "Total Timesteps: 233600 Episode Num: 1168 Reward: -123.44525943898964\n",
      "timesteps not 0\n",
      "Total Timesteps: 233800 Episode Num: 1169 Reward: -122.53621037532781\n",
      "timesteps not 0\n",
      "Total Timesteps: 234000 Episode Num: 1170 Reward: -120.32774948209953\n",
      "timesteps not 0\n",
      "Total Timesteps: 234200 Episode Num: 1171 Reward: -131.89552313688827\n",
      "timesteps not 0\n",
      "Total Timesteps: 234400 Episode Num: 1172 Reward: -228.45772226886885\n",
      "timesteps not 0\n",
      "Total Timesteps: 234600 Episode Num: 1173 Reward: -128.4333382111176\n",
      "timesteps not 0\n",
      "Total Timesteps: 234800 Episode Num: 1174 Reward: -4.519038347726099\n",
      "timesteps not 0\n",
      "Total Timesteps: 235000 Episode Num: 1175 Reward: -130.32959215369223\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -111.500762\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 235200 Episode Num: 1176 Reward: -227.38182064377705\n",
      "timesteps not 0\n",
      "Total Timesteps: 235400 Episode Num: 1177 Reward: -6.720421435061359\n",
      "timesteps not 0\n",
      "Total Timesteps: 235600 Episode Num: 1178 Reward: -127.69966873774145\n",
      "timesteps not 0\n",
      "Total Timesteps: 235800 Episode Num: 1179 Reward: -125.4185467026044\n",
      "timesteps not 0\n",
      "Total Timesteps: 236000 Episode Num: 1180 Reward: -128.04462697689542\n",
      "timesteps not 0\n",
      "Total Timesteps: 236200 Episode Num: 1181 Reward: -3.8102509623520326\n",
      "timesteps not 0\n",
      "Total Timesteps: 236400 Episode Num: 1182 Reward: -233.18032371630255\n",
      "timesteps not 0\n",
      "Total Timesteps: 236600 Episode Num: 1183 Reward: -228.69844879932313\n",
      "timesteps not 0\n",
      "Total Timesteps: 236800 Episode Num: 1184 Reward: -120.3382719218036\n",
      "timesteps not 0\n",
      "Total Timesteps: 237000 Episode Num: 1185 Reward: -4.345010183292434\n",
      "timesteps not 0\n",
      "Total Timesteps: 237200 Episode Num: 1186 Reward: -118.3771169488281\n",
      "timesteps not 0\n",
      "Total Timesteps: 237400 Episode Num: 1187 Reward: -228.20763122973264\n",
      "timesteps not 0\n",
      "Total Timesteps: 237600 Episode Num: 1188 Reward: -231.04196878361344\n",
      "timesteps not 0\n",
      "Total Timesteps: 237800 Episode Num: 1189 Reward: -229.2777235226689\n",
      "timesteps not 0\n",
      "Total Timesteps: 238000 Episode Num: 1190 Reward: -117.49831171840673\n",
      "timesteps not 0\n",
      "Total Timesteps: 238200 Episode Num: 1191 Reward: -2.2342048948289968\n",
      "timesteps not 0\n",
      "Total Timesteps: 238400 Episode Num: 1192 Reward: -116.94230087888229\n",
      "timesteps not 0\n",
      "Total Timesteps: 238600 Episode Num: 1193 Reward: -127.6088008785606\n",
      "timesteps not 0\n",
      "Total Timesteps: 238800 Episode Num: 1194 Reward: -114.98797561792138\n",
      "timesteps not 0\n",
      "Total Timesteps: 239000 Episode Num: 1195 Reward: -119.81570413646998\n",
      "timesteps not 0\n",
      "Total Timesteps: 239200 Episode Num: 1196 Reward: -1.6793294635507179\n",
      "timesteps not 0\n",
      "Total Timesteps: 239400 Episode Num: 1197 Reward: -122.58732950776324\n",
      "timesteps not 0\n",
      "Total Timesteps: 239600 Episode Num: 1198 Reward: -226.3176130988885\n",
      "timesteps not 0\n",
      "Total Timesteps: 239800 Episode Num: 1199 Reward: -126.00554942856311\n",
      "timesteps not 0\n",
      "Total Timesteps: 240000 Episode Num: 1200 Reward: -3.082920230773099\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -142.358451\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 240200 Episode Num: 1201 Reward: -230.74692915267187\n",
      "timesteps not 0\n",
      "Total Timesteps: 240400 Episode Num: 1202 Reward: -127.14696326870286\n",
      "timesteps not 0\n",
      "Total Timesteps: 240600 Episode Num: 1203 Reward: -116.33006766884782\n",
      "timesteps not 0\n",
      "Total Timesteps: 240800 Episode Num: 1204 Reward: -227.90178647319058\n",
      "timesteps not 0\n",
      "Total Timesteps: 241000 Episode Num: 1205 Reward: -123.93352783464054\n",
      "timesteps not 0\n",
      "Total Timesteps: 241200 Episode Num: 1206 Reward: -119.45565645787889\n",
      "timesteps not 0\n",
      "Total Timesteps: 241400 Episode Num: 1207 Reward: -221.23599045296146\n",
      "timesteps not 0\n",
      "Total Timesteps: 241600 Episode Num: 1208 Reward: -244.3947919708962\n",
      "timesteps not 0\n",
      "Total Timesteps: 241800 Episode Num: 1209 Reward: -120.66581468224481\n",
      "timesteps not 0\n",
      "Total Timesteps: 242000 Episode Num: 1210 Reward: -304.6605855700096\n",
      "timesteps not 0\n",
      "Total Timesteps: 242200 Episode Num: 1211 Reward: -223.53342563788547\n",
      "timesteps not 0\n",
      "Total Timesteps: 242400 Episode Num: 1212 Reward: -122.28430497709138\n",
      "timesteps not 0\n",
      "Total Timesteps: 242600 Episode Num: 1213 Reward: -127.55444278220892\n",
      "timesteps not 0\n",
      "Total Timesteps: 242800 Episode Num: 1214 Reward: -133.48536374672332\n",
      "timesteps not 0\n",
      "Total Timesteps: 243000 Episode Num: 1215 Reward: -126.12449102179235\n",
      "timesteps not 0\n",
      "Total Timesteps: 243200 Episode Num: 1216 Reward: -122.62334534643827\n",
      "timesteps not 0\n",
      "Total Timesteps: 243400 Episode Num: 1217 Reward: -4.6585185131202325\n",
      "timesteps not 0\n",
      "Total Timesteps: 243600 Episode Num: 1218 Reward: -119.63536592275577\n",
      "timesteps not 0\n",
      "Total Timesteps: 243800 Episode Num: 1219 Reward: -128.28849057062507\n",
      "timesteps not 0\n",
      "Total Timesteps: 244000 Episode Num: 1220 Reward: -115.16171945995968\n",
      "timesteps not 0\n",
      "Total Timesteps: 244200 Episode Num: 1221 Reward: -127.90383435121146\n",
      "timesteps not 0\n",
      "Total Timesteps: 244400 Episode Num: 1222 Reward: -375.526754927973\n",
      "timesteps not 0\n",
      "Total Timesteps: 244600 Episode Num: 1223 Reward: -117.838368805085\n",
      "timesteps not 0\n",
      "Total Timesteps: 244800 Episode Num: 1224 Reward: -125.83154031486443\n",
      "timesteps not 0\n",
      "Total Timesteps: 245000 Episode Num: 1225 Reward: -128.49122448444191\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -121.276208\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 245200 Episode Num: 1226 Reward: -121.17293810050374\n",
      "timesteps not 0\n",
      "Total Timesteps: 245400 Episode Num: 1227 Reward: -121.67555037981298\n",
      "timesteps not 0\n",
      "Total Timesteps: 245600 Episode Num: 1228 Reward: -125.87179352444623\n",
      "timesteps not 0\n",
      "Total Timesteps: 245800 Episode Num: 1229 Reward: -128.7017552918227\n",
      "timesteps not 0\n",
      "Total Timesteps: 246000 Episode Num: 1230 Reward: -122.98123841256965\n",
      "timesteps not 0\n",
      "Total Timesteps: 246200 Episode Num: 1231 Reward: -125.75439192213341\n",
      "timesteps not 0\n",
      "Total Timesteps: 246400 Episode Num: 1232 Reward: -121.38425429421217\n",
      "timesteps not 0\n",
      "Total Timesteps: 246600 Episode Num: 1233 Reward: -121.74552819154452\n",
      "timesteps not 0\n",
      "Total Timesteps: 246800 Episode Num: 1234 Reward: -234.29558409359228\n",
      "timesteps not 0\n",
      "Total Timesteps: 247000 Episode Num: 1235 Reward: -2.421304214248565\n",
      "timesteps not 0\n",
      "Total Timesteps: 247200 Episode Num: 1236 Reward: -124.85161820006263\n",
      "timesteps not 0\n",
      "Total Timesteps: 247400 Episode Num: 1237 Reward: -126.6891655150819\n",
      "timesteps not 0\n",
      "Total Timesteps: 247600 Episode Num: 1238 Reward: -3.2725067285806393\n",
      "timesteps not 0\n",
      "Total Timesteps: 247800 Episode Num: 1239 Reward: -129.68555867883285\n",
      "timesteps not 0\n",
      "Total Timesteps: 248000 Episode Num: 1240 Reward: -129.66921538439226\n",
      "timesteps not 0\n",
      "Total Timesteps: 248200 Episode Num: 1241 Reward: -2.847425139820256\n",
      "timesteps not 0\n",
      "Total Timesteps: 248400 Episode Num: 1242 Reward: -121.50430610836015\n",
      "timesteps not 0\n",
      "Total Timesteps: 248600 Episode Num: 1243 Reward: -118.30629187998788\n",
      "timesteps not 0\n",
      "Total Timesteps: 248800 Episode Num: 1244 Reward: -3.7896966080684122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps not 0\n",
      "Total Timesteps: 249000 Episode Num: 1245 Reward: -234.69336557303617\n",
      "timesteps not 0\n",
      "Total Timesteps: 249200 Episode Num: 1246 Reward: -122.19495337657851\n",
      "timesteps not 0\n",
      "Total Timesteps: 249400 Episode Num: 1247 Reward: -248.73103344055082\n",
      "timesteps not 0\n",
      "Total Timesteps: 249600 Episode Num: 1248 Reward: -225.38430411670035\n",
      "timesteps not 0\n",
      "Total Timesteps: 249800 Episode Num: 1249 Reward: -118.10932847848302\n",
      "timesteps not 0\n",
      "Total Timesteps: 250000 Episode Num: 1250 Reward: -129.77557925580174\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -158.557349\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 250200 Episode Num: 1251 Reward: -3.415306985345081\n",
      "timesteps not 0\n",
      "Total Timesteps: 250400 Episode Num: 1252 Reward: -133.77867966090577\n",
      "timesteps not 0\n",
      "Total Timesteps: 250600 Episode Num: 1253 Reward: -338.44633626447006\n",
      "timesteps not 0\n",
      "Total Timesteps: 250800 Episode Num: 1254 Reward: -123.07800055977526\n",
      "timesteps not 0\n",
      "Total Timesteps: 251000 Episode Num: 1255 Reward: -3.1550397898502562\n",
      "timesteps not 0\n",
      "Total Timesteps: 251200 Episode Num: 1256 Reward: -123.83610104894679\n",
      "timesteps not 0\n",
      "Total Timesteps: 251400 Episode Num: 1257 Reward: -115.85574195334226\n",
      "timesteps not 0\n",
      "Total Timesteps: 251600 Episode Num: 1258 Reward: -118.37697608363956\n",
      "timesteps not 0\n",
      "Total Timesteps: 251800 Episode Num: 1259 Reward: -129.32936678897067\n",
      "timesteps not 0\n",
      "Total Timesteps: 252000 Episode Num: 1260 Reward: -218.23524611903872\n",
      "timesteps not 0\n",
      "Total Timesteps: 252200 Episode Num: 1261 Reward: -128.03347359580383\n",
      "timesteps not 0\n",
      "Total Timesteps: 252400 Episode Num: 1262 Reward: -125.6627000919522\n",
      "timesteps not 0\n",
      "Total Timesteps: 252600 Episode Num: 1263 Reward: -224.47785130375985\n",
      "timesteps not 0\n",
      "Total Timesteps: 252800 Episode Num: 1264 Reward: -248.4498829903568\n",
      "timesteps not 0\n",
      "Total Timesteps: 253000 Episode Num: 1265 Reward: -119.48966814590987\n",
      "timesteps not 0\n",
      "Total Timesteps: 253200 Episode Num: 1266 Reward: -127.4964365922735\n",
      "timesteps not 0\n",
      "Total Timesteps: 253400 Episode Num: 1267 Reward: -117.53436809950023\n",
      "timesteps not 0\n",
      "Total Timesteps: 253600 Episode Num: 1268 Reward: -245.19933700886517\n",
      "timesteps not 0\n",
      "Total Timesteps: 253800 Episode Num: 1269 Reward: -119.81278207419119\n",
      "timesteps not 0\n",
      "Total Timesteps: 254000 Episode Num: 1270 Reward: -126.38946943551046\n",
      "timesteps not 0\n",
      "Total Timesteps: 254200 Episode Num: 1271 Reward: -115.66515783808828\n",
      "timesteps not 0\n",
      "Total Timesteps: 254400 Episode Num: 1272 Reward: -121.72629008448494\n",
      "timesteps not 0\n",
      "Total Timesteps: 254600 Episode Num: 1273 Reward: -117.7760833846324\n",
      "timesteps not 0\n",
      "Total Timesteps: 254800 Episode Num: 1274 Reward: -2.066233541488406\n",
      "timesteps not 0\n",
      "Total Timesteps: 255000 Episode Num: 1275 Reward: -323.69532092974174\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -163.505301\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 255200 Episode Num: 1276 Reward: -224.96507569136202\n",
      "timesteps not 0\n",
      "Total Timesteps: 255400 Episode Num: 1277 Reward: -118.78280558582081\n",
      "timesteps not 0\n",
      "Total Timesteps: 255600 Episode Num: 1278 Reward: -221.04650363044794\n",
      "timesteps not 0\n",
      "Total Timesteps: 255800 Episode Num: 1279 Reward: -244.7576628110869\n",
      "timesteps not 0\n",
      "Total Timesteps: 256000 Episode Num: 1280 Reward: -120.2636022046718\n",
      "timesteps not 0\n",
      "Total Timesteps: 256200 Episode Num: 1281 Reward: -115.69489493086132\n",
      "timesteps not 0\n",
      "Total Timesteps: 256400 Episode Num: 1282 Reward: -356.06007263028664\n",
      "timesteps not 0\n",
      "Total Timesteps: 256600 Episode Num: 1283 Reward: -4.679217624384287\n",
      "timesteps not 0\n",
      "Total Timesteps: 256800 Episode Num: 1284 Reward: -120.47102642032067\n",
      "timesteps not 0\n",
      "Total Timesteps: 257000 Episode Num: 1285 Reward: -126.69043075908974\n",
      "timesteps not 0\n",
      "Total Timesteps: 257200 Episode Num: 1286 Reward: -238.50237616475545\n",
      "timesteps not 0\n",
      "Total Timesteps: 257400 Episode Num: 1287 Reward: -242.5403096577686\n",
      "timesteps not 0\n",
      "Total Timesteps: 257600 Episode Num: 1288 Reward: -122.08848954802791\n",
      "timesteps not 0\n",
      "Total Timesteps: 257800 Episode Num: 1289 Reward: -126.45707166384977\n",
      "timesteps not 0\n",
      "Total Timesteps: 258000 Episode Num: 1290 Reward: -117.57090316000837\n",
      "timesteps not 0\n",
      "Total Timesteps: 258200 Episode Num: 1291 Reward: -247.11878102588668\n",
      "timesteps not 0\n",
      "Total Timesteps: 258400 Episode Num: 1292 Reward: -3.4990609471022056\n",
      "timesteps not 0\n",
      "Total Timesteps: 258600 Episode Num: 1293 Reward: -121.92987754024308\n",
      "timesteps not 0\n",
      "Total Timesteps: 258800 Episode Num: 1294 Reward: -126.07755601476727\n",
      "timesteps not 0\n",
      "Total Timesteps: 259000 Episode Num: 1295 Reward: -222.0109693317411\n",
      "timesteps not 0\n",
      "Total Timesteps: 259200 Episode Num: 1296 Reward: -125.28585820809946\n",
      "timesteps not 0\n",
      "Total Timesteps: 259400 Episode Num: 1297 Reward: -125.81824783037133\n",
      "timesteps not 0\n",
      "Total Timesteps: 259600 Episode Num: 1298 Reward: -229.57307495879672\n",
      "timesteps not 0\n",
      "Total Timesteps: 259800 Episode Num: 1299 Reward: -121.98924518823408\n",
      "timesteps not 0\n",
      "Total Timesteps: 260000 Episode Num: 1300 Reward: -2.181121271514803\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -127.128238\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 260200 Episode Num: 1301 Reward: -227.01215457365043\n",
      "timesteps not 0\n",
      "Total Timesteps: 260400 Episode Num: 1302 Reward: -128.9755137069963\n",
      "timesteps not 0\n",
      "Total Timesteps: 260600 Episode Num: 1303 Reward: -117.93410529506912\n",
      "timesteps not 0\n",
      "Total Timesteps: 260800 Episode Num: 1304 Reward: -118.0700024635244\n",
      "timesteps not 0\n",
      "Total Timesteps: 261000 Episode Num: 1305 Reward: -291.68665949438747\n",
      "timesteps not 0\n",
      "Total Timesteps: 261200 Episode Num: 1306 Reward: -121.28414836653086\n",
      "timesteps not 0\n",
      "Total Timesteps: 261400 Episode Num: 1307 Reward: -128.34418693458713\n",
      "timesteps not 0\n",
      "Total Timesteps: 261600 Episode Num: 1308 Reward: -129.42690841641786\n",
      "timesteps not 0\n",
      "Total Timesteps: 261800 Episode Num: 1309 Reward: -304.58373387690096\n",
      "timesteps not 0\n",
      "Total Timesteps: 262000 Episode Num: 1310 Reward: -123.82138813805533\n",
      "timesteps not 0\n",
      "Total Timesteps: 262200 Episode Num: 1311 Reward: -129.25411114144134\n",
      "timesteps not 0\n",
      "Total Timesteps: 262400 Episode Num: 1312 Reward: -131.8544744485281\n",
      "timesteps not 0\n",
      "Total Timesteps: 262600 Episode Num: 1313 Reward: -224.24696168736682\n",
      "timesteps not 0\n",
      "Total Timesteps: 262800 Episode Num: 1314 Reward: -128.68452933313242\n",
      "timesteps not 0\n",
      "Total Timesteps: 263000 Episode Num: 1315 Reward: -2.882148858788366\n",
      "timesteps not 0\n",
      "Total Timesteps: 263200 Episode Num: 1316 Reward: -122.08946375593395\n",
      "timesteps not 0\n",
      "Total Timesteps: 263400 Episode Num: 1317 Reward: -237.93254452368018\n",
      "timesteps not 0\n",
      "Total Timesteps: 263600 Episode Num: 1318 Reward: -128.1593587529883\n",
      "timesteps not 0\n",
      "Total Timesteps: 263800 Episode Num: 1319 Reward: -126.7972990701192\n",
      "timesteps not 0\n",
      "Total Timesteps: 264000 Episode Num: 1320 Reward: -124.19699489306628\n",
      "timesteps not 0\n",
      "Total Timesteps: 264200 Episode Num: 1321 Reward: -118.22899798357967\n",
      "timesteps not 0\n",
      "Total Timesteps: 264400 Episode Num: 1322 Reward: -123.61997161034857\n",
      "timesteps not 0\n",
      "Total Timesteps: 264600 Episode Num: 1323 Reward: -134.15253186392573\n",
      "timesteps not 0\n",
      "Total Timesteps: 264800 Episode Num: 1324 Reward: -234.92378995628948\n",
      "timesteps not 0\n",
      "Total Timesteps: 265000 Episode Num: 1325 Reward: -121.78943577412238\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -136.746046\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 265200 Episode Num: 1326 Reward: -257.7291969399003\n",
      "timesteps not 0\n",
      "Total Timesteps: 265400 Episode Num: 1327 Reward: -137.30537376709063\n",
      "timesteps not 0\n",
      "Total Timesteps: 265600 Episode Num: 1328 Reward: -240.61280103338686\n",
      "timesteps not 0\n",
      "Total Timesteps: 265800 Episode Num: 1329 Reward: -11.605925093556696\n",
      "timesteps not 0\n",
      "Total Timesteps: 266000 Episode Num: 1330 Reward: -131.80313851753678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps not 0\n",
      "Total Timesteps: 266200 Episode Num: 1331 Reward: -122.98364225449852\n",
      "timesteps not 0\n",
      "Total Timesteps: 266400 Episode Num: 1332 Reward: -124.46672803635788\n",
      "timesteps not 0\n",
      "Total Timesteps: 266600 Episode Num: 1333 Reward: -121.0453436030708\n",
      "timesteps not 0\n",
      "Total Timesteps: 266800 Episode Num: 1334 Reward: -129.55753233411704\n",
      "timesteps not 0\n",
      "Total Timesteps: 267000 Episode Num: 1335 Reward: -120.24637265580381\n",
      "timesteps not 0\n",
      "Total Timesteps: 267200 Episode Num: 1336 Reward: -124.65235536205614\n",
      "timesteps not 0\n",
      "Total Timesteps: 267400 Episode Num: 1337 Reward: -7.709229331517513\n",
      "timesteps not 0\n",
      "Total Timesteps: 267600 Episode Num: 1338 Reward: -9.227795457506307\n",
      "timesteps not 0\n",
      "Total Timesteps: 267800 Episode Num: 1339 Reward: -130.66373227572856\n",
      "timesteps not 0\n",
      "Total Timesteps: 268000 Episode Num: 1340 Reward: -124.83049123196342\n",
      "timesteps not 0\n",
      "Total Timesteps: 268200 Episode Num: 1341 Reward: -7.486822030110225\n",
      "timesteps not 0\n",
      "Total Timesteps: 268400 Episode Num: 1342 Reward: -229.16039533843312\n",
      "timesteps not 0\n",
      "Total Timesteps: 268600 Episode Num: 1343 Reward: -236.62864484988017\n",
      "timesteps not 0\n",
      "Total Timesteps: 268800 Episode Num: 1344 Reward: -131.51078851658764\n",
      "timesteps not 0\n",
      "Total Timesteps: 269000 Episode Num: 1345 Reward: -233.7592163078608\n",
      "timesteps not 0\n",
      "Total Timesteps: 269200 Episode Num: 1346 Reward: -122.43614467444876\n",
      "timesteps not 0\n",
      "Total Timesteps: 269400 Episode Num: 1347 Reward: -120.57316678313882\n",
      "timesteps not 0\n",
      "Total Timesteps: 269600 Episode Num: 1348 Reward: -121.75665476918611\n",
      "timesteps not 0\n",
      "Total Timesteps: 269800 Episode Num: 1349 Reward: -119.51455392706302\n",
      "timesteps not 0\n",
      "Total Timesteps: 270000 Episode Num: 1350 Reward: -217.55670413816438\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -131.394291\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 270200 Episode Num: 1351 Reward: -121.85427082793835\n",
      "timesteps not 0\n",
      "Total Timesteps: 270400 Episode Num: 1352 Reward: -334.52756023216386\n",
      "timesteps not 0\n",
      "Total Timesteps: 270600 Episode Num: 1353 Reward: -124.07792209573118\n",
      "timesteps not 0\n",
      "Total Timesteps: 270800 Episode Num: 1354 Reward: -229.34277807599216\n",
      "timesteps not 0\n",
      "Total Timesteps: 271000 Episode Num: 1355 Reward: -120.54305089130699\n",
      "timesteps not 0\n",
      "Total Timesteps: 271200 Episode Num: 1356 Reward: -119.26249258164859\n",
      "timesteps not 0\n",
      "Total Timesteps: 271400 Episode Num: 1357 Reward: -127.28002306578038\n",
      "timesteps not 0\n",
      "Total Timesteps: 271600 Episode Num: 1358 Reward: -121.58399470858502\n",
      "timesteps not 0\n",
      "Total Timesteps: 271800 Episode Num: 1359 Reward: -254.5198650553034\n",
      "timesteps not 0\n",
      "Total Timesteps: 272000 Episode Num: 1360 Reward: -128.00526153429445\n",
      "timesteps not 0\n",
      "Total Timesteps: 272200 Episode Num: 1361 Reward: -236.41600435726812\n",
      "timesteps not 0\n",
      "Total Timesteps: 272400 Episode Num: 1362 Reward: -136.52260819277788\n",
      "timesteps not 0\n",
      "Total Timesteps: 272600 Episode Num: 1363 Reward: -129.24076415873103\n",
      "timesteps not 0\n",
      "Total Timesteps: 272800 Episode Num: 1364 Reward: -228.24994251595362\n",
      "timesteps not 0\n",
      "Total Timesteps: 273000 Episode Num: 1365 Reward: -12.885541201249913\n",
      "timesteps not 0\n",
      "Total Timesteps: 273200 Episode Num: 1366 Reward: -135.8499555472682\n",
      "timesteps not 0\n",
      "Total Timesteps: 273400 Episode Num: 1367 Reward: -9.081737312465334\n",
      "timesteps not 0\n",
      "Total Timesteps: 273600 Episode Num: 1368 Reward: -133.09920223727553\n",
      "timesteps not 0\n",
      "Total Timesteps: 273800 Episode Num: 1369 Reward: -128.31010595739866\n",
      "timesteps not 0\n",
      "Total Timesteps: 274000 Episode Num: 1370 Reward: -129.62582853066263\n",
      "timesteps not 0\n",
      "Total Timesteps: 274200 Episode Num: 1371 Reward: -257.8483928540226\n",
      "timesteps not 0\n",
      "Total Timesteps: 274400 Episode Num: 1372 Reward: -120.72234220616222\n",
      "timesteps not 0\n",
      "Total Timesteps: 274600 Episode Num: 1373 Reward: -126.1991069239483\n",
      "timesteps not 0\n",
      "Total Timesteps: 274800 Episode Num: 1374 Reward: -251.58707854684866\n",
      "timesteps not 0\n",
      "Total Timesteps: 275000 Episode Num: 1375 Reward: -127.085972431813\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -99.305768\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 275200 Episode Num: 1376 Reward: -119.4389480676906\n",
      "timesteps not 0\n",
      "Total Timesteps: 275400 Episode Num: 1377 Reward: -121.50674257244908\n",
      "timesteps not 0\n",
      "Total Timesteps: 275600 Episode Num: 1378 Reward: -126.73724320843567\n",
      "timesteps not 0\n",
      "Total Timesteps: 275800 Episode Num: 1379 Reward: -314.0509307467357\n",
      "timesteps not 0\n",
      "Total Timesteps: 276000 Episode Num: 1380 Reward: -2.143315339667789\n",
      "timesteps not 0\n",
      "Total Timesteps: 276200 Episode Num: 1381 Reward: -118.39551883976826\n",
      "timesteps not 0\n",
      "Total Timesteps: 276400 Episode Num: 1382 Reward: -218.0812662078686\n",
      "timesteps not 0\n",
      "Total Timesteps: 276600 Episode Num: 1383 Reward: -117.80799216029003\n",
      "timesteps not 0\n",
      "Total Timesteps: 276800 Episode Num: 1384 Reward: -224.7681254024612\n",
      "timesteps not 0\n",
      "Total Timesteps: 277000 Episode Num: 1385 Reward: -128.38292700246953\n",
      "timesteps not 0\n",
      "Total Timesteps: 277200 Episode Num: 1386 Reward: -233.5429670763885\n",
      "timesteps not 0\n",
      "Total Timesteps: 277400 Episode Num: 1387 Reward: -227.2165659763147\n",
      "timesteps not 0\n",
      "Total Timesteps: 277600 Episode Num: 1388 Reward: -3.864185292565749\n",
      "timesteps not 0\n",
      "Total Timesteps: 277800 Episode Num: 1389 Reward: -4.877905802410933\n",
      "timesteps not 0\n",
      "Total Timesteps: 278000 Episode Num: 1390 Reward: -5.333781516206375\n",
      "timesteps not 0\n",
      "Total Timesteps: 278200 Episode Num: 1391 Reward: -232.8100519613948\n",
      "timesteps not 0\n",
      "Total Timesteps: 278400 Episode Num: 1392 Reward: -126.80913457600042\n",
      "timesteps not 0\n",
      "Total Timesteps: 278600 Episode Num: 1393 Reward: -11.920209597052345\n",
      "timesteps not 0\n",
      "Total Timesteps: 278800 Episode Num: 1394 Reward: -126.34339582823522\n",
      "timesteps not 0\n",
      "Total Timesteps: 279000 Episode Num: 1395 Reward: -124.89539690625077\n",
      "timesteps not 0\n",
      "Total Timesteps: 279200 Episode Num: 1396 Reward: -131.31479022585958\n",
      "timesteps not 0\n",
      "Total Timesteps: 279400 Episode Num: 1397 Reward: -325.05162602005805\n",
      "timesteps not 0\n",
      "Total Timesteps: 279600 Episode Num: 1398 Reward: -230.608435020561\n",
      "timesteps not 0\n",
      "Total Timesteps: 279800 Episode Num: 1399 Reward: -7.952758160080664\n",
      "timesteps not 0\n",
      "Total Timesteps: 280000 Episode Num: 1400 Reward: -118.71444026205198\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -105.497970\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 280200 Episode Num: 1401 Reward: -122.529391809805\n",
      "timesteps not 0\n",
      "Total Timesteps: 280400 Episode Num: 1402 Reward: -126.9254519947145\n",
      "timesteps not 0\n",
      "Total Timesteps: 280600 Episode Num: 1403 Reward: -227.32523954465984\n",
      "timesteps not 0\n",
      "Total Timesteps: 280800 Episode Num: 1404 Reward: -128.70310845816593\n",
      "timesteps not 0\n",
      "Total Timesteps: 281000 Episode Num: 1405 Reward: -126.0374485953519\n",
      "timesteps not 0\n",
      "Total Timesteps: 281200 Episode Num: 1406 Reward: -126.80410865951514\n",
      "timesteps not 0\n",
      "Total Timesteps: 281400 Episode Num: 1407 Reward: -234.96755877333902\n",
      "timesteps not 0\n",
      "Total Timesteps: 281600 Episode Num: 1408 Reward: -125.17322189877548\n",
      "timesteps not 0\n",
      "Total Timesteps: 281800 Episode Num: 1409 Reward: -127.99929444896183\n",
      "timesteps not 0\n",
      "Total Timesteps: 282000 Episode Num: 1410 Reward: -121.975561546326\n",
      "timesteps not 0\n",
      "Total Timesteps: 282200 Episode Num: 1411 Reward: -221.140096297862\n",
      "timesteps not 0\n",
      "Total Timesteps: 282400 Episode Num: 1412 Reward: -118.70902608496282\n",
      "timesteps not 0\n",
      "Total Timesteps: 282600 Episode Num: 1413 Reward: -122.642804528809\n",
      "timesteps not 0\n",
      "Total Timesteps: 282800 Episode Num: 1414 Reward: -127.54121210951271\n",
      "timesteps not 0\n",
      "Total Timesteps: 283000 Episode Num: 1415 Reward: -231.66116746343042\n",
      "timesteps not 0\n",
      "Total Timesteps: 283200 Episode Num: 1416 Reward: -226.41647170494346\n",
      "timesteps not 0\n",
      "Total Timesteps: 283400 Episode Num: 1417 Reward: -3.4825281568582014\n",
      "timesteps not 0\n",
      "Total Timesteps: 283600 Episode Num: 1418 Reward: -117.09551684335823\n",
      "timesteps not 0\n",
      "Total Timesteps: 283800 Episode Num: 1419 Reward: -115.3313840538498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps not 0\n",
      "Total Timesteps: 284000 Episode Num: 1420 Reward: -777.6387097558735\n",
      "timesteps not 0\n",
      "Total Timesteps: 284200 Episode Num: 1421 Reward: -900.1219146661598\n",
      "timesteps not 0\n",
      "Total Timesteps: 284400 Episode Num: 1422 Reward: -225.36940619329724\n",
      "timesteps not 0\n",
      "Total Timesteps: 284600 Episode Num: 1423 Reward: -125.66566916275167\n",
      "timesteps not 0\n",
      "Total Timesteps: 284800 Episode Num: 1424 Reward: -2.0461739298979946\n",
      "timesteps not 0\n",
      "Total Timesteps: 285000 Episode Num: 1425 Reward: -122.00460392189231\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -135.343997\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 285200 Episode Num: 1426 Reward: -125.14548530634455\n",
      "timesteps not 0\n",
      "Total Timesteps: 285400 Episode Num: 1427 Reward: -222.00327744626108\n",
      "timesteps not 0\n",
      "Total Timesteps: 285600 Episode Num: 1428 Reward: -4.98693440635118\n",
      "timesteps not 0\n",
      "Total Timesteps: 285800 Episode Num: 1429 Reward: -291.0661922732455\n",
      "timesteps not 0\n",
      "Total Timesteps: 286000 Episode Num: 1430 Reward: -117.59147631543173\n",
      "timesteps not 0\n",
      "Total Timesteps: 286200 Episode Num: 1431 Reward: -251.3499726953558\n",
      "timesteps not 0\n",
      "Total Timesteps: 286400 Episode Num: 1432 Reward: -232.46503958489865\n",
      "timesteps not 0\n",
      "Total Timesteps: 286600 Episode Num: 1433 Reward: -244.60723703917148\n",
      "timesteps not 0\n",
      "Total Timesteps: 286800 Episode Num: 1434 Reward: -131.64273541399194\n",
      "timesteps not 0\n",
      "Total Timesteps: 287000 Episode Num: 1435 Reward: -131.79326175484528\n",
      "timesteps not 0\n",
      "Total Timesteps: 287200 Episode Num: 1436 Reward: -126.18021054765823\n",
      "timesteps not 0\n",
      "Total Timesteps: 287400 Episode Num: 1437 Reward: -226.45013721607702\n",
      "timesteps not 0\n",
      "Total Timesteps: 287600 Episode Num: 1438 Reward: -13.610422104816868\n",
      "timesteps not 0\n",
      "Total Timesteps: 287800 Episode Num: 1439 Reward: -126.30105298746015\n",
      "timesteps not 0\n",
      "Total Timesteps: 288000 Episode Num: 1440 Reward: -135.60813803000354\n",
      "timesteps not 0\n",
      "Total Timesteps: 288200 Episode Num: 1441 Reward: -131.43274212641222\n",
      "timesteps not 0\n",
      "Total Timesteps: 288400 Episode Num: 1442 Reward: -249.6760196653234\n",
      "timesteps not 0\n",
      "Total Timesteps: 288600 Episode Num: 1443 Reward: -133.4214747887831\n",
      "timesteps not 0\n",
      "Total Timesteps: 288800 Episode Num: 1444 Reward: -122.82614116202778\n",
      "timesteps not 0\n",
      "Total Timesteps: 289000 Episode Num: 1445 Reward: -1.8898441360853353\n",
      "timesteps not 0\n",
      "Total Timesteps: 289200 Episode Num: 1446 Reward: -121.53017731856046\n",
      "timesteps not 0\n",
      "Total Timesteps: 289400 Episode Num: 1447 Reward: -7.348170952851505\n",
      "timesteps not 0\n",
      "Total Timesteps: 289600 Episode Num: 1448 Reward: -7.754517960217416\n",
      "timesteps not 0\n",
      "Total Timesteps: 289800 Episode Num: 1449 Reward: -227.8408020004614\n",
      "timesteps not 0\n",
      "Total Timesteps: 290000 Episode Num: 1450 Reward: -5.555165383332076\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -152.108231\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 290200 Episode Num: 1451 Reward: -329.15098199074396\n",
      "timesteps not 0\n",
      "Total Timesteps: 290400 Episode Num: 1452 Reward: -227.71138592214976\n",
      "timesteps not 0\n",
      "Total Timesteps: 290600 Episode Num: 1453 Reward: -116.44037485397254\n",
      "timesteps not 0\n",
      "Total Timesteps: 290800 Episode Num: 1454 Reward: -120.24887959790509\n",
      "timesteps not 0\n",
      "Total Timesteps: 291000 Episode Num: 1455 Reward: -135.3289159253811\n",
      "timesteps not 0\n",
      "Total Timesteps: 291200 Episode Num: 1456 Reward: -132.55961078811268\n",
      "timesteps not 0\n",
      "Total Timesteps: 291400 Episode Num: 1457 Reward: -126.599180548736\n",
      "timesteps not 0\n",
      "Total Timesteps: 291600 Episode Num: 1458 Reward: -126.69686365752891\n",
      "timesteps not 0\n",
      "Total Timesteps: 291800 Episode Num: 1459 Reward: -238.8884498937736\n",
      "timesteps not 0\n",
      "Total Timesteps: 292000 Episode Num: 1460 Reward: -134.36970306997026\n",
      "timesteps not 0\n",
      "Total Timesteps: 292200 Episode Num: 1461 Reward: -127.41390672132695\n",
      "timesteps not 0\n",
      "Total Timesteps: 292400 Episode Num: 1462 Reward: -9.09108654733113\n",
      "timesteps not 0\n",
      "Total Timesteps: 292600 Episode Num: 1463 Reward: -131.13674623880783\n",
      "timesteps not 0\n",
      "Total Timesteps: 292800 Episode Num: 1464 Reward: -249.75363540429606\n",
      "timesteps not 0\n",
      "Total Timesteps: 293000 Episode Num: 1465 Reward: -227.0322823156162\n",
      "timesteps not 0\n",
      "Total Timesteps: 293200 Episode Num: 1466 Reward: -134.4818199751788\n",
      "timesteps not 0\n",
      "Total Timesteps: 293400 Episode Num: 1467 Reward: -235.1278678208198\n",
      "timesteps not 0\n",
      "Total Timesteps: 293600 Episode Num: 1468 Reward: -7.622913927429757\n",
      "timesteps not 0\n",
      "Total Timesteps: 293800 Episode Num: 1469 Reward: -227.30572870733556\n",
      "timesteps not 0\n",
      "Total Timesteps: 294000 Episode Num: 1470 Reward: -118.95334024014697\n",
      "timesteps not 0\n",
      "Total Timesteps: 294200 Episode Num: 1471 Reward: -126.55567680273052\n",
      "timesteps not 0\n",
      "Total Timesteps: 294400 Episode Num: 1472 Reward: -241.48312970166012\n",
      "timesteps not 0\n",
      "Total Timesteps: 294600 Episode Num: 1473 Reward: -127.03756645766052\n",
      "timesteps not 0\n",
      "Total Timesteps: 294800 Episode Num: 1474 Reward: -124.43064145338127\n",
      "timesteps not 0\n",
      "Total Timesteps: 295000 Episode Num: 1475 Reward: -229.44430165264433\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -156.611496\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 295200 Episode Num: 1476 Reward: -245.66733536771656\n",
      "timesteps not 0\n",
      "Total Timesteps: 295400 Episode Num: 1477 Reward: -222.8360767687093\n",
      "timesteps not 0\n",
      "Total Timesteps: 295600 Episode Num: 1478 Reward: -125.42382295334095\n",
      "timesteps not 0\n",
      "Total Timesteps: 295800 Episode Num: 1479 Reward: -9.784937980102963\n",
      "timesteps not 0\n",
      "Total Timesteps: 296000 Episode Num: 1480 Reward: -9.162155028677379\n",
      "timesteps not 0\n",
      "Total Timesteps: 296200 Episode Num: 1481 Reward: -123.06460734869476\n",
      "timesteps not 0\n",
      "Total Timesteps: 296400 Episode Num: 1482 Reward: -128.63303032102942\n",
      "timesteps not 0\n",
      "Total Timesteps: 296600 Episode Num: 1483 Reward: -235.1530336294885\n",
      "timesteps not 0\n",
      "Total Timesteps: 296800 Episode Num: 1484 Reward: -132.4918549663049\n",
      "timesteps not 0\n",
      "Total Timesteps: 297000 Episode Num: 1485 Reward: -245.5734979586273\n",
      "timesteps not 0\n",
      "Total Timesteps: 297200 Episode Num: 1486 Reward: -235.70954454908866\n",
      "timesteps not 0\n",
      "Total Timesteps: 297400 Episode Num: 1487 Reward: -128.29571036632262\n",
      "timesteps not 0\n",
      "Total Timesteps: 297600 Episode Num: 1488 Reward: -324.2086398090438\n",
      "timesteps not 0\n",
      "Total Timesteps: 297800 Episode Num: 1489 Reward: -132.33251246993726\n",
      "timesteps not 0\n",
      "Total Timesteps: 298000 Episode Num: 1490 Reward: -11.096834411204739\n",
      "timesteps not 0\n",
      "Total Timesteps: 298200 Episode Num: 1491 Reward: -11.171578114645797\n",
      "timesteps not 0\n",
      "Total Timesteps: 298400 Episode Num: 1492 Reward: -129.74051361153522\n",
      "timesteps not 0\n",
      "Total Timesteps: 298600 Episode Num: 1493 Reward: -124.25350284570125\n",
      "timesteps not 0\n",
      "Total Timesteps: 298800 Episode Num: 1494 Reward: -125.31156884760289\n",
      "timesteps not 0\n",
      "Total Timesteps: 299000 Episode Num: 1495 Reward: -246.62380789130376\n",
      "timesteps not 0\n",
      "Total Timesteps: 299200 Episode Num: 1496 Reward: -125.88384924688461\n",
      "timesteps not 0\n",
      "Total Timesteps: 299400 Episode Num: 1497 Reward: -134.04012623767716\n",
      "timesteps not 0\n",
      "Total Timesteps: 299600 Episode Num: 1498 Reward: -133.23990957120006\n",
      "timesteps not 0\n",
      "Total Timesteps: 299800 Episode Num: 1499 Reward: -122.61805438761324\n",
      "timesteps not 0\n",
      "Total Timesteps: 300000 Episode Num: 1500 Reward: -10.816759547315725\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -191.338930\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 300200 Episode Num: 1501 Reward: -125.9587638411977\n",
      "timesteps not 0\n",
      "Total Timesteps: 300400 Episode Num: 1502 Reward: -130.31977968062247\n",
      "timesteps not 0\n",
      "Total Timesteps: 300600 Episode Num: 1503 Reward: -129.9066083898491\n",
      "timesteps not 0\n",
      "Total Timesteps: 300800 Episode Num: 1504 Reward: -10.81012249087535\n",
      "timesteps not 0\n",
      "Total Timesteps: 301000 Episode Num: 1505 Reward: -227.95877480771463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps not 0\n",
      "Total Timesteps: 301200 Episode Num: 1506 Reward: -131.39929942793012\n",
      "timesteps not 0\n",
      "Total Timesteps: 301400 Episode Num: 1507 Reward: -131.086907890606\n",
      "timesteps not 0\n",
      "Total Timesteps: 301600 Episode Num: 1508 Reward: -253.41322329035262\n",
      "timesteps not 0\n",
      "Total Timesteps: 301800 Episode Num: 1509 Reward: -135.28837473156395\n",
      "timesteps not 0\n",
      "Total Timesteps: 302000 Episode Num: 1510 Reward: -125.59514604187497\n",
      "timesteps not 0\n",
      "Total Timesteps: 302200 Episode Num: 1511 Reward: -310.44983526279543\n",
      "timesteps not 0\n",
      "Total Timesteps: 302400 Episode Num: 1512 Reward: -248.08702174464918\n",
      "timesteps not 0\n",
      "Total Timesteps: 302600 Episode Num: 1513 Reward: -124.90984708593267\n",
      "timesteps not 0\n",
      "Total Timesteps: 302800 Episode Num: 1514 Reward: -239.46865772221403\n",
      "timesteps not 0\n",
      "Total Timesteps: 303000 Episode Num: 1515 Reward: -128.00512264781614\n",
      "timesteps not 0\n",
      "Total Timesteps: 303200 Episode Num: 1516 Reward: -234.80143586444268\n",
      "timesteps not 0\n",
      "Total Timesteps: 303400 Episode Num: 1517 Reward: -136.72717678121415\n",
      "timesteps not 0\n",
      "Total Timesteps: 303600 Episode Num: 1518 Reward: -127.12829849284664\n",
      "timesteps not 0\n",
      "Total Timesteps: 303800 Episode Num: 1519 Reward: -253.26295823969414\n",
      "timesteps not 0\n",
      "Total Timesteps: 304000 Episode Num: 1520 Reward: -137.3155904667418\n",
      "timesteps not 0\n",
      "Total Timesteps: 304200 Episode Num: 1521 Reward: -126.04300285172017\n",
      "timesteps not 0\n",
      "Total Timesteps: 304400 Episode Num: 1522 Reward: -136.0707682823542\n",
      "timesteps not 0\n",
      "Total Timesteps: 304600 Episode Num: 1523 Reward: -12.567015509353592\n",
      "timesteps not 0\n",
      "Total Timesteps: 304800 Episode Num: 1524 Reward: -12.634444804919882\n",
      "timesteps not 0\n",
      "Total Timesteps: 305000 Episode Num: 1525 Reward: -131.84573043939398\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -139.614390\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 305200 Episode Num: 1526 Reward: -129.580357002304\n",
      "timesteps not 0\n",
      "Total Timesteps: 305400 Episode Num: 1527 Reward: -123.68818082919762\n",
      "timesteps not 0\n",
      "Total Timesteps: 305600 Episode Num: 1528 Reward: -135.9221572895212\n",
      "timesteps not 0\n",
      "Total Timesteps: 305800 Episode Num: 1529 Reward: -13.23793449812841\n",
      "timesteps not 0\n",
      "Total Timesteps: 306000 Episode Num: 1530 Reward: -231.29255963492986\n",
      "timesteps not 0\n",
      "Total Timesteps: 306200 Episode Num: 1531 Reward: -126.67859047179277\n",
      "timesteps not 0\n",
      "Total Timesteps: 306400 Episode Num: 1532 Reward: -234.79923031751593\n",
      "timesteps not 0\n",
      "Total Timesteps: 306600 Episode Num: 1533 Reward: -234.89329261647458\n",
      "timesteps not 0\n",
      "Total Timesteps: 306800 Episode Num: 1534 Reward: -243.68588896032142\n",
      "timesteps not 0\n",
      "Total Timesteps: 307000 Episode Num: 1535 Reward: -13.88535662828703\n",
      "timesteps not 0\n",
      "Total Timesteps: 307200 Episode Num: 1536 Reward: -132.23770314476724\n",
      "timesteps not 0\n",
      "Total Timesteps: 307400 Episode Num: 1537 Reward: -129.24016338960092\n",
      "timesteps not 0\n",
      "Total Timesteps: 307600 Episode Num: 1538 Reward: -241.2495596049214\n",
      "timesteps not 0\n",
      "Total Timesteps: 307800 Episode Num: 1539 Reward: -140.19384578659782\n",
      "timesteps not 0\n",
      "Total Timesteps: 308000 Episode Num: 1540 Reward: -130.4316491050296\n",
      "timesteps not 0\n",
      "Total Timesteps: 308200 Episode Num: 1541 Reward: -253.81089589203145\n",
      "timesteps not 0\n",
      "Total Timesteps: 308400 Episode Num: 1542 Reward: -128.4909625350084\n",
      "timesteps not 0\n",
      "Total Timesteps: 308600 Episode Num: 1543 Reward: -233.7625052836306\n",
      "timesteps not 0\n",
      "Total Timesteps: 308800 Episode Num: 1544 Reward: -315.8554081214673\n",
      "timesteps not 0\n",
      "Total Timesteps: 309000 Episode Num: 1545 Reward: -123.44694861439672\n",
      "timesteps not 0\n",
      "Total Timesteps: 309200 Episode Num: 1546 Reward: -122.22065366364322\n",
      "timesteps not 0\n",
      "Total Timesteps: 309400 Episode Num: 1547 Reward: -128.2680672435121\n",
      "timesteps not 0\n",
      "Total Timesteps: 309600 Episode Num: 1548 Reward: -122.97066670648815\n",
      "timesteps not 0\n",
      "Total Timesteps: 309800 Episode Num: 1549 Reward: -125.64863523245191\n",
      "timesteps not 0\n",
      "Total Timesteps: 310000 Episode Num: 1550 Reward: -234.9697740425739\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -186.738260\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 310200 Episode Num: 1551 Reward: -136.83464556879903\n",
      "timesteps not 0\n",
      "Total Timesteps: 310400 Episode Num: 1552 Reward: -135.46648666186704\n",
      "timesteps not 0\n",
      "Total Timesteps: 310600 Episode Num: 1553 Reward: -235.1017804742706\n",
      "timesteps not 0\n",
      "Total Timesteps: 310800 Episode Num: 1554 Reward: -137.18658693815513\n",
      "timesteps not 0\n",
      "Total Timesteps: 311000 Episode Num: 1555 Reward: -126.4134825625459\n",
      "timesteps not 0\n",
      "Total Timesteps: 311200 Episode Num: 1556 Reward: -130.36637766357725\n",
      "timesteps not 0\n",
      "Total Timesteps: 311400 Episode Num: 1557 Reward: -240.6118983664424\n",
      "timesteps not 0\n",
      "Total Timesteps: 311600 Episode Num: 1558 Reward: -129.47528501087498\n",
      "timesteps not 0\n",
      "Total Timesteps: 311800 Episode Num: 1559 Reward: -12.530400126434628\n",
      "timesteps not 0\n",
      "Total Timesteps: 312000 Episode Num: 1560 Reward: -251.05667511366374\n",
      "timesteps not 0\n",
      "Total Timesteps: 312200 Episode Num: 1561 Reward: -303.3538233220437\n",
      "timesteps not 0\n",
      "Total Timesteps: 312400 Episode Num: 1562 Reward: -130.26080988899284\n",
      "timesteps not 0\n",
      "Total Timesteps: 312600 Episode Num: 1563 Reward: -122.37843358866321\n",
      "timesteps not 0\n",
      "Total Timesteps: 312800 Episode Num: 1564 Reward: -130.8859295282534\n",
      "timesteps not 0\n",
      "Total Timesteps: 313000 Episode Num: 1565 Reward: -249.30549555278864\n",
      "timesteps not 0\n",
      "Total Timesteps: 313200 Episode Num: 1566 Reward: -9.448605489595966\n",
      "timesteps not 0\n",
      "Total Timesteps: 313400 Episode Num: 1567 Reward: -10.987207700511298\n",
      "timesteps not 0\n",
      "Total Timesteps: 313600 Episode Num: 1568 Reward: -132.56994576307932\n",
      "timesteps not 0\n",
      "Total Timesteps: 313800 Episode Num: 1569 Reward: -11.39484173938524\n",
      "timesteps not 0\n",
      "Total Timesteps: 314000 Episode Num: 1570 Reward: -226.88134951176573\n",
      "timesteps not 0\n",
      "Total Timesteps: 314200 Episode Num: 1571 Reward: -131.56609093515019\n",
      "timesteps not 0\n",
      "Total Timesteps: 314400 Episode Num: 1572 Reward: -321.20226865065297\n",
      "timesteps not 0\n",
      "Total Timesteps: 314600 Episode Num: 1573 Reward: -125.91685918719791\n",
      "timesteps not 0\n",
      "Total Timesteps: 314800 Episode Num: 1574 Reward: -126.7443937391809\n",
      "timesteps not 0\n",
      "Total Timesteps: 315000 Episode Num: 1575 Reward: -128.50129836397716\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -150.528909\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 315200 Episode Num: 1576 Reward: -229.5838512150965\n",
      "timesteps not 0\n",
      "Total Timesteps: 315400 Episode Num: 1577 Reward: -128.8730637735023\n",
      "timesteps not 0\n",
      "Total Timesteps: 315600 Episode Num: 1578 Reward: -137.51703648118544\n",
      "timesteps not 0\n",
      "Total Timesteps: 315800 Episode Num: 1579 Reward: -131.567806106608\n",
      "timesteps not 0\n",
      "Total Timesteps: 316000 Episode Num: 1580 Reward: -133.19023951263267\n",
      "timesteps not 0\n",
      "Total Timesteps: 316200 Episode Num: 1581 Reward: -340.5128164174508\n",
      "timesteps not 0\n",
      "Total Timesteps: 316400 Episode Num: 1582 Reward: -235.1099016892325\n",
      "timesteps not 0\n",
      "Total Timesteps: 316600 Episode Num: 1583 Reward: -232.7394479122391\n",
      "timesteps not 0\n",
      "Total Timesteps: 316800 Episode Num: 1584 Reward: -309.2174177417764\n",
      "timesteps not 0\n",
      "Total Timesteps: 317000 Episode Num: 1585 Reward: -137.2445532702842\n",
      "timesteps not 0\n",
      "Total Timesteps: 317200 Episode Num: 1586 Reward: -129.09357748821256\n",
      "timesteps not 0\n",
      "Total Timesteps: 317400 Episode Num: 1587 Reward: -138.1031861585865\n",
      "timesteps not 0\n",
      "Total Timesteps: 317600 Episode Num: 1588 Reward: -133.24465597116185\n",
      "timesteps not 0\n",
      "Total Timesteps: 317800 Episode Num: 1589 Reward: -132.1654230360443\n",
      "timesteps not 0\n",
      "Total Timesteps: 318000 Episode Num: 1590 Reward: -136.9972188286853\n",
      "timesteps not 0\n",
      "Total Timesteps: 318200 Episode Num: 1591 Reward: -127.77636339274537\n",
      "timesteps not 0\n",
      "Total Timesteps: 318400 Episode Num: 1592 Reward: -132.45070834124445\n",
      "timesteps not 0\n",
      "Total Timesteps: 318600 Episode Num: 1593 Reward: -135.78619642493837\n",
      "timesteps not 0\n",
      "Total Timesteps: 318800 Episode Num: 1594 Reward: -238.09951487046067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps not 0\n",
      "Total Timesteps: 319000 Episode Num: 1595 Reward: -129.07841768792807\n",
      "timesteps not 0\n",
      "Total Timesteps: 319200 Episode Num: 1596 Reward: -16.320169053105467\n",
      "timesteps not 0\n",
      "Total Timesteps: 319400 Episode Num: 1597 Reward: -14.264199675487845\n",
      "timesteps not 0\n",
      "Total Timesteps: 319600 Episode Num: 1598 Reward: -138.12505176060142\n",
      "timesteps not 0\n",
      "Total Timesteps: 319800 Episode Num: 1599 Reward: -136.8189728354454\n",
      "timesteps not 0\n",
      "Total Timesteps: 320000 Episode Num: 1600 Reward: -368.80149222421437\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -135.329445\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 320200 Episode Num: 1601 Reward: -12.627675194297321\n",
      "timesteps not 0\n",
      "Total Timesteps: 320400 Episode Num: 1602 Reward: -130.85852377775464\n",
      "timesteps not 0\n",
      "Total Timesteps: 320600 Episode Num: 1603 Reward: -322.4448409215611\n",
      "timesteps not 0\n",
      "Total Timesteps: 320800 Episode Num: 1604 Reward: -138.05444917623592\n",
      "timesteps not 0\n",
      "Total Timesteps: 321000 Episode Num: 1605 Reward: -126.2271653714646\n",
      "timesteps not 0\n",
      "Total Timesteps: 321200 Episode Num: 1606 Reward: -139.82708532019393\n",
      "timesteps not 0\n",
      "Total Timesteps: 321400 Episode Num: 1607 Reward: -247.88403515801838\n",
      "timesteps not 0\n",
      "Total Timesteps: 321600 Episode Num: 1608 Reward: -130.72021591043872\n",
      "timesteps not 0\n",
      "Total Timesteps: 321800 Episode Num: 1609 Reward: -281.02484297084084\n",
      "timesteps not 0\n",
      "Total Timesteps: 322000 Episode Num: 1610 Reward: -233.59672346388825\n",
      "timesteps not 0\n",
      "Total Timesteps: 322200 Episode Num: 1611 Reward: -229.95142468583003\n",
      "timesteps not 0\n",
      "Total Timesteps: 322400 Episode Num: 1612 Reward: -129.40805169154467\n",
      "timesteps not 0\n",
      "Total Timesteps: 322600 Episode Num: 1613 Reward: -134.08739613136476\n",
      "timesteps not 0\n",
      "Total Timesteps: 322800 Episode Num: 1614 Reward: -128.9991672625851\n",
      "timesteps not 0\n",
      "Total Timesteps: 323000 Episode Num: 1615 Reward: -133.04359643373607\n",
      "timesteps not 0\n",
      "Total Timesteps: 323200 Episode Num: 1616 Reward: -130.0164492211036\n",
      "timesteps not 0\n",
      "Total Timesteps: 323400 Episode Num: 1617 Reward: -136.17409892133227\n",
      "timesteps not 0\n",
      "Total Timesteps: 323600 Episode Num: 1618 Reward: -125.09922094516077\n",
      "timesteps not 0\n",
      "Total Timesteps: 323800 Episode Num: 1619 Reward: -124.4838168571663\n",
      "timesteps not 0\n",
      "Total Timesteps: 324000 Episode Num: 1620 Reward: -135.8985113882279\n",
      "timesteps not 0\n",
      "Total Timesteps: 324200 Episode Num: 1621 Reward: -236.41907762911723\n",
      "timesteps not 0\n",
      "Total Timesteps: 324400 Episode Num: 1622 Reward: -134.08171834941987\n",
      "timesteps not 0\n",
      "Total Timesteps: 324600 Episode Num: 1623 Reward: -239.61942745974525\n",
      "timesteps not 0\n",
      "Total Timesteps: 324800 Episode Num: 1624 Reward: -126.85385876184912\n",
      "timesteps not 0\n",
      "Total Timesteps: 325000 Episode Num: 1625 Reward: -280.6682512150006\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -127.320461\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 325200 Episode Num: 1626 Reward: -234.43447727784493\n",
      "timesteps not 0\n",
      "Total Timesteps: 325400 Episode Num: 1627 Reward: -323.8678618659516\n",
      "timesteps not 0\n",
      "Total Timesteps: 325600 Episode Num: 1628 Reward: -226.99446707703333\n",
      "timesteps not 0\n",
      "Total Timesteps: 325800 Episode Num: 1629 Reward: -253.3385779897708\n",
      "timesteps not 0\n",
      "Total Timesteps: 326000 Episode Num: 1630 Reward: -136.8121825219866\n",
      "timesteps not 0\n",
      "Total Timesteps: 326200 Episode Num: 1631 Reward: -14.867531698025427\n",
      "timesteps not 0\n",
      "Total Timesteps: 326400 Episode Num: 1632 Reward: -127.45886223421041\n",
      "timesteps not 0\n",
      "Total Timesteps: 326600 Episode Num: 1633 Reward: -134.53458126283405\n",
      "timesteps not 0\n",
      "Total Timesteps: 326800 Episode Num: 1634 Reward: -243.14899911652947\n",
      "timesteps not 0\n",
      "Total Timesteps: 327000 Episode Num: 1635 Reward: -16.03648471585219\n",
      "timesteps not 0\n",
      "Total Timesteps: 327200 Episode Num: 1636 Reward: -128.5786985536184\n",
      "timesteps not 0\n",
      "Total Timesteps: 327400 Episode Num: 1637 Reward: -134.7397582986146\n",
      "timesteps not 0\n",
      "Total Timesteps: 327600 Episode Num: 1638 Reward: -127.92763053825227\n",
      "timesteps not 0\n",
      "Total Timesteps: 327800 Episode Num: 1639 Reward: -16.42515014263138\n",
      "timesteps not 0\n",
      "Total Timesteps: 328000 Episode Num: 1640 Reward: -15.815701946513357\n",
      "timesteps not 0\n",
      "Total Timesteps: 328200 Episode Num: 1641 Reward: -131.2728991455981\n",
      "timesteps not 0\n",
      "Total Timesteps: 328400 Episode Num: 1642 Reward: -132.86289900327904\n",
      "timesteps not 0\n",
      "Total Timesteps: 328600 Episode Num: 1643 Reward: -133.65607527494717\n",
      "timesteps not 0\n",
      "Total Timesteps: 328800 Episode Num: 1644 Reward: -241.56095141043778\n",
      "timesteps not 0\n",
      "Total Timesteps: 329000 Episode Num: 1645 Reward: -132.67499181074868\n",
      "timesteps not 0\n",
      "Total Timesteps: 329200 Episode Num: 1646 Reward: -132.47984301408135\n",
      "timesteps not 0\n",
      "Total Timesteps: 329400 Episode Num: 1647 Reward: -130.6164185276188\n",
      "timesteps not 0\n",
      "Total Timesteps: 329600 Episode Num: 1648 Reward: -236.48286604157116\n",
      "timesteps not 0\n",
      "Total Timesteps: 329800 Episode Num: 1649 Reward: -233.13712957496077\n",
      "timesteps not 0\n",
      "Total Timesteps: 330000 Episode Num: 1650 Reward: -134.86050622020184\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -141.694837\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 330200 Episode Num: 1651 Reward: -278.6425871032988\n",
      "timesteps not 0\n",
      "Total Timesteps: 330400 Episode Num: 1652 Reward: -141.52834423310605\n",
      "timesteps not 0\n",
      "Total Timesteps: 330600 Episode Num: 1653 Reward: -135.31417245131954\n",
      "timesteps not 0\n",
      "Total Timesteps: 330800 Episode Num: 1654 Reward: -13.951830095078698\n",
      "timesteps not 0\n",
      "Total Timesteps: 331000 Episode Num: 1655 Reward: -238.18281044238964\n",
      "timesteps not 0\n",
      "Total Timesteps: 331200 Episode Num: 1656 Reward: -131.68816138299206\n",
      "timesteps not 0\n",
      "Total Timesteps: 331400 Episode Num: 1657 Reward: -124.80351689784332\n",
      "timesteps not 0\n",
      "Total Timesteps: 331600 Episode Num: 1658 Reward: -238.7881856889052\n",
      "timesteps not 0\n",
      "Total Timesteps: 331800 Episode Num: 1659 Reward: -129.62786841119706\n",
      "timesteps not 0\n",
      "Total Timesteps: 332000 Episode Num: 1660 Reward: -140.68429412809905\n",
      "timesteps not 0\n",
      "Total Timesteps: 332200 Episode Num: 1661 Reward: -128.88904594207455\n",
      "timesteps not 0\n",
      "Total Timesteps: 332400 Episode Num: 1662 Reward: -255.8177249471588\n",
      "timesteps not 0\n",
      "Total Timesteps: 332600 Episode Num: 1663 Reward: -237.92038384379183\n",
      "timesteps not 0\n",
      "Total Timesteps: 332800 Episode Num: 1664 Reward: -313.55322190581495\n",
      "timesteps not 0\n",
      "Total Timesteps: 333000 Episode Num: 1665 Reward: -128.4564980856677\n",
      "timesteps not 0\n",
      "Total Timesteps: 333200 Episode Num: 1666 Reward: -122.44847000927626\n",
      "timesteps not 0\n",
      "Total Timesteps: 333400 Episode Num: 1667 Reward: -224.88906053386444\n",
      "timesteps not 0\n",
      "Total Timesteps: 333600 Episode Num: 1668 Reward: -232.81890801925803\n",
      "timesteps not 0\n",
      "Total Timesteps: 333800 Episode Num: 1669 Reward: -129.66998273313408\n",
      "timesteps not 0\n",
      "Total Timesteps: 334000 Episode Num: 1670 Reward: -124.60743230972055\n",
      "timesteps not 0\n",
      "Total Timesteps: 334200 Episode Num: 1671 Reward: -128.62381515119702\n",
      "timesteps not 0\n",
      "Total Timesteps: 334400 Episode Num: 1672 Reward: -128.22842930015017\n",
      "timesteps not 0\n",
      "Total Timesteps: 334600 Episode Num: 1673 Reward: -130.75941800656852\n",
      "timesteps not 0\n",
      "Total Timesteps: 334800 Episode Num: 1674 Reward: -234.1782389457913\n",
      "timesteps not 0\n",
      "Total Timesteps: 335000 Episode Num: 1675 Reward: -131.80648362921767\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -158.724224\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 335200 Episode Num: 1676 Reward: -124.2876357897614\n",
      "timesteps not 0\n",
      "Total Timesteps: 335400 Episode Num: 1677 Reward: -124.57871336852143\n",
      "timesteps not 0\n",
      "Total Timesteps: 335600 Episode Num: 1678 Reward: -344.2624355767626\n",
      "timesteps not 0\n",
      "Total Timesteps: 335800 Episode Num: 1679 Reward: -247.54606859120452\n",
      "timesteps not 0\n",
      "Total Timesteps: 336000 Episode Num: 1680 Reward: -258.79393016045174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps not 0\n",
      "Total Timesteps: 336200 Episode Num: 1681 Reward: -233.30960912585306\n",
      "timesteps not 0\n",
      "Total Timesteps: 336400 Episode Num: 1682 Reward: -240.53719381533244\n",
      "timesteps not 0\n",
      "Total Timesteps: 336600 Episode Num: 1683 Reward: -125.4993065550555\n",
      "timesteps not 0\n",
      "Total Timesteps: 336800 Episode Num: 1684 Reward: -124.95032661058536\n",
      "timesteps not 0\n",
      "Total Timesteps: 337000 Episode Num: 1685 Reward: -123.32569067410466\n",
      "timesteps not 0\n",
      "Total Timesteps: 337200 Episode Num: 1686 Reward: -125.09038269353549\n",
      "timesteps not 0\n",
      "Total Timesteps: 337400 Episode Num: 1687 Reward: -344.0881198228079\n",
      "timesteps not 0\n",
      "Total Timesteps: 337600 Episode Num: 1688 Reward: -234.36371642716534\n",
      "timesteps not 0\n",
      "Total Timesteps: 337800 Episode Num: 1689 Reward: -230.91619384758886\n",
      "timesteps not 0\n",
      "Total Timesteps: 338000 Episode Num: 1690 Reward: -237.4661593555551\n",
      "timesteps not 0\n",
      "Total Timesteps: 338200 Episode Num: 1691 Reward: -232.52597195554904\n",
      "timesteps not 0\n",
      "Total Timesteps: 338400 Episode Num: 1692 Reward: -238.7420710531492\n",
      "timesteps not 0\n",
      "Total Timesteps: 338600 Episode Num: 1693 Reward: -252.05307840266772\n",
      "timesteps not 0\n",
      "Total Timesteps: 338800 Episode Num: 1694 Reward: -224.93249885237958\n",
      "timesteps not 0\n",
      "Total Timesteps: 339000 Episode Num: 1695 Reward: -121.8233296543038\n",
      "timesteps not 0\n",
      "Total Timesteps: 339200 Episode Num: 1696 Reward: -319.53963448513235\n",
      "timesteps not 0\n",
      "Total Timesteps: 339400 Episode Num: 1697 Reward: -245.18029961648\n",
      "timesteps not 0\n",
      "Total Timesteps: 339600 Episode Num: 1698 Reward: -241.6038460000518\n",
      "timesteps not 0\n",
      "Total Timesteps: 339800 Episode Num: 1699 Reward: -8.68673116718298\n",
      "timesteps not 0\n",
      "Total Timesteps: 340000 Episode Num: 1700 Reward: -249.63462974294868\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -190.954490\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 340200 Episode Num: 1701 Reward: -224.23468167786007\n",
      "timesteps not 0\n",
      "Total Timesteps: 340400 Episode Num: 1702 Reward: -223.40681170247788\n",
      "timesteps not 0\n",
      "Total Timesteps: 340600 Episode Num: 1703 Reward: -238.78703335088883\n",
      "timesteps not 0\n",
      "Total Timesteps: 340800 Episode Num: 1704 Reward: -132.49642437509445\n",
      "timesteps not 0\n",
      "Total Timesteps: 341000 Episode Num: 1705 Reward: -134.894124683729\n",
      "timesteps not 0\n",
      "Total Timesteps: 341200 Episode Num: 1706 Reward: -366.87804800703685\n",
      "timesteps not 0\n",
      "Total Timesteps: 341400 Episode Num: 1707 Reward: -231.8355280036822\n",
      "timesteps not 0\n",
      "Total Timesteps: 341600 Episode Num: 1708 Reward: -131.06822598613337\n",
      "timesteps not 0\n",
      "Total Timesteps: 341800 Episode Num: 1709 Reward: -232.313734138955\n",
      "timesteps not 0\n",
      "Total Timesteps: 342000 Episode Num: 1710 Reward: -124.87105360572268\n",
      "timesteps not 0\n",
      "Total Timesteps: 342200 Episode Num: 1711 Reward: -11.013877325066677\n",
      "timesteps not 0\n",
      "Total Timesteps: 342400 Episode Num: 1712 Reward: -9.73828528855611\n",
      "timesteps not 0\n",
      "Total Timesteps: 342600 Episode Num: 1713 Reward: -124.99242450964239\n",
      "timesteps not 0\n",
      "Total Timesteps: 342800 Episode Num: 1714 Reward: -11.497089927711805\n",
      "timesteps not 0\n",
      "Total Timesteps: 343000 Episode Num: 1715 Reward: -239.12924472783243\n",
      "timesteps not 0\n",
      "Total Timesteps: 343200 Episode Num: 1716 Reward: -132.56060614432317\n",
      "timesteps not 0\n",
      "Total Timesteps: 343400 Episode Num: 1717 Reward: -133.51821458645998\n",
      "timesteps not 0\n",
      "Total Timesteps: 343600 Episode Num: 1718 Reward: -303.474395630821\n",
      "timesteps not 0\n",
      "Total Timesteps: 343800 Episode Num: 1719 Reward: -5.027080988692172\n",
      "timesteps not 0\n",
      "Total Timesteps: 344000 Episode Num: 1720 Reward: -119.61752320779074\n",
      "timesteps not 0\n",
      "Total Timesteps: 344200 Episode Num: 1721 Reward: -225.5639835079498\n",
      "timesteps not 0\n",
      "Total Timesteps: 344400 Episode Num: 1722 Reward: -125.63617996590658\n",
      "timesteps not 0\n",
      "Total Timesteps: 344600 Episode Num: 1723 Reward: -331.22011003164573\n",
      "timesteps not 0\n",
      "Total Timesteps: 344800 Episode Num: 1724 Reward: -124.40635869389999\n",
      "timesteps not 0\n",
      "Total Timesteps: 345000 Episode Num: 1725 Reward: -124.55710028005664\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -147.731067\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 345200 Episode Num: 1726 Reward: -130.23439819565513\n",
      "timesteps not 0\n",
      "Total Timesteps: 345400 Episode Num: 1727 Reward: -134.34594325342607\n",
      "timesteps not 0\n",
      "Total Timesteps: 345600 Episode Num: 1728 Reward: -131.58555889141212\n",
      "timesteps not 0\n",
      "Total Timesteps: 345800 Episode Num: 1729 Reward: -134.92669970662848\n",
      "timesteps not 0\n",
      "Total Timesteps: 346000 Episode Num: 1730 Reward: -129.61851704687803\n",
      "timesteps not 0\n",
      "Total Timesteps: 346200 Episode Num: 1731 Reward: -6.713409334858052\n",
      "timesteps not 0\n",
      "Total Timesteps: 346400 Episode Num: 1732 Reward: -123.84031560485906\n",
      "timesteps not 0\n",
      "Total Timesteps: 346600 Episode Num: 1733 Reward: -130.7799847781026\n",
      "timesteps not 0\n",
      "Total Timesteps: 346800 Episode Num: 1734 Reward: -122.54885187821962\n",
      "timesteps not 0\n",
      "Total Timesteps: 347000 Episode Num: 1735 Reward: -123.18368310704452\n",
      "timesteps not 0\n",
      "Total Timesteps: 347200 Episode Num: 1736 Reward: -126.82406717981027\n",
      "timesteps not 0\n",
      "Total Timesteps: 347400 Episode Num: 1737 Reward: -124.28759573492164\n",
      "timesteps not 0\n",
      "Total Timesteps: 347600 Episode Num: 1738 Reward: -126.03783773159053\n",
      "timesteps not 0\n",
      "Total Timesteps: 347800 Episode Num: 1739 Reward: -226.5663691888072\n",
      "timesteps not 0\n",
      "Total Timesteps: 348000 Episode Num: 1740 Reward: -227.14480373316633\n",
      "timesteps not 0\n",
      "Total Timesteps: 348200 Episode Num: 1741 Reward: -128.67456484482747\n",
      "timesteps not 0\n",
      "Total Timesteps: 348400 Episode Num: 1742 Reward: -121.2625015671827\n",
      "timesteps not 0\n",
      "Total Timesteps: 348600 Episode Num: 1743 Reward: -128.59746112845048\n",
      "timesteps not 0\n",
      "Total Timesteps: 348800 Episode Num: 1744 Reward: -228.09857103184947\n",
      "timesteps not 0\n",
      "Total Timesteps: 349000 Episode Num: 1745 Reward: -128.27979934638648\n",
      "timesteps not 0\n",
      "Total Timesteps: 349200 Episode Num: 1746 Reward: -127.85705768071513\n",
      "timesteps not 0\n",
      "Total Timesteps: 349400 Episode Num: 1747 Reward: -225.12349880701765\n",
      "timesteps not 0\n",
      "Total Timesteps: 349600 Episode Num: 1748 Reward: -239.66072137585684\n",
      "timesteps not 0\n",
      "Total Timesteps: 349800 Episode Num: 1749 Reward: -119.26213422781628\n",
      "timesteps not 0\n",
      "Total Timesteps: 350000 Episode Num: 1750 Reward: -233.80896225207783\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -161.268725\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 350200 Episode Num: 1751 Reward: -122.37004219371035\n",
      "timesteps not 0\n",
      "Total Timesteps: 350400 Episode Num: 1752 Reward: -122.85040872562385\n",
      "timesteps not 0\n",
      "Total Timesteps: 350600 Episode Num: 1753 Reward: -121.09424205521958\n",
      "timesteps not 0\n",
      "Total Timesteps: 350800 Episode Num: 1754 Reward: -301.9269890788223\n",
      "timesteps not 0\n",
      "Total Timesteps: 351000 Episode Num: 1755 Reward: -125.42551637710642\n",
      "timesteps not 0\n",
      "Total Timesteps: 351200 Episode Num: 1756 Reward: -3.817203309434767\n",
      "timesteps not 0\n",
      "Total Timesteps: 351400 Episode Num: 1757 Reward: -2.9976720828502263\n",
      "timesteps not 0\n",
      "Total Timesteps: 351600 Episode Num: 1758 Reward: -118.6495076918883\n",
      "timesteps not 0\n",
      "Total Timesteps: 351800 Episode Num: 1759 Reward: -224.47126404688606\n",
      "timesteps not 0\n",
      "Total Timesteps: 352000 Episode Num: 1760 Reward: -125.70252324252183\n",
      "timesteps not 0\n",
      "Total Timesteps: 352200 Episode Num: 1761 Reward: -244.75775081988422\n",
      "timesteps not 0\n",
      "Total Timesteps: 352400 Episode Num: 1762 Reward: -226.34117195616454\n",
      "timesteps not 0\n",
      "Total Timesteps: 352600 Episode Num: 1763 Reward: -224.3520374173598\n",
      "timesteps not 0\n",
      "Total Timesteps: 352800 Episode Num: 1764 Reward: -129.36506617930573\n",
      "timesteps not 0\n",
      "Total Timesteps: 353000 Episode Num: 1765 Reward: -117.26922052550002\n",
      "timesteps not 0\n",
      "Total Timesteps: 353200 Episode Num: 1766 Reward: -224.58443811016863\n",
      "timesteps not 0\n",
      "Total Timesteps: 353400 Episode Num: 1767 Reward: -119.80631283528366\n",
      "timesteps not 0\n",
      "Total Timesteps: 353600 Episode Num: 1768 Reward: -114.61046907641249\n",
      "timesteps not 0\n",
      "Total Timesteps: 353800 Episode Num: 1769 Reward: -120.77225124221911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps not 0\n",
      "Total Timesteps: 354000 Episode Num: 1770 Reward: -118.33478106069916\n",
      "timesteps not 0\n",
      "Total Timesteps: 354200 Episode Num: 1771 Reward: -123.25005062049472\n",
      "timesteps not 0\n",
      "Total Timesteps: 354400 Episode Num: 1772 Reward: -224.1305296232785\n",
      "timesteps not 0\n",
      "Total Timesteps: 354600 Episode Num: 1773 Reward: -116.95627662117593\n",
      "timesteps not 0\n",
      "Total Timesteps: 354800 Episode Num: 1774 Reward: -3.5409929466813406\n",
      "timesteps not 0\n",
      "Total Timesteps: 355000 Episode Num: 1775 Reward: -126.41967122969346\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -114.305852\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 355200 Episode Num: 1776 Reward: -220.05740862454638\n",
      "timesteps not 0\n",
      "Total Timesteps: 355400 Episode Num: 1777 Reward: -323.46020477923247\n",
      "timesteps not 0\n",
      "Total Timesteps: 355600 Episode Num: 1778 Reward: -220.093716531522\n",
      "timesteps not 0\n",
      "Total Timesteps: 355800 Episode Num: 1779 Reward: -0.6821259125461594\n",
      "timesteps not 0\n",
      "Total Timesteps: 356000 Episode Num: 1780 Reward: -1.0730630158246746\n",
      "timesteps not 0\n",
      "Total Timesteps: 356200 Episode Num: 1781 Reward: -115.46253703662154\n",
      "timesteps not 0\n",
      "Total Timesteps: 356400 Episode Num: 1782 Reward: -119.62372820272401\n",
      "timesteps not 0\n",
      "Total Timesteps: 356600 Episode Num: 1783 Reward: -231.61612300819505\n",
      "timesteps not 0\n",
      "Total Timesteps: 356800 Episode Num: 1784 Reward: -223.23474107770048\n",
      "timesteps not 0\n",
      "Total Timesteps: 357000 Episode Num: 1785 Reward: -124.39415830466018\n",
      "timesteps not 0\n",
      "Total Timesteps: 357200 Episode Num: 1786 Reward: -124.80306167180655\n",
      "timesteps not 0\n",
      "Total Timesteps: 357400 Episode Num: 1787 Reward: -217.18741252778491\n",
      "timesteps not 0\n",
      "Total Timesteps: 357600 Episode Num: 1788 Reward: -234.4192740482335\n",
      "timesteps not 0\n",
      "Total Timesteps: 357800 Episode Num: 1789 Reward: -123.15275511322473\n",
      "timesteps not 0\n",
      "Total Timesteps: 358000 Episode Num: 1790 Reward: -4.92508337888822\n",
      "timesteps not 0\n",
      "Total Timesteps: 358200 Episode Num: 1791 Reward: -250.71399199109348\n",
      "timesteps not 0\n",
      "Total Timesteps: 358400 Episode Num: 1792 Reward: -222.38817067670988\n",
      "timesteps not 0\n",
      "Total Timesteps: 358600 Episode Num: 1793 Reward: -120.31468270701734\n",
      "timesteps not 0\n",
      "Total Timesteps: 358800 Episode Num: 1794 Reward: -119.00530405865891\n",
      "timesteps not 0\n",
      "Total Timesteps: 359000 Episode Num: 1795 Reward: -235.108776961559\n",
      "timesteps not 0\n",
      "Total Timesteps: 359200 Episode Num: 1796 Reward: -242.64635783429918\n",
      "timesteps not 0\n",
      "Total Timesteps: 359400 Episode Num: 1797 Reward: -226.4597221005766\n",
      "timesteps not 0\n",
      "Total Timesteps: 359600 Episode Num: 1798 Reward: -121.21437193178144\n",
      "timesteps not 0\n",
      "Total Timesteps: 359800 Episode Num: 1799 Reward: -244.0501426111913\n",
      "timesteps not 0\n",
      "Total Timesteps: 360000 Episode Num: 1800 Reward: -266.7462845054797\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -132.124581\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 360200 Episode Num: 1801 Reward: -120.388758779112\n",
      "timesteps not 0\n",
      "Total Timesteps: 360400 Episode Num: 1802 Reward: -4.521422135493955\n",
      "timesteps not 0\n",
      "Total Timesteps: 360600 Episode Num: 1803 Reward: -6.0847599582224134\n",
      "timesteps not 0\n",
      "Total Timesteps: 360800 Episode Num: 1804 Reward: -224.27773450901557\n",
      "timesteps not 0\n",
      "Total Timesteps: 361000 Episode Num: 1805 Reward: -124.02136341001138\n",
      "timesteps not 0\n",
      "Total Timesteps: 361200 Episode Num: 1806 Reward: -126.32588523932655\n",
      "timesteps not 0\n",
      "Total Timesteps: 361400 Episode Num: 1807 Reward: -221.56675105820906\n",
      "timesteps not 0\n",
      "Total Timesteps: 361600 Episode Num: 1808 Reward: -3.3576779300142174\n",
      "timesteps not 0\n",
      "Total Timesteps: 361800 Episode Num: 1809 Reward: -120.38024397462652\n",
      "timesteps not 0\n",
      "Total Timesteps: 362000 Episode Num: 1810 Reward: -236.72070269335694\n",
      "timesteps not 0\n",
      "Total Timesteps: 362200 Episode Num: 1811 Reward: -120.7464782756129\n",
      "timesteps not 0\n",
      "Total Timesteps: 362400 Episode Num: 1812 Reward: -120.67851360497309\n",
      "timesteps not 0\n",
      "Total Timesteps: 362600 Episode Num: 1813 Reward: -119.34153861694621\n",
      "timesteps not 0\n",
      "Total Timesteps: 362800 Episode Num: 1814 Reward: -121.44772466481153\n",
      "timesteps not 0\n",
      "Total Timesteps: 363000 Episode Num: 1815 Reward: -119.1657772764215\n",
      "timesteps not 0\n",
      "Total Timesteps: 363200 Episode Num: 1816 Reward: -128.26831172475752\n",
      "timesteps not 0\n",
      "Total Timesteps: 363400 Episode Num: 1817 Reward: -3.789660530032488\n",
      "timesteps not 0\n",
      "Total Timesteps: 363600 Episode Num: 1818 Reward: -245.3791228468952\n",
      "timesteps not 0\n",
      "Total Timesteps: 363800 Episode Num: 1819 Reward: -131.37006806594044\n",
      "timesteps not 0\n",
      "Total Timesteps: 364000 Episode Num: 1820 Reward: -119.73971272051331\n",
      "timesteps not 0\n",
      "Total Timesteps: 364200 Episode Num: 1821 Reward: -3.0389227711022717\n",
      "timesteps not 0\n",
      "Total Timesteps: 364400 Episode Num: 1822 Reward: -298.9855994187184\n",
      "timesteps not 0\n",
      "Total Timesteps: 364600 Episode Num: 1823 Reward: -129.3181780389529\n",
      "timesteps not 0\n",
      "Total Timesteps: 364800 Episode Num: 1824 Reward: -129.82308843062583\n",
      "timesteps not 0\n",
      "Total Timesteps: 365000 Episode Num: 1825 Reward: -243.08305234962947\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -154.539915\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 365200 Episode Num: 1826 Reward: -122.5715997117644\n",
      "timesteps not 0\n",
      "Total Timesteps: 365400 Episode Num: 1827 Reward: -124.71502022525358\n",
      "timesteps not 0\n",
      "Total Timesteps: 365600 Episode Num: 1828 Reward: -350.9960814202876\n",
      "timesteps not 0\n",
      "Total Timesteps: 365800 Episode Num: 1829 Reward: -244.68369066495032\n",
      "timesteps not 0\n",
      "Total Timesteps: 366000 Episode Num: 1830 Reward: -221.78024257908777\n",
      "timesteps not 0\n",
      "Total Timesteps: 366200 Episode Num: 1831 Reward: -314.2925174726829\n",
      "timesteps not 0\n",
      "Total Timesteps: 366400 Episode Num: 1832 Reward: -127.63807599340036\n",
      "timesteps not 0\n",
      "Total Timesteps: 366600 Episode Num: 1833 Reward: -230.6667164092857\n",
      "timesteps not 0\n",
      "Total Timesteps: 366800 Episode Num: 1834 Reward: -114.51758069800783\n",
      "timesteps not 0\n",
      "Total Timesteps: 367000 Episode Num: 1835 Reward: -0.16404201342688277\n",
      "timesteps not 0\n",
      "Total Timesteps: 367200 Episode Num: 1836 Reward: -226.02414940532387\n",
      "timesteps not 0\n",
      "Total Timesteps: 367400 Episode Num: 1837 Reward: -121.0998890451457\n",
      "timesteps not 0\n",
      "Total Timesteps: 367600 Episode Num: 1838 Reward: -223.582511661826\n",
      "timesteps not 0\n",
      "Total Timesteps: 367800 Episode Num: 1839 Reward: -119.71981188539884\n",
      "timesteps not 0\n",
      "Total Timesteps: 368000 Episode Num: 1840 Reward: -236.54352109136374\n",
      "timesteps not 0\n",
      "Total Timesteps: 368200 Episode Num: 1841 Reward: -238.67781066735603\n",
      "timesteps not 0\n",
      "Total Timesteps: 368400 Episode Num: 1842 Reward: -124.65951886149332\n",
      "timesteps not 0\n",
      "Total Timesteps: 368600 Episode Num: 1843 Reward: -121.04845146437191\n",
      "timesteps not 0\n",
      "Total Timesteps: 368800 Episode Num: 1844 Reward: -362.8634033779765\n",
      "timesteps not 0\n",
      "Total Timesteps: 369000 Episode Num: 1845 Reward: -126.8842100496371\n",
      "timesteps not 0\n",
      "Total Timesteps: 369200 Episode Num: 1846 Reward: -127.02042862101979\n",
      "timesteps not 0\n",
      "Total Timesteps: 369400 Episode Num: 1847 Reward: -127.704949265288\n",
      "timesteps not 0\n",
      "Total Timesteps: 369600 Episode Num: 1848 Reward: -115.12337920012376\n",
      "timesteps not 0\n",
      "Total Timesteps: 369800 Episode Num: 1849 Reward: -117.82618822052828\n",
      "timesteps not 0\n",
      "Total Timesteps: 370000 Episode Num: 1850 Reward: -242.77089959660796\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -125.684416\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 370200 Episode Num: 1851 Reward: -340.1144489492243\n",
      "timesteps not 0\n",
      "Total Timesteps: 370400 Episode Num: 1852 Reward: -124.16302133462135\n",
      "timesteps not 0\n",
      "Total Timesteps: 370600 Episode Num: 1853 Reward: -125.85436440742018\n",
      "timesteps not 0\n",
      "Total Timesteps: 370800 Episode Num: 1854 Reward: -115.6328691669593\n",
      "timesteps not 0\n",
      "Total Timesteps: 371000 Episode Num: 1855 Reward: -114.34613439433197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps not 0\n",
      "Total Timesteps: 371200 Episode Num: 1856 Reward: -125.44927787957042\n",
      "timesteps not 0\n",
      "Total Timesteps: 371400 Episode Num: 1857 Reward: -113.62289000346344\n",
      "timesteps not 0\n",
      "Total Timesteps: 371600 Episode Num: 1858 Reward: -128.16997232788123\n",
      "timesteps not 0\n",
      "Total Timesteps: 371800 Episode Num: 1859 Reward: -114.8576007932158\n",
      "timesteps not 0\n",
      "Total Timesteps: 372000 Episode Num: 1860 Reward: -128.9686023042708\n",
      "timesteps not 0\n",
      "Total Timesteps: 372200 Episode Num: 1861 Reward: -221.50073026534372\n",
      "timesteps not 0\n",
      "Total Timesteps: 372400 Episode Num: 1862 Reward: -120.91254928932783\n",
      "timesteps not 0\n",
      "Total Timesteps: 372600 Episode Num: 1863 Reward: -127.29313038640443\n",
      "timesteps not 0\n",
      "Total Timesteps: 372800 Episode Num: 1864 Reward: -219.69821491001176\n",
      "timesteps not 0\n",
      "Total Timesteps: 373000 Episode Num: 1865 Reward: -290.3168817360285\n",
      "timesteps not 0\n",
      "Total Timesteps: 373200 Episode Num: 1866 Reward: -119.08961090880366\n",
      "timesteps not 0\n",
      "Total Timesteps: 373400 Episode Num: 1867 Reward: -223.4242556301222\n",
      "timesteps not 0\n",
      "Total Timesteps: 373600 Episode Num: 1868 Reward: -123.1848456386319\n",
      "timesteps not 0\n",
      "Total Timesteps: 373800 Episode Num: 1869 Reward: -232.053644306821\n",
      "timesteps not 0\n",
      "Total Timesteps: 374000 Episode Num: 1870 Reward: -119.84907067346997\n",
      "timesteps not 0\n",
      "Total Timesteps: 374200 Episode Num: 1871 Reward: -120.92342355040698\n",
      "timesteps not 0\n",
      "Total Timesteps: 374400 Episode Num: 1872 Reward: -2.851869924474547\n",
      "timesteps not 0\n",
      "Total Timesteps: 374600 Episode Num: 1873 Reward: -126.07214083225583\n",
      "timesteps not 0\n",
      "Total Timesteps: 374800 Episode Num: 1874 Reward: -127.02268068667236\n",
      "timesteps not 0\n",
      "Total Timesteps: 375000 Episode Num: 1875 Reward: -125.95901494927651\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -97.508176\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 375200 Episode Num: 1876 Reward: -117.21067514649019\n",
      "timesteps not 0\n",
      "Total Timesteps: 375400 Episode Num: 1877 Reward: -219.831100995358\n",
      "timesteps not 0\n",
      "Total Timesteps: 375600 Episode Num: 1878 Reward: -296.1067654154592\n",
      "timesteps not 0\n",
      "Total Timesteps: 375800 Episode Num: 1879 Reward: -116.80517493343787\n",
      "timesteps not 0\n",
      "Total Timesteps: 376000 Episode Num: 1880 Reward: -117.3968895018035\n",
      "timesteps not 0\n",
      "Total Timesteps: 376200 Episode Num: 1881 Reward: -117.82340707664548\n",
      "timesteps not 0\n",
      "Total Timesteps: 376400 Episode Num: 1882 Reward: -128.48523686873935\n",
      "timesteps not 0\n",
      "Total Timesteps: 376600 Episode Num: 1883 Reward: -231.2117815138075\n",
      "timesteps not 0\n",
      "Total Timesteps: 376800 Episode Num: 1884 Reward: -120.26166241793776\n",
      "timesteps not 0\n",
      "Total Timesteps: 377000 Episode Num: 1885 Reward: -123.29186351356911\n",
      "timesteps not 0\n",
      "Total Timesteps: 377200 Episode Num: 1886 Reward: -125.46331171633706\n",
      "timesteps not 0\n",
      "Total Timesteps: 377400 Episode Num: 1887 Reward: -240.31785692884233\n",
      "timesteps not 0\n",
      "Total Timesteps: 377600 Episode Num: 1888 Reward: -227.21125581764056\n",
      "timesteps not 0\n",
      "Total Timesteps: 377800 Episode Num: 1889 Reward: -229.9653246332607\n",
      "timesteps not 0\n",
      "Total Timesteps: 378000 Episode Num: 1890 Reward: -236.07815515670012\n",
      "timesteps not 0\n",
      "Total Timesteps: 378200 Episode Num: 1891 Reward: -214.85280887173965\n",
      "timesteps not 0\n",
      "Total Timesteps: 378400 Episode Num: 1892 Reward: -117.9327497344067\n",
      "timesteps not 0\n",
      "Total Timesteps: 378600 Episode Num: 1893 Reward: -124.8594415207415\n",
      "timesteps not 0\n",
      "Total Timesteps: 378800 Episode Num: 1894 Reward: -0.7361639379127725\n",
      "timesteps not 0\n",
      "Total Timesteps: 379000 Episode Num: 1895 Reward: -127.11458452341968\n",
      "timesteps not 0\n",
      "Total Timesteps: 379200 Episode Num: 1896 Reward: -0.7787276691814999\n",
      "timesteps not 0\n",
      "Total Timesteps: 379400 Episode Num: 1897 Reward: -224.08808270224225\n",
      "timesteps not 0\n",
      "Total Timesteps: 379600 Episode Num: 1898 Reward: -116.25069321180554\n",
      "timesteps not 0\n",
      "Total Timesteps: 379800 Episode Num: 1899 Reward: -129.07398290979836\n",
      "timesteps not 0\n",
      "Total Timesteps: 380000 Episode Num: 1900 Reward: -233.26342507566358\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -170.948030\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 380200 Episode Num: 1901 Reward: -0.8664971739567557\n",
      "timesteps not 0\n",
      "Total Timesteps: 380400 Episode Num: 1902 Reward: -244.70214710973138\n",
      "timesteps not 0\n",
      "Total Timesteps: 380600 Episode Num: 1903 Reward: -120.19730706160077\n",
      "timesteps not 0\n",
      "Total Timesteps: 380800 Episode Num: 1904 Reward: -4.6746392165611885\n",
      "timesteps not 0\n",
      "Total Timesteps: 381000 Episode Num: 1905 Reward: -122.43735621079982\n",
      "timesteps not 0\n",
      "Total Timesteps: 381200 Episode Num: 1906 Reward: -119.85157931953518\n",
      "timesteps not 0\n",
      "Total Timesteps: 381400 Episode Num: 1907 Reward: -121.81551951025128\n",
      "timesteps not 0\n",
      "Total Timesteps: 381600 Episode Num: 1908 Reward: -291.37677291881295\n",
      "timesteps not 0\n",
      "Total Timesteps: 381800 Episode Num: 1909 Reward: -5.541714078844157\n",
      "timesteps not 0\n",
      "Total Timesteps: 382000 Episode Num: 1910 Reward: -127.37865097846654\n",
      "timesteps not 0\n",
      "Total Timesteps: 382200 Episode Num: 1911 Reward: -120.44014339192148\n",
      "timesteps not 0\n",
      "Total Timesteps: 382400 Episode Num: 1912 Reward: -120.04405860140574\n",
      "timesteps not 0\n",
      "Total Timesteps: 382600 Episode Num: 1913 Reward: -233.33042864100412\n",
      "timesteps not 0\n",
      "Total Timesteps: 382800 Episode Num: 1914 Reward: -336.11089808050576\n",
      "timesteps not 0\n",
      "Total Timesteps: 383000 Episode Num: 1915 Reward: -119.76362104392312\n",
      "timesteps not 0\n",
      "Total Timesteps: 383200 Episode Num: 1916 Reward: -121.06957379148027\n",
      "timesteps not 0\n",
      "Total Timesteps: 383400 Episode Num: 1917 Reward: -122.26968163309714\n",
      "timesteps not 0\n",
      "Total Timesteps: 383600 Episode Num: 1918 Reward: -298.26227815105483\n",
      "timesteps not 0\n",
      "Total Timesteps: 383800 Episode Num: 1919 Reward: -120.8077345285501\n",
      "timesteps not 0\n",
      "Total Timesteps: 384000 Episode Num: 1920 Reward: -3.6682823076312427\n",
      "timesteps not 0\n",
      "Total Timesteps: 384200 Episode Num: 1921 Reward: -229.35803068239287\n",
      "timesteps not 0\n",
      "Total Timesteps: 384400 Episode Num: 1922 Reward: -129.552508839902\n",
      "timesteps not 0\n",
      "Total Timesteps: 384600 Episode Num: 1923 Reward: -118.06542833620666\n",
      "timesteps not 0\n",
      "Total Timesteps: 384800 Episode Num: 1924 Reward: -234.7732095245178\n",
      "timesteps not 0\n",
      "Total Timesteps: 385000 Episode Num: 1925 Reward: -226.16758915435827\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -143.932844\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 385200 Episode Num: 1926 Reward: -3.520962252373358\n",
      "timesteps not 0\n",
      "Total Timesteps: 385400 Episode Num: 1927 Reward: -124.05938025698332\n",
      "timesteps not 0\n",
      "Total Timesteps: 385600 Episode Num: 1928 Reward: -219.3111601410795\n",
      "timesteps not 0\n",
      "Total Timesteps: 385800 Episode Num: 1929 Reward: -220.68417028015185\n",
      "timesteps not 0\n",
      "Total Timesteps: 386000 Episode Num: 1930 Reward: -118.44533518205759\n",
      "timesteps not 0\n",
      "Total Timesteps: 386200 Episode Num: 1931 Reward: -121.24774600305942\n",
      "timesteps not 0\n",
      "Total Timesteps: 386400 Episode Num: 1932 Reward: -120.65730637123046\n",
      "timesteps not 0\n",
      "Total Timesteps: 386600 Episode Num: 1933 Reward: -223.6366077286393\n",
      "timesteps not 0\n",
      "Total Timesteps: 386800 Episode Num: 1934 Reward: -229.32279151487427\n",
      "timesteps not 0\n",
      "Total Timesteps: 387000 Episode Num: 1935 Reward: -121.28891762880897\n",
      "timesteps not 0\n",
      "Total Timesteps: 387200 Episode Num: 1936 Reward: -6.564087557813222\n",
      "timesteps not 0\n",
      "Total Timesteps: 387400 Episode Num: 1937 Reward: -122.75307561327291\n",
      "timesteps not 0\n",
      "Total Timesteps: 387600 Episode Num: 1938 Reward: -129.03991886953884\n",
      "timesteps not 0\n",
      "Total Timesteps: 387800 Episode Num: 1939 Reward: -131.08488399459907\n",
      "timesteps not 0\n",
      "Total Timesteps: 388000 Episode Num: 1940 Reward: -120.73753909078575\n",
      "timesteps not 0\n",
      "Total Timesteps: 388200 Episode Num: 1941 Reward: -133.5031118022674\n",
      "timesteps not 0\n",
      "Total Timesteps: 388400 Episode Num: 1942 Reward: -129.15614586067784\n",
      "timesteps not 0\n",
      "Total Timesteps: 388600 Episode Num: 1943 Reward: -121.99271624466073\n",
      "timesteps not 0\n",
      "Total Timesteps: 388800 Episode Num: 1944 Reward: -121.36448674904199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps not 0\n",
      "Total Timesteps: 389000 Episode Num: 1945 Reward: -123.66242758217753\n",
      "timesteps not 0\n",
      "Total Timesteps: 389200 Episode Num: 1946 Reward: -116.79074764917851\n",
      "timesteps not 0\n",
      "Total Timesteps: 389400 Episode Num: 1947 Reward: -133.05375063975907\n",
      "timesteps not 0\n",
      "Total Timesteps: 389600 Episode Num: 1948 Reward: -217.9592665360316\n",
      "timesteps not 0\n",
      "Total Timesteps: 389800 Episode Num: 1949 Reward: -220.4627153992924\n",
      "timesteps not 0\n",
      "Total Timesteps: 390000 Episode Num: 1950 Reward: -121.09846228713704\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -161.848018\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 390200 Episode Num: 1951 Reward: -312.5611553818514\n",
      "timesteps not 0\n",
      "Total Timesteps: 390400 Episode Num: 1952 Reward: -325.6779375163869\n",
      "timesteps not 0\n",
      "Total Timesteps: 390600 Episode Num: 1953 Reward: -252.98227953285644\n",
      "timesteps not 0\n",
      "Total Timesteps: 390800 Episode Num: 1954 Reward: -128.0925113522711\n",
      "timesteps not 0\n",
      "Total Timesteps: 391000 Episode Num: 1955 Reward: -117.88802231789043\n",
      "timesteps not 0\n",
      "Total Timesteps: 391200 Episode Num: 1956 Reward: -232.22139207668027\n",
      "timesteps not 0\n",
      "Total Timesteps: 391400 Episode Num: 1957 Reward: -123.2399349193802\n",
      "timesteps not 0\n",
      "Total Timesteps: 391600 Episode Num: 1958 Reward: -123.27504895319832\n",
      "timesteps not 0\n",
      "Total Timesteps: 391800 Episode Num: 1959 Reward: -274.9679660091781\n",
      "timesteps not 0\n",
      "Total Timesteps: 392000 Episode Num: 1960 Reward: -130.81525980522846\n",
      "timesteps not 0\n",
      "Total Timesteps: 392200 Episode Num: 1961 Reward: -122.7293074083659\n",
      "timesteps not 0\n",
      "Total Timesteps: 392400 Episode Num: 1962 Reward: -122.19298117271386\n",
      "timesteps not 0\n",
      "Total Timesteps: 392600 Episode Num: 1963 Reward: -128.91053286528927\n",
      "timesteps not 0\n",
      "Total Timesteps: 392800 Episode Num: 1964 Reward: -118.95393265911683\n",
      "timesteps not 0\n",
      "Total Timesteps: 393000 Episode Num: 1965 Reward: -1.547201183686034\n",
      "timesteps not 0\n",
      "Total Timesteps: 393200 Episode Num: 1966 Reward: -117.35940937396279\n",
      "timesteps not 0\n",
      "Total Timesteps: 393400 Episode Num: 1967 Reward: -116.62408186390202\n",
      "timesteps not 0\n",
      "Total Timesteps: 393600 Episode Num: 1968 Reward: -122.5806162300887\n",
      "timesteps not 0\n",
      "Total Timesteps: 393800 Episode Num: 1969 Reward: -3.132436192111573\n",
      "timesteps not 0\n",
      "Total Timesteps: 394000 Episode Num: 1970 Reward: -121.53670387138666\n",
      "timesteps not 0\n",
      "Total Timesteps: 394200 Episode Num: 1971 Reward: -119.55345021601087\n",
      "timesteps not 0\n",
      "Total Timesteps: 394400 Episode Num: 1972 Reward: -0.9909787321086436\n",
      "timesteps not 0\n",
      "Total Timesteps: 394600 Episode Num: 1973 Reward: -0.38179588021558153\n",
      "timesteps not 0\n",
      "Total Timesteps: 394800 Episode Num: 1974 Reward: -123.63764233664446\n",
      "timesteps not 0\n",
      "Total Timesteps: 395000 Episode Num: 1975 Reward: -118.59483131537858\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -115.764566\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 395200 Episode Num: 1976 Reward: -228.4515462343811\n",
      "timesteps not 0\n",
      "Total Timesteps: 395400 Episode Num: 1977 Reward: -0.7771392429773812\n",
      "timesteps not 0\n",
      "Total Timesteps: 395600 Episode Num: 1978 Reward: -240.31549922160465\n",
      "timesteps not 0\n",
      "Total Timesteps: 395800 Episode Num: 1979 Reward: -124.27739023622065\n",
      "timesteps not 0\n",
      "Total Timesteps: 396000 Episode Num: 1980 Reward: -116.22778788352615\n",
      "timesteps not 0\n",
      "Total Timesteps: 396200 Episode Num: 1981 Reward: -1.0578683623083263\n",
      "timesteps not 0\n",
      "Total Timesteps: 396400 Episode Num: 1982 Reward: -116.21363410378886\n",
      "timesteps not 0\n",
      "Total Timesteps: 396600 Episode Num: 1983 Reward: -117.84181465941272\n",
      "timesteps not 0\n",
      "Total Timesteps: 396800 Episode Num: 1984 Reward: -217.17698902445593\n",
      "timesteps not 0\n",
      "Total Timesteps: 397000 Episode Num: 1985 Reward: -116.51438709067669\n",
      "timesteps not 0\n",
      "Total Timesteps: 397200 Episode Num: 1986 Reward: -114.53072049348857\n",
      "timesteps not 0\n",
      "Total Timesteps: 397400 Episode Num: 1987 Reward: -128.03168310417013\n",
      "timesteps not 0\n",
      "Total Timesteps: 397600 Episode Num: 1988 Reward: -226.49476989557036\n",
      "timesteps not 0\n",
      "Total Timesteps: 397800 Episode Num: 1989 Reward: -293.17492041212074\n",
      "timesteps not 0\n",
      "Total Timesteps: 398000 Episode Num: 1990 Reward: -1.8234556542283695\n",
      "timesteps not 0\n",
      "Total Timesteps: 398200 Episode Num: 1991 Reward: -2.718169275645336\n",
      "timesteps not 0\n",
      "Total Timesteps: 398400 Episode Num: 1992 Reward: -114.73721411743935\n",
      "timesteps not 0\n",
      "Total Timesteps: 398600 Episode Num: 1993 Reward: -125.5048064993645\n",
      "timesteps not 0\n",
      "Total Timesteps: 398800 Episode Num: 1994 Reward: -116.45968202196336\n",
      "timesteps not 0\n",
      "Total Timesteps: 399000 Episode Num: 1995 Reward: -2.0870474151969756\n",
      "timesteps not 0\n",
      "Total Timesteps: 399200 Episode Num: 1996 Reward: -127.09050787799892\n",
      "timesteps not 0\n",
      "Total Timesteps: 399400 Episode Num: 1997 Reward: -218.08902731550796\n",
      "timesteps not 0\n",
      "Total Timesteps: 399600 Episode Num: 1998 Reward: -298.28778283179076\n",
      "timesteps not 0\n",
      "Total Timesteps: 399800 Episode Num: 1999 Reward: -5.738271579735557\n",
      "timesteps not 0\n",
      "Total Timesteps: 400000 Episode Num: 2000 Reward: -230.36656532550788\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -146.689427\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 400200 Episode Num: 2001 Reward: -2.3102588782257936\n",
      "timesteps not 0\n",
      "Total Timesteps: 400400 Episode Num: 2002 Reward: -120.40856046740718\n",
      "timesteps not 0\n",
      "Total Timesteps: 400600 Episode Num: 2003 Reward: -242.46547852264368\n",
      "timesteps not 0\n",
      "Total Timesteps: 400800 Episode Num: 2004 Reward: -124.59127658385579\n",
      "timesteps not 0\n",
      "Total Timesteps: 401000 Episode Num: 2005 Reward: -120.84760100901002\n",
      "timesteps not 0\n",
      "Total Timesteps: 401200 Episode Num: 2006 Reward: -0.4542930689214911\n",
      "timesteps not 0\n",
      "Total Timesteps: 401400 Episode Num: 2007 Reward: -0.9378208922841462\n",
      "timesteps not 0\n",
      "Total Timesteps: 401600 Episode Num: 2008 Reward: -117.10280064231172\n",
      "timesteps not 0\n",
      "Total Timesteps: 401800 Episode Num: 2009 Reward: -222.27734137242385\n",
      "timesteps not 0\n",
      "Total Timesteps: 402000 Episode Num: 2010 Reward: -126.13715723527567\n",
      "timesteps not 0\n",
      "Total Timesteps: 402200 Episode Num: 2011 Reward: -116.60252733670058\n",
      "timesteps not 0\n",
      "Total Timesteps: 402400 Episode Num: 2012 Reward: -229.8137962862025\n",
      "timesteps not 0\n",
      "Total Timesteps: 402600 Episode Num: 2013 Reward: -121.28604462038419\n",
      "timesteps not 0\n",
      "Total Timesteps: 402800 Episode Num: 2014 Reward: -117.0882137720988\n",
      "timesteps not 0\n",
      "Total Timesteps: 403000 Episode Num: 2015 Reward: -124.59688290987896\n",
      "timesteps not 0\n",
      "Total Timesteps: 403200 Episode Num: 2016 Reward: -115.26912102793592\n",
      "timesteps not 0\n",
      "Total Timesteps: 403400 Episode Num: 2017 Reward: -126.13428095232439\n",
      "timesteps not 0\n",
      "Total Timesteps: 403600 Episode Num: 2018 Reward: -119.35380888522297\n",
      "timesteps not 0\n",
      "Total Timesteps: 403800 Episode Num: 2019 Reward: -3.997041713643421\n",
      "timesteps not 0\n",
      "Total Timesteps: 404000 Episode Num: 2020 Reward: -232.6802652094482\n",
      "timesteps not 0\n",
      "Total Timesteps: 404200 Episode Num: 2021 Reward: -125.59881784556319\n",
      "timesteps not 0\n",
      "Total Timesteps: 404400 Episode Num: 2022 Reward: -122.47090602990565\n",
      "timesteps not 0\n",
      "Total Timesteps: 404600 Episode Num: 2023 Reward: -119.66619155355555\n",
      "timesteps not 0\n",
      "Total Timesteps: 404800 Episode Num: 2024 Reward: -328.56473948511893\n",
      "timesteps not 0\n",
      "Total Timesteps: 405000 Episode Num: 2025 Reward: -3.8222848777167253\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -194.668478\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 405200 Episode Num: 2026 Reward: -238.75416119128315\n",
      "timesteps not 0\n",
      "Total Timesteps: 405400 Episode Num: 2027 Reward: -219.56039456870283\n",
      "timesteps not 0\n",
      "Total Timesteps: 405600 Episode Num: 2028 Reward: -246.8215342549021\n",
      "timesteps not 0\n",
      "Total Timesteps: 405800 Episode Num: 2029 Reward: -124.55316057919173\n",
      "timesteps not 0\n",
      "Total Timesteps: 406000 Episode Num: 2030 Reward: -2.8944316778326202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps not 0\n",
      "Total Timesteps: 406200 Episode Num: 2031 Reward: -2.839236402273535\n",
      "timesteps not 0\n",
      "Total Timesteps: 406400 Episode Num: 2032 Reward: -128.67821851857173\n",
      "timesteps not 0\n",
      "Total Timesteps: 406600 Episode Num: 2033 Reward: -117.89197248810368\n",
      "timesteps not 0\n",
      "Total Timesteps: 406800 Episode Num: 2034 Reward: -122.37387755824847\n",
      "timesteps not 0\n",
      "Total Timesteps: 407000 Episode Num: 2035 Reward: -129.87644313091366\n",
      "timesteps not 0\n",
      "Total Timesteps: 407200 Episode Num: 2036 Reward: -128.59530229870168\n",
      "timesteps not 0\n",
      "Total Timesteps: 407400 Episode Num: 2037 Reward: -121.75766222952676\n",
      "timesteps not 0\n",
      "Total Timesteps: 407600 Episode Num: 2038 Reward: -231.1419325320075\n",
      "timesteps not 0\n",
      "Total Timesteps: 407800 Episode Num: 2039 Reward: -128.8423874624029\n",
      "timesteps not 0\n",
      "Total Timesteps: 408000 Episode Num: 2040 Reward: -336.3823909296596\n",
      "timesteps not 0\n",
      "Total Timesteps: 408200 Episode Num: 2041 Reward: -251.5329740170837\n",
      "timesteps not 0\n",
      "Total Timesteps: 408400 Episode Num: 2042 Reward: -237.88001151449828\n",
      "timesteps not 0\n",
      "Total Timesteps: 408600 Episode Num: 2043 Reward: -118.35594340675465\n",
      "timesteps not 0\n",
      "Total Timesteps: 408800 Episode Num: 2044 Reward: -233.7030197045846\n",
      "timesteps not 0\n",
      "Total Timesteps: 409000 Episode Num: 2045 Reward: -120.26467115010594\n",
      "timesteps not 0\n",
      "Total Timesteps: 409200 Episode Num: 2046 Reward: -118.3878757326857\n",
      "timesteps not 0\n",
      "Total Timesteps: 409400 Episode Num: 2047 Reward: -124.2755499405666\n",
      "timesteps not 0\n",
      "Total Timesteps: 409600 Episode Num: 2048 Reward: -221.27874949173696\n",
      "timesteps not 0\n",
      "Total Timesteps: 409800 Episode Num: 2049 Reward: -118.82777176087588\n",
      "timesteps not 0\n",
      "Total Timesteps: 410000 Episode Num: 2050 Reward: -238.41425254168513\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -109.428133\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 410200 Episode Num: 2051 Reward: -128.09271791218362\n",
      "timesteps not 0\n",
      "Total Timesteps: 410400 Episode Num: 2052 Reward: -0.9533577685373315\n",
      "timesteps not 0\n",
      "Total Timesteps: 410600 Episode Num: 2053 Reward: -243.3488012819916\n",
      "timesteps not 0\n",
      "Total Timesteps: 410800 Episode Num: 2054 Reward: -114.48685482639993\n",
      "timesteps not 0\n",
      "Total Timesteps: 411000 Episode Num: 2055 Reward: -0.5825443406652291\n",
      "timesteps not 0\n",
      "Total Timesteps: 411200 Episode Num: 2056 Reward: -2.973813505336107\n",
      "timesteps not 0\n",
      "Total Timesteps: 411400 Episode Num: 2057 Reward: -1.0885836007599305\n",
      "timesteps not 0\n",
      "Total Timesteps: 411600 Episode Num: 2058 Reward: -116.91757549085352\n",
      "timesteps not 0\n",
      "Total Timesteps: 411800 Episode Num: 2059 Reward: -220.3374217546487\n",
      "timesteps not 0\n",
      "Total Timesteps: 412000 Episode Num: 2060 Reward: -219.49433768356\n",
      "timesteps not 0\n",
      "Total Timesteps: 412200 Episode Num: 2061 Reward: -122.63008172605458\n",
      "timesteps not 0\n",
      "Total Timesteps: 412400 Episode Num: 2062 Reward: -121.86092717488793\n",
      "timesteps not 0\n",
      "Total Timesteps: 412600 Episode Num: 2063 Reward: -1.0215557407988338\n",
      "timesteps not 0\n",
      "Total Timesteps: 412800 Episode Num: 2064 Reward: -119.814839901496\n",
      "timesteps not 0\n",
      "Total Timesteps: 413000 Episode Num: 2065 Reward: -0.23850366433080739\n",
      "timesteps not 0\n",
      "Total Timesteps: 413200 Episode Num: 2066 Reward: -124.23637671831564\n",
      "timesteps not 0\n",
      "Total Timesteps: 413400 Episode Num: 2067 Reward: -114.77295688344415\n",
      "timesteps not 0\n",
      "Total Timesteps: 413600 Episode Num: 2068 Reward: -124.64903897568183\n",
      "timesteps not 0\n",
      "Total Timesteps: 413800 Episode Num: 2069 Reward: -123.99753331497074\n",
      "timesteps not 0\n",
      "Total Timesteps: 414000 Episode Num: 2070 Reward: -221.90253951694623\n",
      "timesteps not 0\n",
      "Total Timesteps: 414200 Episode Num: 2071 Reward: -221.8839695907486\n",
      "timesteps not 0\n",
      "Total Timesteps: 414400 Episode Num: 2072 Reward: -117.36889394912167\n",
      "timesteps not 0\n",
      "Total Timesteps: 414600 Episode Num: 2073 Reward: -127.43437351248869\n",
      "timesteps not 0\n",
      "Total Timesteps: 414800 Episode Num: 2074 Reward: -5.1865478103639875\n",
      "timesteps not 0\n",
      "Total Timesteps: 415000 Episode Num: 2075 Reward: -221.54630613644454\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -123.992622\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 415200 Episode Num: 2076 Reward: -126.58456949294441\n",
      "timesteps not 0\n",
      "Total Timesteps: 415400 Episode Num: 2077 Reward: -122.04185147877021\n",
      "timesteps not 0\n",
      "Total Timesteps: 415600 Episode Num: 2078 Reward: -117.5625600051016\n",
      "timesteps not 0\n",
      "Total Timesteps: 415800 Episode Num: 2079 Reward: -234.80400694417926\n",
      "timesteps not 0\n",
      "Total Timesteps: 416000 Episode Num: 2080 Reward: -2.247390936106656\n",
      "timesteps not 0\n",
      "Total Timesteps: 416200 Episode Num: 2081 Reward: -122.22943600366358\n",
      "timesteps not 0\n",
      "Total Timesteps: 416400 Episode Num: 2082 Reward: -114.66728295178187\n",
      "timesteps not 0\n",
      "Total Timesteps: 416600 Episode Num: 2083 Reward: -245.4728441809597\n",
      "timesteps not 0\n",
      "Total Timesteps: 416800 Episode Num: 2084 Reward: -248.7139399104738\n",
      "timesteps not 0\n",
      "Total Timesteps: 417000 Episode Num: 2085 Reward: -220.1938698839661\n",
      "timesteps not 0\n",
      "Total Timesteps: 417200 Episode Num: 2086 Reward: -125.05389406925475\n",
      "timesteps not 0\n",
      "Total Timesteps: 417400 Episode Num: 2087 Reward: -224.2192992680734\n",
      "timesteps not 0\n",
      "Total Timesteps: 417600 Episode Num: 2088 Reward: -2.0084437482197117\n",
      "timesteps not 0\n",
      "Total Timesteps: 417800 Episode Num: 2089 Reward: -124.74702536062148\n",
      "timesteps not 0\n",
      "Total Timesteps: 418000 Episode Num: 2090 Reward: -123.32352367008025\n",
      "timesteps not 0\n",
      "Total Timesteps: 418200 Episode Num: 2091 Reward: -120.01460947291508\n",
      "timesteps not 0\n",
      "Total Timesteps: 418400 Episode Num: 2092 Reward: -121.56696270991908\n",
      "timesteps not 0\n",
      "Total Timesteps: 418600 Episode Num: 2093 Reward: -115.04279149467456\n",
      "timesteps not 0\n",
      "Total Timesteps: 418800 Episode Num: 2094 Reward: -1.166553448330672\n",
      "timesteps not 0\n",
      "Total Timesteps: 419000 Episode Num: 2095 Reward: -115.15990789335518\n",
      "timesteps not 0\n",
      "Total Timesteps: 419200 Episode Num: 2096 Reward: -2.3174378786666927\n",
      "timesteps not 0\n",
      "Total Timesteps: 419400 Episode Num: 2097 Reward: -123.87894038329183\n",
      "timesteps not 0\n",
      "Total Timesteps: 419600 Episode Num: 2098 Reward: -219.4401333024784\n",
      "timesteps not 0\n",
      "Total Timesteps: 419800 Episode Num: 2099 Reward: -3.1444384753476653\n",
      "timesteps not 0\n",
      "Total Timesteps: 420000 Episode Num: 2100 Reward: -303.5809571225366\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -121.481380\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 420200 Episode Num: 2101 Reward: -123.93585099333878\n",
      "timesteps not 0\n",
      "Total Timesteps: 420400 Episode Num: 2102 Reward: -116.15057583513257\n",
      "timesteps not 0\n",
      "Total Timesteps: 420600 Episode Num: 2103 Reward: -239.65314663419812\n",
      "timesteps not 0\n",
      "Total Timesteps: 420800 Episode Num: 2104 Reward: -118.74444318027682\n",
      "timesteps not 0\n",
      "Total Timesteps: 421000 Episode Num: 2105 Reward: -240.74329124068538\n",
      "timesteps not 0\n",
      "Total Timesteps: 421200 Episode Num: 2106 Reward: -1.8935359857097565\n",
      "timesteps not 0\n",
      "Total Timesteps: 421400 Episode Num: 2107 Reward: -324.5579326686136\n",
      "timesteps not 0\n",
      "Total Timesteps: 421600 Episode Num: 2108 Reward: -127.1721161665003\n",
      "timesteps not 0\n",
      "Total Timesteps: 421800 Episode Num: 2109 Reward: -124.30791780404226\n",
      "timesteps not 0\n",
      "Total Timesteps: 422000 Episode Num: 2110 Reward: -2.2248819925043803\n",
      "timesteps not 0\n",
      "Total Timesteps: 422200 Episode Num: 2111 Reward: -119.01990657642324\n",
      "timesteps not 0\n",
      "Total Timesteps: 422400 Episode Num: 2112 Reward: -117.64902404551738\n",
      "timesteps not 0\n",
      "Total Timesteps: 422600 Episode Num: 2113 Reward: -126.82222056224552\n",
      "timesteps not 0\n",
      "Total Timesteps: 422800 Episode Num: 2114 Reward: -225.05764549763515\n",
      "timesteps not 0\n",
      "Total Timesteps: 423000 Episode Num: 2115 Reward: -118.60517006629112\n",
      "timesteps not 0\n",
      "Total Timesteps: 423200 Episode Num: 2116 Reward: -126.50239177963135\n",
      "timesteps not 0\n",
      "Total Timesteps: 423400 Episode Num: 2117 Reward: -2.5143254309182037\n",
      "timesteps not 0\n",
      "Total Timesteps: 423600 Episode Num: 2118 Reward: -119.83376216237778\n",
      "timesteps not 0\n",
      "Total Timesteps: 423800 Episode Num: 2119 Reward: -238.2004130942323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps not 0\n",
      "Total Timesteps: 424000 Episode Num: 2120 Reward: -1.3428263003050238\n",
      "timesteps not 0\n",
      "Total Timesteps: 424200 Episode Num: 2121 Reward: -122.48020155042207\n",
      "timesteps not 0\n",
      "Total Timesteps: 424400 Episode Num: 2122 Reward: -2.464217780211263\n",
      "timesteps not 0\n",
      "Total Timesteps: 424600 Episode Num: 2123 Reward: -3.9909708607776726\n",
      "timesteps not 0\n",
      "Total Timesteps: 424800 Episode Num: 2124 Reward: -117.9481024941555\n",
      "timesteps not 0\n",
      "Total Timesteps: 425000 Episode Num: 2125 Reward: -120.84245785761813\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -163.019528\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 425200 Episode Num: 2126 Reward: -221.8327075875191\n",
      "timesteps not 0\n",
      "Total Timesteps: 425400 Episode Num: 2127 Reward: -124.76393802017158\n",
      "timesteps not 0\n",
      "Total Timesteps: 425600 Episode Num: 2128 Reward: -220.94752331755564\n",
      "timesteps not 0\n",
      "Total Timesteps: 425800 Episode Num: 2129 Reward: -224.65891929747022\n",
      "timesteps not 0\n",
      "Total Timesteps: 426000 Episode Num: 2130 Reward: -129.0464441390797\n",
      "timesteps not 0\n",
      "Total Timesteps: 426200 Episode Num: 2131 Reward: -248.3132852349398\n",
      "timesteps not 0\n",
      "Total Timesteps: 426400 Episode Num: 2132 Reward: -230.97937307151005\n",
      "timesteps not 0\n",
      "Total Timesteps: 426600 Episode Num: 2133 Reward: -118.10929597019857\n",
      "timesteps not 0\n",
      "Total Timesteps: 426800 Episode Num: 2134 Reward: -121.35050587721753\n",
      "timesteps not 0\n",
      "Total Timesteps: 427000 Episode Num: 2135 Reward: -230.9558326691045\n",
      "timesteps not 0\n",
      "Total Timesteps: 427200 Episode Num: 2136 Reward: -118.62950338731773\n",
      "timesteps not 0\n",
      "Total Timesteps: 427400 Episode Num: 2137 Reward: -119.45161889275292\n",
      "timesteps not 0\n",
      "Total Timesteps: 427600 Episode Num: 2138 Reward: -118.46322918716855\n",
      "timesteps not 0\n",
      "Total Timesteps: 427800 Episode Num: 2139 Reward: -221.7627953802642\n",
      "timesteps not 0\n",
      "Total Timesteps: 428000 Episode Num: 2140 Reward: -118.87246807483145\n",
      "timesteps not 0\n",
      "Total Timesteps: 428200 Episode Num: 2141 Reward: -225.69566190274088\n",
      "timesteps not 0\n",
      "Total Timesteps: 428400 Episode Num: 2142 Reward: -119.08596531937219\n",
      "timesteps not 0\n",
      "Total Timesteps: 428600 Episode Num: 2143 Reward: -220.06547962978948\n",
      "timesteps not 0\n",
      "Total Timesteps: 428800 Episode Num: 2144 Reward: -0.7830406381157591\n",
      "timesteps not 0\n",
      "Total Timesteps: 429000 Episode Num: 2145 Reward: -119.69564399025786\n",
      "timesteps not 0\n",
      "Total Timesteps: 429200 Episode Num: 2146 Reward: -122.88604958571668\n",
      "timesteps not 0\n",
      "Total Timesteps: 429400 Episode Num: 2147 Reward: -130.7333829141734\n",
      "timesteps not 0\n",
      "Total Timesteps: 429600 Episode Num: 2148 Reward: -121.05811900098287\n",
      "timesteps not 0\n",
      "Total Timesteps: 429800 Episode Num: 2149 Reward: -286.04395576578753\n",
      "timesteps not 0\n",
      "Total Timesteps: 430000 Episode Num: 2150 Reward: -2.0119699729742386\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -137.071728\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 430200 Episode Num: 2151 Reward: -222.1531179421826\n",
      "timesteps not 0\n",
      "Total Timesteps: 430400 Episode Num: 2152 Reward: -264.1523373436256\n",
      "timesteps not 0\n",
      "Total Timesteps: 430600 Episode Num: 2153 Reward: -299.64585932275156\n",
      "timesteps not 0\n",
      "Total Timesteps: 430800 Episode Num: 2154 Reward: -1.3507179655924983\n",
      "timesteps not 0\n",
      "Total Timesteps: 431000 Episode Num: 2155 Reward: -0.8759001354094109\n",
      "timesteps not 0\n",
      "Total Timesteps: 431200 Episode Num: 2156 Reward: -215.40865520328705\n",
      "timesteps not 0\n",
      "Total Timesteps: 431400 Episode Num: 2157 Reward: -117.17151526374798\n",
      "timesteps not 0\n",
      "Total Timesteps: 431600 Episode Num: 2158 Reward: -120.31663194450024\n",
      "timesteps not 0\n",
      "Total Timesteps: 431800 Episode Num: 2159 Reward: -118.5856480059301\n",
      "timesteps not 0\n",
      "Total Timesteps: 432000 Episode Num: 2160 Reward: -119.15663941112443\n",
      "timesteps not 0\n",
      "Total Timesteps: 432200 Episode Num: 2161 Reward: -120.48946275454708\n",
      "timesteps not 0\n",
      "Total Timesteps: 432400 Episode Num: 2162 Reward: -119.31531208326936\n",
      "timesteps not 0\n",
      "Total Timesteps: 432600 Episode Num: 2163 Reward: -123.49353480078393\n",
      "timesteps not 0\n",
      "Total Timesteps: 432800 Episode Num: 2164 Reward: -126.46390876436398\n",
      "timesteps not 0\n",
      "Total Timesteps: 433000 Episode Num: 2165 Reward: -231.8918654753619\n",
      "timesteps not 0\n",
      "Total Timesteps: 433200 Episode Num: 2166 Reward: -116.80669887029623\n",
      "timesteps not 0\n",
      "Total Timesteps: 433400 Episode Num: 2167 Reward: -126.4370839083001\n",
      "timesteps not 0\n",
      "Total Timesteps: 433600 Episode Num: 2168 Reward: -234.18843329768347\n",
      "timesteps not 0\n",
      "Total Timesteps: 433800 Episode Num: 2169 Reward: -127.38765130930899\n",
      "timesteps not 0\n",
      "Total Timesteps: 434000 Episode Num: 2170 Reward: -114.39896236167864\n",
      "timesteps not 0\n",
      "Total Timesteps: 434200 Episode Num: 2171 Reward: -228.1041578972424\n",
      "timesteps not 0\n",
      "Total Timesteps: 434400 Episode Num: 2172 Reward: -247.20932053188739\n",
      "timesteps not 0\n",
      "Total Timesteps: 434600 Episode Num: 2173 Reward: -122.94842550396196\n",
      "timesteps not 0\n",
      "Total Timesteps: 434800 Episode Num: 2174 Reward: -0.6705995515415093\n",
      "timesteps not 0\n",
      "Total Timesteps: 435000 Episode Num: 2175 Reward: -1.907678662401861\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -158.643960\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 435200 Episode Num: 2176 Reward: -119.02486470294834\n",
      "timesteps not 0\n",
      "Total Timesteps: 435400 Episode Num: 2177 Reward: -216.38454625093667\n",
      "timesteps not 0\n",
      "Total Timesteps: 435600 Episode Num: 2178 Reward: -127.40999251370643\n",
      "timesteps not 0\n",
      "Total Timesteps: 435800 Episode Num: 2179 Reward: -1.1106037937843583\n",
      "timesteps not 0\n",
      "Total Timesteps: 436000 Episode Num: 2180 Reward: -290.57061058108485\n",
      "timesteps not 0\n",
      "Total Timesteps: 436200 Episode Num: 2181 Reward: -320.585685310726\n",
      "timesteps not 0\n",
      "Total Timesteps: 436400 Episode Num: 2182 Reward: -228.0691499841451\n",
      "timesteps not 0\n",
      "Total Timesteps: 436600 Episode Num: 2183 Reward: -119.63028070154944\n",
      "timesteps not 0\n",
      "Total Timesteps: 436800 Episode Num: 2184 Reward: -118.93518959739546\n",
      "timesteps not 0\n",
      "Total Timesteps: 437000 Episode Num: 2185 Reward: -1.7210404666472345\n",
      "timesteps not 0\n",
      "Total Timesteps: 437200 Episode Num: 2186 Reward: -1.5319017824287398\n",
      "timesteps not 0\n",
      "Total Timesteps: 437400 Episode Num: 2187 Reward: -222.39049163173084\n",
      "timesteps not 0\n",
      "Total Timesteps: 437600 Episode Num: 2188 Reward: -122.24542196443923\n",
      "timesteps not 0\n",
      "Total Timesteps: 437800 Episode Num: 2189 Reward: -122.04953509340514\n",
      "timesteps not 0\n",
      "Total Timesteps: 438000 Episode Num: 2190 Reward: -116.38721531955606\n",
      "timesteps not 0\n",
      "Total Timesteps: 438200 Episode Num: 2191 Reward: -241.13211889259327\n",
      "timesteps not 0\n",
      "Total Timesteps: 438400 Episode Num: 2192 Reward: -227.85794347991342\n",
      "timesteps not 0\n",
      "Total Timesteps: 438600 Episode Num: 2193 Reward: -222.57776856621751\n",
      "timesteps not 0\n",
      "Total Timesteps: 438800 Episode Num: 2194 Reward: -118.34703345552812\n",
      "timesteps not 0\n",
      "Total Timesteps: 439000 Episode Num: 2195 Reward: -120.5076782143755\n",
      "timesteps not 0\n",
      "Total Timesteps: 439200 Episode Num: 2196 Reward: -1.0681009670820374\n",
      "timesteps not 0\n",
      "Total Timesteps: 439400 Episode Num: 2197 Reward: -2.102522711439991\n",
      "timesteps not 0\n",
      "Total Timesteps: 439600 Episode Num: 2198 Reward: -129.50994406985566\n",
      "timesteps not 0\n",
      "Total Timesteps: 439800 Episode Num: 2199 Reward: -116.51838733359598\n",
      "timesteps not 0\n",
      "Total Timesteps: 440000 Episode Num: 2200 Reward: -0.6135903587723208\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -96.226780\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 440200 Episode Num: 2201 Reward: -250.90937247219534\n",
      "timesteps not 0\n",
      "Total Timesteps: 440400 Episode Num: 2202 Reward: -0.8530381436979463\n",
      "timesteps not 0\n",
      "Total Timesteps: 440600 Episode Num: 2203 Reward: -129.17888844696355\n",
      "timesteps not 0\n",
      "Total Timesteps: 440800 Episode Num: 2204 Reward: -114.3912887605974\n",
      "timesteps not 0\n",
      "Total Timesteps: 441000 Episode Num: 2205 Reward: -1.1020344562809095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps not 0\n",
      "Total Timesteps: 441200 Episode Num: 2206 Reward: -118.44008466110382\n",
      "timesteps not 0\n",
      "Total Timesteps: 441400 Episode Num: 2207 Reward: -118.42992444222169\n",
      "timesteps not 0\n",
      "Total Timesteps: 441600 Episode Num: 2208 Reward: -117.88869144032668\n",
      "timesteps not 0\n",
      "Total Timesteps: 441800 Episode Num: 2209 Reward: -128.9932638026795\n",
      "timesteps not 0\n",
      "Total Timesteps: 442000 Episode Num: 2210 Reward: -116.88301874625921\n",
      "timesteps not 0\n",
      "Total Timesteps: 442200 Episode Num: 2211 Reward: -232.7548487256497\n",
      "timesteps not 0\n",
      "Total Timesteps: 442400 Episode Num: 2212 Reward: -120.33386332034078\n",
      "timesteps not 0\n",
      "Total Timesteps: 442600 Episode Num: 2213 Reward: -119.16638713039107\n",
      "timesteps not 0\n",
      "Total Timesteps: 442800 Episode Num: 2214 Reward: -121.01424608219727\n",
      "timesteps not 0\n",
      "Total Timesteps: 443000 Episode Num: 2215 Reward: -117.7532725843575\n",
      "timesteps not 0\n",
      "Total Timesteps: 443200 Episode Num: 2216 Reward: -116.28335907309628\n",
      "timesteps not 0\n",
      "Total Timesteps: 443400 Episode Num: 2217 Reward: -117.44188352702886\n",
      "timesteps not 0\n",
      "Total Timesteps: 443600 Episode Num: 2218 Reward: -114.93242471265184\n",
      "timesteps not 0\n",
      "Total Timesteps: 443800 Episode Num: 2219 Reward: -127.67518422965354\n",
      "timesteps not 0\n",
      "Total Timesteps: 444000 Episode Num: 2220 Reward: -128.38617111934823\n",
      "timesteps not 0\n",
      "Total Timesteps: 444200 Episode Num: 2221 Reward: -221.56525082348128\n",
      "timesteps not 0\n",
      "Total Timesteps: 444400 Episode Num: 2222 Reward: -128.7596164789037\n",
      "timesteps not 0\n",
      "Total Timesteps: 444600 Episode Num: 2223 Reward: -127.66153457022466\n",
      "timesteps not 0\n",
      "Total Timesteps: 444800 Episode Num: 2224 Reward: -117.70053027548855\n",
      "timesteps not 0\n",
      "Total Timesteps: 445000 Episode Num: 2225 Reward: -223.99970204963677\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -165.147728\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 445200 Episode Num: 2226 Reward: -128.31329746300233\n",
      "timesteps not 0\n",
      "Total Timesteps: 445400 Episode Num: 2227 Reward: -1.187757556391677\n",
      "timesteps not 0\n",
      "Total Timesteps: 445600 Episode Num: 2228 Reward: -229.90679799830943\n",
      "timesteps not 0\n",
      "Total Timesteps: 445800 Episode Num: 2229 Reward: -116.90975346676278\n",
      "timesteps not 0\n",
      "Total Timesteps: 446000 Episode Num: 2230 Reward: -125.73227630304159\n",
      "timesteps not 0\n",
      "Total Timesteps: 446200 Episode Num: 2231 Reward: -216.4707692150957\n",
      "timesteps not 0\n",
      "Total Timesteps: 446400 Episode Num: 2232 Reward: -1.1736848197302594\n",
      "timesteps not 0\n",
      "Total Timesteps: 446600 Episode Num: 2233 Reward: -120.02290705447578\n",
      "timesteps not 0\n",
      "Total Timesteps: 446800 Episode Num: 2234 Reward: -2.42840984185888\n",
      "timesteps not 0\n",
      "Total Timesteps: 447000 Episode Num: 2235 Reward: -125.99958712762493\n",
      "timesteps not 0\n",
      "Total Timesteps: 447200 Episode Num: 2236 Reward: -119.24817331021842\n",
      "timesteps not 0\n",
      "Total Timesteps: 447400 Episode Num: 2237 Reward: -284.62586787219976\n",
      "timesteps not 0\n",
      "Total Timesteps: 447600 Episode Num: 2238 Reward: -126.16987069412868\n",
      "timesteps not 0\n",
      "Total Timesteps: 447800 Episode Num: 2239 Reward: -0.816876529839484\n",
      "timesteps not 0\n",
      "Total Timesteps: 448000 Episode Num: 2240 Reward: -226.49323802554468\n",
      "timesteps not 0\n",
      "Total Timesteps: 448200 Episode Num: 2241 Reward: -121.7131406816329\n",
      "timesteps not 0\n",
      "Total Timesteps: 448400 Episode Num: 2242 Reward: -225.9241982547445\n",
      "timesteps not 0\n",
      "Total Timesteps: 448600 Episode Num: 2243 Reward: -116.63519675591793\n",
      "timesteps not 0\n",
      "Total Timesteps: 448800 Episode Num: 2244 Reward: -232.90064451419818\n",
      "timesteps not 0\n",
      "Total Timesteps: 449000 Episode Num: 2245 Reward: -120.34170836740431\n",
      "timesteps not 0\n",
      "Total Timesteps: 449200 Episode Num: 2246 Reward: -230.9315998247859\n",
      "timesteps not 0\n",
      "Total Timesteps: 449400 Episode Num: 2247 Reward: -230.2690139940259\n",
      "timesteps not 0\n",
      "Total Timesteps: 449600 Episode Num: 2248 Reward: -117.91283367585687\n",
      "timesteps not 0\n",
      "Total Timesteps: 449800 Episode Num: 2249 Reward: -124.4613667420984\n",
      "timesteps not 0\n",
      "Total Timesteps: 450000 Episode Num: 2250 Reward: -326.9110480598739\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -148.091736\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 450200 Episode Num: 2251 Reward: -224.24268608080445\n",
      "timesteps not 0\n",
      "Total Timesteps: 450400 Episode Num: 2252 Reward: -230.7566590521099\n",
      "timesteps not 0\n",
      "Total Timesteps: 450600 Episode Num: 2253 Reward: -243.5295216952719\n",
      "timesteps not 0\n",
      "Total Timesteps: 450800 Episode Num: 2254 Reward: -126.3825467386611\n",
      "timesteps not 0\n",
      "Total Timesteps: 451000 Episode Num: 2255 Reward: -229.66204253104522\n",
      "timesteps not 0\n",
      "Total Timesteps: 451200 Episode Num: 2256 Reward: -119.30540922612269\n",
      "timesteps not 0\n",
      "Total Timesteps: 451400 Episode Num: 2257 Reward: -127.13244639023603\n",
      "timesteps not 0\n",
      "Total Timesteps: 451600 Episode Num: 2258 Reward: -1.862933474441897\n",
      "timesteps not 0\n",
      "Total Timesteps: 451800 Episode Num: 2259 Reward: -226.59634920726123\n",
      "timesteps not 0\n",
      "Total Timesteps: 452000 Episode Num: 2260 Reward: -119.98877828101308\n",
      "timesteps not 0\n",
      "Total Timesteps: 452200 Episode Num: 2261 Reward: -115.15596411663255\n",
      "timesteps not 0\n",
      "Total Timesteps: 452400 Episode Num: 2262 Reward: -116.30860142717533\n",
      "timesteps not 0\n",
      "Total Timesteps: 452600 Episode Num: 2263 Reward: -118.73189755441456\n",
      "timesteps not 0\n",
      "Total Timesteps: 452800 Episode Num: 2264 Reward: -238.86473360685085\n",
      "timesteps not 0\n",
      "Total Timesteps: 453000 Episode Num: 2265 Reward: -231.32992881858704\n",
      "timesteps not 0\n",
      "Total Timesteps: 453200 Episode Num: 2266 Reward: -5.167795641946171\n",
      "timesteps not 0\n",
      "Total Timesteps: 453400 Episode Num: 2267 Reward: -116.41642379046147\n",
      "timesteps not 0\n",
      "Total Timesteps: 453600 Episode Num: 2268 Reward: -1.9666435245503713\n",
      "timesteps not 0\n",
      "Total Timesteps: 453800 Episode Num: 2269 Reward: -235.55134553674776\n",
      "timesteps not 0\n",
      "Total Timesteps: 454000 Episode Num: 2270 Reward: -241.781205519632\n",
      "timesteps not 0\n",
      "Total Timesteps: 454200 Episode Num: 2271 Reward: -125.04278634531202\n",
      "timesteps not 0\n",
      "Total Timesteps: 454400 Episode Num: 2272 Reward: -115.19902655372566\n",
      "timesteps not 0\n",
      "Total Timesteps: 454600 Episode Num: 2273 Reward: -227.57161165897128\n",
      "timesteps not 0\n",
      "Total Timesteps: 454800 Episode Num: 2274 Reward: -117.95258683376201\n",
      "timesteps not 0\n",
      "Total Timesteps: 455000 Episode Num: 2275 Reward: -128.72823879512492\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -169.597695\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 455200 Episode Num: 2276 Reward: -120.20833992830507\n",
      "timesteps not 0\n",
      "Total Timesteps: 455400 Episode Num: 2277 Reward: -120.19483703048493\n",
      "timesteps not 0\n",
      "Total Timesteps: 455600 Episode Num: 2278 Reward: -117.5973375510928\n",
      "timesteps not 0\n",
      "Total Timesteps: 455800 Episode Num: 2279 Reward: -1.8106717088383661\n",
      "timesteps not 0\n",
      "Total Timesteps: 456000 Episode Num: 2280 Reward: -233.84344898751425\n",
      "timesteps not 0\n",
      "Total Timesteps: 456200 Episode Num: 2281 Reward: -126.20703954024461\n",
      "timesteps not 0\n",
      "Total Timesteps: 456400 Episode Num: 2282 Reward: -117.43367142077243\n",
      "timesteps not 0\n",
      "Total Timesteps: 456600 Episode Num: 2283 Reward: -0.8481879117018205\n",
      "timesteps not 0\n",
      "Total Timesteps: 456800 Episode Num: 2284 Reward: -128.37158815480007\n",
      "timesteps not 0\n",
      "Total Timesteps: 457000 Episode Num: 2285 Reward: -126.76838431558542\n",
      "timesteps not 0\n",
      "Total Timesteps: 457200 Episode Num: 2286 Reward: -114.77475432733955\n",
      "timesteps not 0\n",
      "Total Timesteps: 457400 Episode Num: 2287 Reward: -223.1878291465454\n",
      "timesteps not 0\n",
      "Total Timesteps: 457600 Episode Num: 2288 Reward: -116.71853812967802\n",
      "timesteps not 0\n",
      "Total Timesteps: 457800 Episode Num: 2289 Reward: -116.41238082010126\n",
      "timesteps not 0\n",
      "Total Timesteps: 458000 Episode Num: 2290 Reward: -116.23383202286561\n",
      "timesteps not 0\n",
      "Total Timesteps: 458200 Episode Num: 2291 Reward: -0.7161450226015291\n",
      "timesteps not 0\n",
      "Total Timesteps: 458400 Episode Num: 2292 Reward: -226.12596441196783\n",
      "timesteps not 0\n",
      "Total Timesteps: 458600 Episode Num: 2293 Reward: -117.94073137003757\n",
      "timesteps not 0\n",
      "Total Timesteps: 458800 Episode Num: 2294 Reward: -119.9154019729256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps not 0\n",
      "Total Timesteps: 459000 Episode Num: 2295 Reward: -218.27805151763647\n",
      "timesteps not 0\n",
      "Total Timesteps: 459200 Episode Num: 2296 Reward: -0.5657590341407389\n",
      "timesteps not 0\n",
      "Total Timesteps: 459400 Episode Num: 2297 Reward: -116.30437457844826\n",
      "timesteps not 0\n",
      "Total Timesteps: 459600 Episode Num: 2298 Reward: -124.83563100979528\n",
      "timesteps not 0\n",
      "Total Timesteps: 459800 Episode Num: 2299 Reward: -120.69715745845703\n",
      "timesteps not 0\n",
      "Total Timesteps: 460000 Episode Num: 2300 Reward: -115.75043354534083\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -136.351860\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 460200 Episode Num: 2301 Reward: -219.96269496861888\n",
      "timesteps not 0\n",
      "Total Timesteps: 460400 Episode Num: 2302 Reward: -118.28017404168683\n",
      "timesteps not 0\n",
      "Total Timesteps: 460600 Episode Num: 2303 Reward: -118.79041055978333\n",
      "timesteps not 0\n",
      "Total Timesteps: 460800 Episode Num: 2304 Reward: -296.0948961892591\n",
      "timesteps not 0\n",
      "Total Timesteps: 461000 Episode Num: 2305 Reward: -120.18252595263569\n",
      "timesteps not 0\n",
      "Total Timesteps: 461200 Episode Num: 2306 Reward: -301.8932164289718\n",
      "timesteps not 0\n",
      "Total Timesteps: 461400 Episode Num: 2307 Reward: -116.77030831421975\n",
      "timesteps not 0\n",
      "Total Timesteps: 461600 Episode Num: 2308 Reward: -117.02365588563286\n",
      "timesteps not 0\n",
      "Total Timesteps: 461800 Episode Num: 2309 Reward: -116.62012737892294\n",
      "timesteps not 0\n",
      "Total Timesteps: 462000 Episode Num: 2310 Reward: -115.51204745064702\n",
      "timesteps not 0\n",
      "Total Timesteps: 462200 Episode Num: 2311 Reward: -219.60029895798567\n",
      "timesteps not 0\n",
      "Total Timesteps: 462400 Episode Num: 2312 Reward: -125.5528984660175\n",
      "timesteps not 0\n",
      "Total Timesteps: 462600 Episode Num: 2313 Reward: -114.75333344667074\n",
      "timesteps not 0\n",
      "Total Timesteps: 462800 Episode Num: 2314 Reward: -119.79107839929702\n",
      "timesteps not 0\n",
      "Total Timesteps: 463000 Episode Num: 2315 Reward: -231.9671547187778\n",
      "timesteps not 0\n",
      "Total Timesteps: 463200 Episode Num: 2316 Reward: -115.95083913533186\n",
      "timesteps not 0\n",
      "Total Timesteps: 463400 Episode Num: 2317 Reward: -120.43498241893408\n",
      "timesteps not 0\n",
      "Total Timesteps: 463600 Episode Num: 2318 Reward: -1.3222498782034533\n",
      "timesteps not 0\n",
      "Total Timesteps: 463800 Episode Num: 2319 Reward: -1.5779149263556045\n",
      "timesteps not 0\n",
      "Total Timesteps: 464000 Episode Num: 2320 Reward: -127.02771674899306\n",
      "timesteps not 0\n",
      "Total Timesteps: 464200 Episode Num: 2321 Reward: -1.0706285252485366\n",
      "timesteps not 0\n",
      "Total Timesteps: 464400 Episode Num: 2322 Reward: -0.5964105145284958\n",
      "timesteps not 0\n",
      "Total Timesteps: 464600 Episode Num: 2323 Reward: -119.00035500344899\n",
      "timesteps not 0\n",
      "Total Timesteps: 464800 Episode Num: 2324 Reward: -122.24095002628843\n",
      "timesteps not 0\n",
      "Total Timesteps: 465000 Episode Num: 2325 Reward: -118.93635491124303\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -159.902528\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 465200 Episode Num: 2326 Reward: -125.77154366032643\n",
      "timesteps not 0\n",
      "Total Timesteps: 465400 Episode Num: 2327 Reward: -126.86031101156227\n",
      "timesteps not 0\n",
      "Total Timesteps: 465600 Episode Num: 2328 Reward: -233.80305688648534\n",
      "timesteps not 0\n",
      "Total Timesteps: 465800 Episode Num: 2329 Reward: -120.5329720859798\n",
      "timesteps not 0\n",
      "Total Timesteps: 466000 Episode Num: 2330 Reward: -114.73140358313823\n",
      "timesteps not 0\n",
      "Total Timesteps: 466200 Episode Num: 2331 Reward: -119.37505835440533\n",
      "timesteps not 0\n",
      "Total Timesteps: 466400 Episode Num: 2332 Reward: -124.63044445236059\n",
      "timesteps not 0\n",
      "Total Timesteps: 466600 Episode Num: 2333 Reward: -245.25149407899772\n",
      "timesteps not 0\n",
      "Total Timesteps: 466800 Episode Num: 2334 Reward: -5.008507009421074\n",
      "timesteps not 0\n",
      "Total Timesteps: 467000 Episode Num: 2335 Reward: -121.80639129480961\n",
      "timesteps not 0\n",
      "Total Timesteps: 467200 Episode Num: 2336 Reward: -118.33957042979134\n",
      "timesteps not 0\n",
      "Total Timesteps: 467400 Episode Num: 2337 Reward: -1.8817566111642672\n",
      "timesteps not 0\n",
      "Total Timesteps: 467600 Episode Num: 2338 Reward: -120.54230235019881\n",
      "timesteps not 0\n",
      "Total Timesteps: 467800 Episode Num: 2339 Reward: -122.34753239798033\n",
      "timesteps not 0\n",
      "Total Timesteps: 468000 Episode Num: 2340 Reward: -0.4902287721845094\n",
      "timesteps not 0\n",
      "Total Timesteps: 468200 Episode Num: 2341 Reward: -125.10457326782246\n",
      "timesteps not 0\n",
      "Total Timesteps: 468400 Episode Num: 2342 Reward: -232.86983186553792\n",
      "timesteps not 0\n",
      "Total Timesteps: 468600 Episode Num: 2343 Reward: -0.47859940441137383\n",
      "timesteps not 0\n",
      "Total Timesteps: 468800 Episode Num: 2344 Reward: -221.91739733959346\n",
      "timesteps not 0\n",
      "Total Timesteps: 469000 Episode Num: 2345 Reward: -224.97163933699233\n",
      "timesteps not 0\n",
      "Total Timesteps: 469200 Episode Num: 2346 Reward: -223.79729913000787\n",
      "timesteps not 0\n",
      "Total Timesteps: 469400 Episode Num: 2347 Reward: -1.8061150591334485\n",
      "timesteps not 0\n",
      "Total Timesteps: 469600 Episode Num: 2348 Reward: -222.5354391077776\n",
      "timesteps not 0\n",
      "Total Timesteps: 469800 Episode Num: 2349 Reward: -219.50515233417812\n",
      "timesteps not 0\n",
      "Total Timesteps: 470000 Episode Num: 2350 Reward: -119.32394405401894\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -126.056093\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 470200 Episode Num: 2351 Reward: -219.1612558337797\n",
      "timesteps not 0\n",
      "Total Timesteps: 470400 Episode Num: 2352 Reward: -124.5653556866499\n",
      "timesteps not 0\n",
      "Total Timesteps: 470600 Episode Num: 2353 Reward: -325.0895851056106\n",
      "timesteps not 0\n",
      "Total Timesteps: 470800 Episode Num: 2354 Reward: -119.45620459845871\n",
      "timesteps not 0\n",
      "Total Timesteps: 471000 Episode Num: 2355 Reward: -226.50292536623854\n",
      "timesteps not 0\n",
      "Total Timesteps: 471200 Episode Num: 2356 Reward: -337.66985989196843\n",
      "timesteps not 0\n",
      "Total Timesteps: 471400 Episode Num: 2357 Reward: -0.9932420080852348\n",
      "timesteps not 0\n",
      "Total Timesteps: 471600 Episode Num: 2358 Reward: -117.23292399935406\n",
      "timesteps not 0\n",
      "Total Timesteps: 471800 Episode Num: 2359 Reward: -116.42823806464943\n",
      "timesteps not 0\n",
      "Total Timesteps: 472000 Episode Num: 2360 Reward: -368.1744420788399\n",
      "timesteps not 0\n",
      "Total Timesteps: 472200 Episode Num: 2361 Reward: -231.12589029771348\n",
      "timesteps not 0\n",
      "Total Timesteps: 472400 Episode Num: 2362 Reward: -219.53452213519063\n",
      "timesteps not 0\n",
      "Total Timesteps: 472600 Episode Num: 2363 Reward: -308.37457426701695\n",
      "timesteps not 0\n",
      "Total Timesteps: 472800 Episode Num: 2364 Reward: -121.80492236160688\n",
      "timesteps not 0\n",
      "Total Timesteps: 473000 Episode Num: 2365 Reward: -118.94661597802784\n",
      "timesteps not 0\n",
      "Total Timesteps: 473200 Episode Num: 2366 Reward: -121.7516454416184\n",
      "timesteps not 0\n",
      "Total Timesteps: 473400 Episode Num: 2367 Reward: -126.1620579964191\n",
      "timesteps not 0\n",
      "Total Timesteps: 473600 Episode Num: 2368 Reward: -128.82385531975314\n",
      "timesteps not 0\n",
      "Total Timesteps: 473800 Episode Num: 2369 Reward: -253.5822028268844\n",
      "timesteps not 0\n",
      "Total Timesteps: 474000 Episode Num: 2370 Reward: -239.08916600727432\n",
      "timesteps not 0\n",
      "Total Timesteps: 474200 Episode Num: 2371 Reward: -116.29530090235839\n",
      "timesteps not 0\n",
      "Total Timesteps: 474400 Episode Num: 2372 Reward: -233.403390528937\n",
      "timesteps not 0\n",
      "Total Timesteps: 474600 Episode Num: 2373 Reward: -118.22783568133238\n",
      "timesteps not 0\n",
      "Total Timesteps: 474800 Episode Num: 2374 Reward: -126.72329056711105\n",
      "timesteps not 0\n",
      "Total Timesteps: 475000 Episode Num: 2375 Reward: -229.46762979222058\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -117.703818\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 475200 Episode Num: 2376 Reward: -247.7920432901817\n",
      "timesteps not 0\n",
      "Total Timesteps: 475400 Episode Num: 2377 Reward: -246.0958852532427\n",
      "timesteps not 0\n",
      "Total Timesteps: 475600 Episode Num: 2378 Reward: -118.9433251277944\n",
      "timesteps not 0\n",
      "Total Timesteps: 475800 Episode Num: 2379 Reward: -282.4825986770687\n",
      "timesteps not 0\n",
      "Total Timesteps: 476000 Episode Num: 2380 Reward: -251.65773507943476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps not 0\n",
      "Total Timesteps: 476200 Episode Num: 2381 Reward: -0.9208141125072994\n",
      "timesteps not 0\n",
      "Total Timesteps: 476400 Episode Num: 2382 Reward: -286.51135040902847\n",
      "timesteps not 0\n",
      "Total Timesteps: 476600 Episode Num: 2383 Reward: -2.872597810998436\n",
      "timesteps not 0\n",
      "Total Timesteps: 476800 Episode Num: 2384 Reward: -124.4811608499131\n",
      "timesteps not 0\n",
      "Total Timesteps: 477000 Episode Num: 2385 Reward: -218.89281785723358\n",
      "timesteps not 0\n",
      "Total Timesteps: 477200 Episode Num: 2386 Reward: -115.29030178192234\n",
      "timesteps not 0\n",
      "Total Timesteps: 477400 Episode Num: 2387 Reward: -124.12508618164411\n",
      "timesteps not 0\n",
      "Total Timesteps: 477600 Episode Num: 2388 Reward: -120.26325729437791\n",
      "timesteps not 0\n",
      "Total Timesteps: 477800 Episode Num: 2389 Reward: -118.74634311045406\n",
      "timesteps not 0\n",
      "Total Timesteps: 478000 Episode Num: 2390 Reward: -125.72777895008143\n",
      "timesteps not 0\n",
      "Total Timesteps: 478200 Episode Num: 2391 Reward: -309.78283427656146\n",
      "timesteps not 0\n",
      "Total Timesteps: 478400 Episode Num: 2392 Reward: -115.60822745440208\n",
      "timesteps not 0\n",
      "Total Timesteps: 478600 Episode Num: 2393 Reward: -228.99011804842408\n",
      "timesteps not 0\n",
      "Total Timesteps: 478800 Episode Num: 2394 Reward: -287.20409527756505\n",
      "timesteps not 0\n",
      "Total Timesteps: 479000 Episode Num: 2395 Reward: -125.57510291690228\n",
      "timesteps not 0\n",
      "Total Timesteps: 479200 Episode Num: 2396 Reward: -1.4533256524710705\n",
      "timesteps not 0\n",
      "Total Timesteps: 479400 Episode Num: 2397 Reward: -247.68619482201876\n",
      "timesteps not 0\n",
      "Total Timesteps: 479600 Episode Num: 2398 Reward: -225.39920773995723\n",
      "timesteps not 0\n",
      "Total Timesteps: 479800 Episode Num: 2399 Reward: -224.7100750121089\n",
      "timesteps not 0\n",
      "Total Timesteps: 480000 Episode Num: 2400 Reward: -1.0286644418894608\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -162.786560\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 480200 Episode Num: 2401 Reward: -225.86215970843452\n",
      "timesteps not 0\n",
      "Total Timesteps: 480400 Episode Num: 2402 Reward: -117.56928688433135\n",
      "timesteps not 0\n",
      "Total Timesteps: 480600 Episode Num: 2403 Reward: -121.96705344917632\n",
      "timesteps not 0\n",
      "Total Timesteps: 480800 Episode Num: 2404 Reward: -259.51773195783795\n",
      "timesteps not 0\n",
      "Total Timesteps: 481000 Episode Num: 2405 Reward: -0.5900431645091071\n",
      "timesteps not 0\n",
      "Total Timesteps: 481200 Episode Num: 2406 Reward: -261.42683497086267\n",
      "timesteps not 0\n",
      "Total Timesteps: 481400 Episode Num: 2407 Reward: -0.9666514431992577\n",
      "timesteps not 0\n",
      "Total Timesteps: 481600 Episode Num: 2408 Reward: -126.0828745715543\n",
      "timesteps not 0\n",
      "Total Timesteps: 481800 Episode Num: 2409 Reward: -0.9763407735732323\n",
      "timesteps not 0\n",
      "Total Timesteps: 482000 Episode Num: 2410 Reward: -237.98637647974905\n",
      "timesteps not 0\n",
      "Total Timesteps: 482200 Episode Num: 2411 Reward: -120.8039458895533\n",
      "timesteps not 0\n",
      "Total Timesteps: 482400 Episode Num: 2412 Reward: -119.54839791963225\n",
      "timesteps not 0\n",
      "Total Timesteps: 482600 Episode Num: 2413 Reward: -120.63961014688884\n",
      "timesteps not 0\n",
      "Total Timesteps: 482800 Episode Num: 2414 Reward: -0.494164905364644\n",
      "timesteps not 0\n",
      "Total Timesteps: 483000 Episode Num: 2415 Reward: -117.82024082129497\n",
      "timesteps not 0\n",
      "Total Timesteps: 483200 Episode Num: 2416 Reward: -345.49969421107477\n",
      "timesteps not 0\n",
      "Total Timesteps: 483400 Episode Num: 2417 Reward: -225.22845629861095\n",
      "timesteps not 0\n",
      "Total Timesteps: 483600 Episode Num: 2418 Reward: -120.75241891038019\n",
      "timesteps not 0\n",
      "Total Timesteps: 483800 Episode Num: 2419 Reward: -115.73124706477974\n",
      "timesteps not 0\n",
      "Total Timesteps: 484000 Episode Num: 2420 Reward: -127.16977014366593\n",
      "timesteps not 0\n",
      "Total Timesteps: 484200 Episode Num: 2421 Reward: -233.74774091378526\n",
      "timesteps not 0\n",
      "Total Timesteps: 484400 Episode Num: 2422 Reward: -226.8984282038539\n",
      "timesteps not 0\n",
      "Total Timesteps: 484600 Episode Num: 2423 Reward: -114.0118296468093\n",
      "timesteps not 0\n",
      "Total Timesteps: 484800 Episode Num: 2424 Reward: -118.83810285268714\n",
      "timesteps not 0\n",
      "Total Timesteps: 485000 Episode Num: 2425 Reward: -116.33675348650108\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -130.278407\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 485200 Episode Num: 2426 Reward: -128.5628648553615\n",
      "timesteps not 0\n",
      "Total Timesteps: 485400 Episode Num: 2427 Reward: -223.15731776331717\n",
      "timesteps not 0\n",
      "Total Timesteps: 485600 Episode Num: 2428 Reward: -291.00617756421724\n",
      "timesteps not 0\n",
      "Total Timesteps: 485800 Episode Num: 2429 Reward: -219.17217336418722\n",
      "timesteps not 0\n",
      "Total Timesteps: 486000 Episode Num: 2430 Reward: -120.60916877901133\n",
      "timesteps not 0\n",
      "Total Timesteps: 486200 Episode Num: 2431 Reward: -226.69805706186133\n",
      "timesteps not 0\n",
      "Total Timesteps: 486400 Episode Num: 2432 Reward: -118.91278805553681\n",
      "timesteps not 0\n",
      "Total Timesteps: 486600 Episode Num: 2433 Reward: -0.9896476630682554\n",
      "timesteps not 0\n",
      "Total Timesteps: 486800 Episode Num: 2434 Reward: -0.5894438899259837\n",
      "timesteps not 0\n",
      "Total Timesteps: 487000 Episode Num: 2435 Reward: -127.3472802569105\n",
      "timesteps not 0\n",
      "Total Timesteps: 487200 Episode Num: 2436 Reward: -118.92115766338435\n",
      "timesteps not 0\n",
      "Total Timesteps: 487400 Episode Num: 2437 Reward: -117.01522889315925\n",
      "timesteps not 0\n",
      "Total Timesteps: 487600 Episode Num: 2438 Reward: -219.68468422828565\n",
      "timesteps not 0\n",
      "Total Timesteps: 487800 Episode Num: 2439 Reward: -230.8903913908917\n",
      "timesteps not 0\n",
      "Total Timesteps: 488000 Episode Num: 2440 Reward: -120.77753074678611\n",
      "timesteps not 0\n",
      "Total Timesteps: 488200 Episode Num: 2441 Reward: -120.13201457195068\n",
      "timesteps not 0\n",
      "Total Timesteps: 488400 Episode Num: 2442 Reward: -0.6901708847659306\n",
      "timesteps not 0\n",
      "Total Timesteps: 488600 Episode Num: 2443 Reward: -120.5995779951845\n",
      "timesteps not 0\n",
      "Total Timesteps: 488800 Episode Num: 2444 Reward: -127.82504089865981\n",
      "timesteps not 0\n",
      "Total Timesteps: 489000 Episode Num: 2445 Reward: -219.787712481139\n",
      "timesteps not 0\n",
      "Total Timesteps: 489200 Episode Num: 2446 Reward: -216.62953123669038\n",
      "timesteps not 0\n",
      "Total Timesteps: 489400 Episode Num: 2447 Reward: -123.38100248613084\n",
      "timesteps not 0\n",
      "Total Timesteps: 489600 Episode Num: 2448 Reward: -118.2607443596338\n",
      "timesteps not 0\n",
      "Total Timesteps: 489800 Episode Num: 2449 Reward: -222.5780625303053\n",
      "timesteps not 0\n",
      "Total Timesteps: 490000 Episode Num: 2450 Reward: -120.98167247210067\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -130.930068\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 490200 Episode Num: 2451 Reward: -242.44793752801965\n",
      "timesteps not 0\n",
      "Total Timesteps: 490400 Episode Num: 2452 Reward: -121.52001218511944\n",
      "timesteps not 0\n",
      "Total Timesteps: 490600 Episode Num: 2453 Reward: -225.4832196411777\n",
      "timesteps not 0\n",
      "Total Timesteps: 490800 Episode Num: 2454 Reward: -117.30693034092836\n",
      "timesteps not 0\n",
      "Total Timesteps: 491000 Episode Num: 2455 Reward: -367.26108852943975\n",
      "timesteps not 0\n",
      "Total Timesteps: 491200 Episode Num: 2456 Reward: -125.81553222182349\n",
      "timesteps not 0\n",
      "Total Timesteps: 491400 Episode Num: 2457 Reward: -1.0299095759026409\n",
      "timesteps not 0\n",
      "Total Timesteps: 491600 Episode Num: 2458 Reward: -1.202931862619146\n",
      "timesteps not 0\n",
      "Total Timesteps: 491800 Episode Num: 2459 Reward: -2.475914557588461\n",
      "timesteps not 0\n",
      "Total Timesteps: 492000 Episode Num: 2460 Reward: -114.83592932801797\n",
      "timesteps not 0\n",
      "Total Timesteps: 492200 Episode Num: 2461 Reward: -118.7670382620576\n",
      "timesteps not 0\n",
      "Total Timesteps: 492400 Episode Num: 2462 Reward: -124.03673854844723\n",
      "timesteps not 0\n",
      "Total Timesteps: 492600 Episode Num: 2463 Reward: -218.07029603013066\n",
      "timesteps not 0\n",
      "Total Timesteps: 492800 Episode Num: 2464 Reward: -375.33595875031375\n",
      "timesteps not 0\n",
      "Total Timesteps: 493000 Episode Num: 2465 Reward: -118.37297336389874\n",
      "timesteps not 0\n",
      "Total Timesteps: 493200 Episode Num: 2466 Reward: -227.34388419162624\n",
      "timesteps not 0\n",
      "Total Timesteps: 493400 Episode Num: 2467 Reward: -120.37262861223554\n",
      "timesteps not 0\n",
      "Total Timesteps: 493600 Episode Num: 2468 Reward: -218.34703880120932\n",
      "timesteps not 0\n",
      "Total Timesteps: 493800 Episode Num: 2469 Reward: -126.76253828430477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps not 0\n",
      "Total Timesteps: 494000 Episode Num: 2470 Reward: -116.50466556628673\n",
      "timesteps not 0\n",
      "Total Timesteps: 494200 Episode Num: 2471 Reward: -127.83694390691694\n",
      "timesteps not 0\n",
      "Total Timesteps: 494400 Episode Num: 2472 Reward: -246.52088704025604\n",
      "timesteps not 0\n",
      "Total Timesteps: 494600 Episode Num: 2473 Reward: -322.0994792660175\n",
      "timesteps not 0\n",
      "Total Timesteps: 494800 Episode Num: 2474 Reward: -117.0674328161748\n",
      "timesteps not 0\n",
      "Total Timesteps: 495000 Episode Num: 2475 Reward: -220.37543834603892\n",
      "timesteps since eval more than eval freq\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -148.130912\n",
      "--------------------------------------------------------\n",
      "timesteps not 0\n",
      "Total Timesteps: 495200 Episode Num: 2476 Reward: -330.25806819020846\n",
      "timesteps not 0\n",
      "Total Timesteps: 495400 Episode Num: 2477 Reward: -1.003443458863845\n",
      "timesteps not 0\n",
      "Total Timesteps: 495600 Episode Num: 2478 Reward: -222.73606342115053\n",
      "timesteps not 0\n",
      "Total Timesteps: 495800 Episode Num: 2479 Reward: -126.7456561133719\n",
      "timesteps not 0\n",
      "Total Timesteps: 496000 Episode Num: 2480 Reward: -119.80598026924316\n",
      "timesteps not 0\n",
      "Total Timesteps: 496200 Episode Num: 2481 Reward: -126.30201550607588\n",
      "timesteps not 0\n",
      "Total Timesteps: 496400 Episode Num: 2482 Reward: -364.0713198075014\n",
      "timesteps not 0\n",
      "Total Timesteps: 496600 Episode Num: 2483 Reward: -124.88737743765785\n",
      "timesteps not 0\n",
      "Total Timesteps: 496800 Episode Num: 2484 Reward: -309.0710370209503\n",
      "timesteps not 0\n",
      "Total Timesteps: 497000 Episode Num: 2485 Reward: -0.785587682852273\n",
      "timesteps not 0\n",
      "Total Timesteps: 497200 Episode Num: 2486 Reward: -2.913770314835633\n",
      "timesteps not 0\n",
      "Total Timesteps: 497400 Episode Num: 2487 Reward: -119.41845657384901\n",
      "timesteps not 0\n",
      "Total Timesteps: 497600 Episode Num: 2488 Reward: -118.645090646609\n",
      "timesteps not 0\n",
      "Total Timesteps: 497800 Episode Num: 2489 Reward: -117.54688316607918\n",
      "timesteps not 0\n",
      "Total Timesteps: 498000 Episode Num: 2490 Reward: -126.18720563730841\n",
      "timesteps not 0\n",
      "Total Timesteps: 498200 Episode Num: 2491 Reward: -0.46275079097912125\n",
      "timesteps not 0\n",
      "Total Timesteps: 498400 Episode Num: 2492 Reward: -130.08851798055585\n",
      "timesteps not 0\n",
      "Total Timesteps: 498600 Episode Num: 2493 Reward: -226.8262453367044\n",
      "timesteps not 0\n",
      "Total Timesteps: 498800 Episode Num: 2494 Reward: -117.20108773303197\n",
      "timesteps not 0\n",
      "Total Timesteps: 499000 Episode Num: 2495 Reward: -2.6568538659149534\n",
      "timesteps not 0\n",
      "Total Timesteps: 499200 Episode Num: 2496 Reward: -122.98279084127289\n",
      "timesteps not 0\n",
      "Total Timesteps: 499400 Episode Num: 2497 Reward: -240.66442725903093\n",
      "timesteps not 0\n",
      "Total Timesteps: 499600 Episode Num: 2498 Reward: -114.89307940531778\n",
      "timesteps not 0\n",
      "Total Timesteps: 499800 Episode Num: 2499 Reward: -127.23117706568421\n",
      "--------------------------------------------------------\n",
      "Average Reward over the Evaluation Step: -115.965469\n",
      "--------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# to start the training, we first need to define the number of timesteps we want to run it\n",
    "while total_timesteps < max_timesteps:\n",
    "    # if the episode is done\n",
    "    if done:\n",
    "        # if we have performed some steps and got done, then let's complete the evaluation and then save the model\n",
    "        # after that we reset the variables and start training again\n",
    "        if total_timesteps != 0:\n",
    "            print('timesteps not 0')\n",
    "            print(\"Total Timesteps: {} Episode Num: {} Reward: {}\".format(total_timesteps, episode_num, episode_reward))\n",
    "            policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
    "            \n",
    "        # here's where we evaluate the episode and save the policy\n",
    "        if timesteps_since_eval >= eval_freq:\n",
    "            print('timesteps since eval more than eval freq')\n",
    "            timesteps_since_eval %= eval_freq\n",
    "            evaluations.append(evaluate_policy(policy))\n",
    "            policy.save(file_name,directory='./pytorch_models')\n",
    "            np.save('./results/%s' % (file_name), evaluations)\n",
    "            \n",
    "        # when the training step is done, we reset the state of the environment\n",
    "        obs = env.reset()\n",
    "        \n",
    "        # Set the done to false again\n",
    "        done = False\n",
    "        \n",
    "        # set teh rewards and episode timesteps to zero\n",
    "        episode_reward = 0\n",
    "        episode_timesteps = 0\n",
    "        episode_num += 1\n",
    "    # to get some initial data, we will run random actions for the number of timesteps defined in the start_timesteps variable\n",
    "    if total_timesteps < start_timesteps:\n",
    "        action = env.action_space.sample()\n",
    "    else: # meaning after we reach the number of timesteps defined in the start_timesteps variable\n",
    "        # get a proper action from the select_action function by passing obs variable for state\n",
    "        action = policy.select_action(np.array(obs))\n",
    "        # if the explore_noise parameter is not 0, we add noise to the action and we clip it\n",
    "        if expl_noise != 0:\n",
    "            action = (action + np.random.normal(0, expl_noise, size=env.action_space.shape[0])).clip(env.action_space.low, env.action_space.high)\n",
    "        \n",
    "    # lets send the new action we have decided to go with and send that to the environment and get the next state and the reward\n",
    "    new_obs, reward, done, _ = env.step(action)\n",
    "\n",
    "    # check if the episode is done\n",
    "    done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)\n",
    "\n",
    "    # let's add the new reward to the total episode reward\n",
    "    episode_reward += reward\n",
    "\n",
    "    # the following command will store the new transition to the the experience replay memory (i.e. the replaybuffer)\n",
    "    replay_buffer.add((obs, new_obs, action, reward, done_bool))\n",
    "\n",
    "    # now we update the state, the episode timestep, the total timesteps and the timesteps since the evaluation of the policy\n",
    "    obs = new_obs\n",
    "    episode_timesteps += 1\n",
    "    total_timesteps += 1\n",
    "    timesteps_since_eval += 1\n",
    "        \n",
    "# We add the last policy evaluation to our list of evaluations and we save our model\n",
    "evaluations.append(evaluate_policy(policy))\n",
    "if save_models: policy.save('%s' % (file_name), directory='./pytorch_models')\n",
    "np.save('./results/%s' % (file_name), evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b127724b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
